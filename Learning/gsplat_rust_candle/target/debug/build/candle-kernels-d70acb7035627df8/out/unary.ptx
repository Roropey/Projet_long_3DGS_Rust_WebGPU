//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_75
.address_size 64

	// .globl	ucopy_f16
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.global .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.global .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry ucopy_f16(
	.param .u64 ucopy_f16_param_0,
	.param .u64 ucopy_f16_param_1,
	.param .u64 ucopy_f16_param_2,
	.param .u64 ucopy_f16_param_3,
	.param .u64 ucopy_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [ucopy_f16_param_0];
	ld.param.u64 	%rd25, [ucopy_f16_param_1];
	ld.param.u64 	%rd27, [ucopy_f16_param_2];
	ld.param.u64 	%rd26, [ucopy_f16_param_3];
	ld.param.u64 	%rd28, [ucopy_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB0_4;

	mov.u64 	%rd61, 1;
	mov.u32 	%r38, 0;

$L__BB0_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd30, %r24;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd61, %rd35;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB0_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd61, %rd38, %rd61;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd39, %r38;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB0_2;

$L__BB0_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd62, %r4;
	@%p21 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_5;

$L__BB0_9:
	setp.ge.u64 	%p12, %rd62, %rd24;
	@%p12 bra 	$L__BB0_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB0_20;

$L__BB0_11:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB0_12:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd44, %r31;
	add.s64 	%rd45, %rd44, %rd25;
	cvt.u64.u32 	%rd11, %r43;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd12, %rd3, %rd47;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd48, %rd13, -4294967296;
	setp.eq.s64 	%p14, %rd48, 0;
	@%p14 bra 	$L__BB0_14;

	div.u64 	%rd64, %rd11, %rd13;
	mul.lo.s64 	%rd49, %rd64, %rd13;
	sub.s64 	%rd65, %rd11, %rd49;
	bra.uni 	$L__BB0_15;

$L__BB0_14:
	cvt.u32.u64 	%r32, %rd13;
	cvt.u32.u64 	%r33, %rd11;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd64, %r34;
	cvt.u64.u32 	%rd65, %r36;

$L__BB0_15:
	shl.b64 	%rd50, %rd25, 3;
	add.s64 	%rd51, %rd12, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd65;
	cvt.u32.u64 	%r37, %rd53;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd64;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd54, %r42;
	setp.lt.u64 	%p15, %rd54, %rd25;
	@%p15 bra 	$L__BB0_12;

	setp.eq.s64 	%p16, %rd26, 0;
	shl.b64 	%rd55, %rd62, 1;
	add.s64 	%rd20, %rd1, %rd55;
	@%p16 bra 	$L__BB0_18;

	mul.wide.u32 	%rd56, %r44, 2;
	add.s64 	%rd57, %rd2, %rd56;
	ld.global.u16 	%rs6, [%rd57];
	bra.uni 	$L__BB0_19;

$L__BB0_18:
	ld.global.u16 	%rs6, [%rd20];

$L__BB0_19:
	st.global.u16 	[%rd20], %rs6;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd62, %r4;
	setp.lt.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_23;

$L__BB0_5:
	setp.ge.u64 	%p8, %rd62, %rd24;
	@%p8 bra 	$L__BB0_23;

	setp.eq.s64 	%p9, %rd26, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB0_8;

$L__BB0_7:
	shl.b64 	%rd40, %rd62, 1;
	add.s64 	%rd41, %rd2, %rd40;
	ld.global.u16 	%rs4, [%rd41];
	add.s64 	%rd42, %rd1, %rd40;
	st.global.u16 	[%rd42], %rs4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd62, %r4;
	setp.lt.u64 	%p10, %rd62, %rd24;
	@%p10 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_23;

$L__BB0_8:
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd43, %r4;
	setp.lt.u64 	%p11, %rd43, %rd24;
	@%p11 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_23;

$L__BB0_20:
	setp.eq.s64 	%p18, %rd26, 0;
	@%p18 bra 	$L__BB0_22;

$L__BB0_21:
	ld.global.u16 	%rs5, [%rd2];
	shl.b64 	%rd58, %rd62, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs5;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd62, %r4;
	setp.lt.u64 	%p19, %rd62, %rd24;
	@%p19 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_23;

$L__BB0_22:
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd60, %r4;
	setp.lt.u64 	%p20, %rd60, %rd24;
	@%p20 bra 	$L__BB0_22;

$L__BB0_23:
	ret;

}
	// .globl	uneg_f16
.visible .entry uneg_f16(
	.param .u64 uneg_f16_param_0,
	.param .u64 uneg_f16_param_1,
	.param .u64 uneg_f16_param_2,
	.param .u64 uneg_f16_param_3,
	.param .u64 uneg_f16_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uneg_f16_param_0];
	ld.param.u64 	%rd27, [uneg_f16_param_1];
	ld.param.u64 	%rd29, [uneg_f16_param_2];
	ld.param.u64 	%rd28, [uneg_f16_param_3];
	ld.param.u64 	%rd30, [uneg_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB1_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB1_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB1_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB1_2;

$L__BB1_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_5;

$L__BB1_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB1_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB1_16;

$L__BB1_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB1_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB1_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB1_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB1_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 2;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 1;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.u16 	%rs4, [%rd60];
	// begin inline asm
	{neg.f16 %rs3,%rs4;
}
	// end inline asm
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB1_10;
	bra.uni 	$L__BB1_19;

$L__BB1_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB1_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB1_7:
	shl.b64 	%rd42, %rd7, 1;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.u16 	%rs2, [%rd43];
	// begin inline asm
	{neg.f16 %rs1,%rs2;
}
	// end inline asm
	add.s64 	%rd44, %rd1, %rd42;
	st.global.u16 	[%rd44], %rs1;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB1_7;

$L__BB1_19:
	ret;

$L__BB1_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB1_18;

$L__BB1_17:
	ld.global.u16 	%rs6, [%rd2];
	// begin inline asm
	{neg.f16 %rs5,%rs6;
}
	// end inline asm
	shl.b64 	%rd61, %rd7, 1;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.u16 	[%rd62], %rs5;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_19;

$L__BB1_18:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.u16 	%rs8, [%rd64];
	// begin inline asm
	{neg.f16 %rs7,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd64], %rs7;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB1_18;
	bra.uni 	$L__BB1_19;

}
	// .globl	urecip_f16
.visible .entry urecip_f16(
	.param .u64 urecip_f16_param_0,
	.param .u64 urecip_f16_param_1,
	.param .u64 urecip_f16_param_2,
	.param .u64 urecip_f16_param_3,
	.param .u64 urecip_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<85>;
	.reg .f32 	%f<53>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<74>;


	ld.param.u64 	%rd27, [urecip_f16_param_0];
	ld.param.u64 	%rd28, [urecip_f16_param_1];
	ld.param.u64 	%rd31, [urecip_f16_param_2];
	ld.param.u64 	%rd29, [urecip_f16_param_3];
	ld.param.u64 	%rd30, [urecip_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd31;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB2_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r36, 0;

$L__BB2_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd4, %rd34, 4294967295;
	add.s64 	%rd35, %rd4, %rd28;
	shl.b64 	%rd36, %rd35, 3;
	add.s64 	%rd37, %rd2, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	setp.ne.s64 	%p5, %rd67, %rd38;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB2_4;

	shl.b64 	%rd39, %rd4, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd67, %rd41, %rd67;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p7, %rd42, %rd28;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB2_2;

$L__BB2_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd6, %r4;
	@%p28 bra 	$L__BB2_16;
	bra.uni 	$L__BB2_5;

$L__BB2_16:
	setp.ge.u64 	%p16, %rd6, %rd27;
	@%p16 bra 	$L__BB2_38;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r26;
	@%p3 bra 	$L__BB2_30;

$L__BB2_18:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r4;
	mov.u32 	%r42, %r40;

$L__BB2_19:
	not.b32 	%r29, %r40;
	cvt.u64.u32 	%rd49, %r29;
	add.s64 	%rd50, %rd49, %rd28;
	cvt.u64.u32 	%rd13, %r41;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd2, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB2_21;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	cvt.u32.u64 	%r30, %rd15;
	cvt.u32.u64 	%r31, %rd13;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd71, %r32;
	cvt.u64.u32 	%rd72, %r34;

$L__BB2_22:
	shl.b64 	%rd55, %rd28, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r35, %rd58;
	add.s32 	%r42, %r42, %r35;
	cvt.u32.u64 	%r41, %rd71;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd59, %r40;
	setp.lt.u64 	%p19, %rd59, %rd28;
	@%p19 bra 	$L__BB2_19;

	setp.eq.s64 	%p20, %rd29, 0;
	shl.b64 	%rd61, %rd6, 1;
	add.s64 	%rd22, %rd1, %rd61;
	@%p20 bra 	$L__BB2_25;

	cvta.to.global.u64 	%rd62, %rd29;
	mul.wide.u32 	%rd63, %r42, 2;
	add.s64 	%rd64, %rd62, %rd63;
	ld.global.u16 	%rs81, [%rd64];
	bra.uni 	$L__BB2_26;

$L__BB2_25:
	ld.global.u16 	%rs81, [%rd22];

$L__BB2_26:
	mov.f64 	%fd3, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs51, %fd3;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f35, %rs51;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f36, %rs81;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f37, %f36;
}
	// end inline asm
	mul.f32 	%f39, %f35, %f37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs82, %f39;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs55,%rs82;
}
	// end inline asm
	mov.u16 	%rs59, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs55, %rs59;
  selp.u16 %rs57, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p21, %rs57, 0;
	@%p21 bra 	$L__BB2_29;

	mov.f32 	%f40, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f40;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs60, %rs55;
  selp.u16 %rs61, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p22, %rs61, 0;
	@%p22 bra 	$L__BB2_29;

	neg.f32 	%f42, %f36;
	fma.rn.f32 	%f43, %f42, %f39, %f35;
	fma.rn.f32 	%f41, %f37, %f43, %f39;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs82, %f41;}

	// end inline asm

$L__BB2_29:
	st.global.u16 	[%rd22], %rs82;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd6, %r4;
	setp.lt.u64 	%p23, %rd6, %rd27;
	@%p23 bra 	$L__BB2_18;
	bra.uni 	$L__BB2_38;

$L__BB2_5:
	setp.ge.u64 	%p8, %rd6, %rd27;
	@%p8 bra 	$L__BB2_38;

	setp.eq.s64 	%p9, %rd29, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB2_12;

	cvta.to.global.u64 	%rd43, %rd29;

$L__BB2_8:
	shl.b64 	%rd44, %rd6, 1;
	add.s64 	%rd45, %rd43, %rd44;
	ld.global.u16 	%rs25, [%rd45];
	mov.f64 	%fd1, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs23, %fd1;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs23;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs25;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f19, %f18;
}
	// end inline asm
	mul.f32 	%f21, %f17, %f19;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f21;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs27,%rs79;
}
	// end inline asm
	mov.u16 	%rs31, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs27, %rs31;
  selp.u16 %rs29, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p10, %rs29, 0;
	@%p10 bra 	$L__BB2_11;

	mov.f32 	%f22, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f22;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs32, %rs27;
  selp.u16 %rs33, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p11, %rs33, 0;
	@%p11 bra 	$L__BB2_11;

	neg.f32 	%f24, %f18;
	fma.rn.f32 	%f25, %f24, %f21, %f17;
	fma.rn.f32 	%f23, %f19, %f25, %f21;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f23;}

	// end inline asm

$L__BB2_11:
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs79;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd6, %r4;
	setp.lt.u64 	%p12, %rd6, %rd27;
	@%p12 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_38;

$L__BB2_12:
	shl.b64 	%rd48, %rd6, 1;
	add.s64 	%rd10, %rd1, %rd48;
	ld.global.u16 	%rs39, [%rd10];
	mov.f64 	%fd2, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs37, %fd2;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs39;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f28, %f27;
}
	// end inline asm
	mul.f32 	%f30, %f26, %f28;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f30;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs41,%rs80;
}
	// end inline asm
	mov.u16 	%rs45, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs41, %rs45;
  selp.u16 %rs43, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p13, %rs43, 0;
	@%p13 bra 	$L__BB2_15;

	mov.f32 	%f31, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f31;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs46, %rs41;
  selp.u16 %rs47, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p14, %rs47, 0;
	@%p14 bra 	$L__BB2_15;

	neg.f32 	%f33, %f27;
	fma.rn.f32 	%f34, %f33, %f30, %f26;
	fma.rn.f32 	%f32, %f28, %f34, %f30;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f32;}

	// end inline asm

$L__BB2_15:
	st.global.u16 	[%rd10], %rs80;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd6, %r4;
	setp.lt.u64 	%p15, %rd6, %rd27;
	@%p15 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_38;

$L__BB2_30:
	cvta.to.global.u64 	%rd66, %rd29;

$L__BB2_31:
	shl.b64 	%rd65, %rd6, 1;
	add.s64 	%rd25, %rd1, %rd65;
	setp.eq.s64 	%p24, %rd29, 0;
	@%p24 bra 	$L__BB2_33;

	ld.global.u16 	%rs83, [%rd66];
	bra.uni 	$L__BB2_34;

$L__BB2_33:
	ld.global.u16 	%rs83, [%rd25];

$L__BB2_34:
	mov.f64 	%fd4, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs65, %fd4;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f44, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f45, %rs83;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f46, %f45;
}
	// end inline asm
	mul.f32 	%f48, %f44, %f46;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f48;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs69,%rs84;
}
	// end inline asm
	mov.u16 	%rs73, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs69, %rs73;
  selp.u16 %rs71, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p25, %rs71, 0;
	@%p25 bra 	$L__BB2_37;

	mov.f32 	%f49, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f49;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs74, %rs69;
  selp.u16 %rs75, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p26, %rs75, 0;
	@%p26 bra 	$L__BB2_37;

	neg.f32 	%f51, %f45;
	fma.rn.f32 	%f52, %f51, %f48, %f44;
	fma.rn.f32 	%f50, %f46, %f52, %f48;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f50;}

	// end inline asm

$L__BB2_37:
	st.global.u16 	[%rd25], %rs84;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd6, %r4;
	setp.lt.u64 	%p27, %rd6, %rd27;
	@%p27 bra 	$L__BB2_31;

$L__BB2_38:
	ret;

}
	// .globl	uexp_f16
.visible .entry uexp_f16(
	.param .u64 uexp_f16_param_0,
	.param .u64 uexp_f16_param_1,
	.param .u64 uexp_f16_param_2,
	.param .u64 uexp_f16_param_3,
	.param .u64 uexp_f16_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd27, [uexp_f16_param_0];
	ld.param.u64 	%rd28, [uexp_f16_param_1];
	ld.param.u64 	%rd30, [uexp_f16_param_2];
	ld.param.u64 	%rd29, [uexp_f16_param_3];
	ld.param.u64 	%rd31, [uexp_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB3_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r36, 0;

$L__BB3_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd5, %rd34, 4294967295;
	add.s64 	%rd35, %rd5, %rd28;
	shl.b64 	%rd36, %rd35, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	setp.ne.s64 	%p5, %rd63, %rd38;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB3_4;

	shl.b64 	%rd39, %rd5, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd63, %rd41, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p7, %rd42, %rd28;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB3_2;

$L__BB3_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB3_9;
	bra.uni 	$L__BB3_5;

$L__BB3_9:
	setp.ge.u64 	%p12, %rd7, %rd27;
	@%p12 bra 	$L__BB3_24;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r26;
	@%p3 bra 	$L__BB3_20;

$L__BB3_11:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r4;
	mov.u32 	%r42, %r40;

$L__BB3_12:
	not.b32 	%r29, %r40;
	cvt.u64.u32 	%rd48, %r29;
	add.s64 	%rd49, %rd48, %rd28;
	cvt.u64.u32 	%rd13, %r41;
	shl.b64 	%rd50, %rd49, 3;
	and.b64  	%rd51, %rd50, 34359738360;
	add.s64 	%rd14, %rd3, %rd51;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd52, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd52, 0;
	@%p14 bra 	$L__BB3_14;

	div.u64 	%rd67, %rd13, %rd15;
	mul.lo.s64 	%rd53, %rd67, %rd15;
	sub.s64 	%rd68, %rd13, %rd53;
	bra.uni 	$L__BB3_15;

$L__BB3_14:
	cvt.u32.u64 	%r30, %rd15;
	cvt.u32.u64 	%r31, %rd13;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd67, %r32;
	cvt.u64.u32 	%rd68, %r34;

$L__BB3_15:
	shl.b64 	%rd54, %rd28, 3;
	add.s64 	%rd55, %rd14, %rd54;
	ld.global.u64 	%rd56, [%rd55];
	mul.lo.s64 	%rd57, %rd56, %rd68;
	cvt.u32.u64 	%r35, %rd57;
	add.s32 	%r42, %r42, %r35;
	cvt.u32.u64 	%r41, %rd67;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd58, %r40;
	setp.lt.u64 	%p15, %rd58, %rd28;
	@%p15 bra 	$L__BB3_12;

	setp.eq.s64 	%p16, %rd29, 0;
	shl.b64 	%rd59, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd59;
	@%p16 bra 	$L__BB3_18;

	mul.wide.u32 	%rd60, %r42, 2;
	add.s64 	%rd61, %rd2, %rd60;
	ld.global.u16 	%rs15, [%rd61];
	bra.uni 	$L__BB3_19;

$L__BB3_18:
	ld.global.u16 	%rs15, [%rd22];

$L__BB3_19:
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs15;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs11,r;           
}
	// end inline asm
	st.global.u16 	[%rd22], %rs11;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd27;
	@%p17 bra 	$L__BB3_11;
	bra.uni 	$L__BB3_24;

$L__BB3_20:
	shl.b64 	%rd62, %rd7, 1;
	add.s64 	%rd25, %rd1, %rd62;
	setp.eq.s64 	%p18, %rd29, 0;
	@%p18 bra 	$L__BB3_22;

	ld.global.u16 	%rs16, [%rd2];
	bra.uni 	$L__BB3_23;

$L__BB3_22:
	ld.global.u16 	%rs16, [%rd25];

$L__BB3_23:
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs16;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs13,r;           
}
	// end inline asm
	st.global.u16 	[%rd25], %rs13;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd27;
	@%p19 bra 	$L__BB3_20;
	bra.uni 	$L__BB3_24;

$L__BB3_5:
	setp.ge.u64 	%p8, %rd7, %rd27;
	@%p8 bra 	$L__BB3_24;

	setp.eq.s64 	%p9, %rd29, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB3_8;

$L__BB3_7:
	shl.b64 	%rd43, %rd7, 1;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.u16 	%rs8, [%rd44];
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs8;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs7,r;           
}
	// end inline asm
	add.s64 	%rd45, %rd1, %rd43;
	st.global.u16 	[%rd45], %rs7;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd27;
	@%p10 bra 	$L__BB3_7;
	bra.uni 	$L__BB3_24;

$L__BB3_8:
	shl.b64 	%rd46, %rd7, 1;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.u16 	%rs10, [%rd47];
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs10;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs9,r;           
}
	// end inline asm
	st.global.u16 	[%rd47], %rs9;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd27;
	@%p11 bra 	$L__BB3_8;

$L__BB3_24:
	ret;

}
	// .globl	ulog_f16
.visible .entry ulog_f16(
	.param .u64 ulog_f16_param_0,
	.param .u64 ulog_f16_param_1,
	.param .u64 ulog_f16_param_2,
	.param .u64 ulog_f16_param_3,
	.param .u64 ulog_f16_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd27, [ulog_f16_param_0];
	ld.param.u64 	%rd28, [ulog_f16_param_1];
	ld.param.u64 	%rd30, [ulog_f16_param_2];
	ld.param.u64 	%rd29, [ulog_f16_param_3];
	ld.param.u64 	%rd31, [ulog_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB4_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r36, 0;

$L__BB4_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd5, %rd34, 4294967295;
	add.s64 	%rd35, %rd5, %rd28;
	shl.b64 	%rd36, %rd35, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	setp.ne.s64 	%p5, %rd63, %rd38;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB4_4;

	shl.b64 	%rd39, %rd5, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd63, %rd41, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p7, %rd42, %rd28;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB4_2;

$L__BB4_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB4_9;
	bra.uni 	$L__BB4_5;

$L__BB4_9:
	setp.ge.u64 	%p12, %rd7, %rd27;
	@%p12 bra 	$L__BB4_24;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r26;
	@%p3 bra 	$L__BB4_20;

$L__BB4_11:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r4;
	mov.u32 	%r42, %r40;

$L__BB4_12:
	not.b32 	%r29, %r40;
	cvt.u64.u32 	%rd48, %r29;
	add.s64 	%rd49, %rd48, %rd28;
	cvt.u64.u32 	%rd13, %r41;
	shl.b64 	%rd50, %rd49, 3;
	and.b64  	%rd51, %rd50, 34359738360;
	add.s64 	%rd14, %rd3, %rd51;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd52, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd52, 0;
	@%p14 bra 	$L__BB4_14;

	div.u64 	%rd67, %rd13, %rd15;
	mul.lo.s64 	%rd53, %rd67, %rd15;
	sub.s64 	%rd68, %rd13, %rd53;
	bra.uni 	$L__BB4_15;

$L__BB4_14:
	cvt.u32.u64 	%r30, %rd15;
	cvt.u32.u64 	%r31, %rd13;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd67, %r32;
	cvt.u64.u32 	%rd68, %r34;

$L__BB4_15:
	shl.b64 	%rd54, %rd28, 3;
	add.s64 	%rd55, %rd14, %rd54;
	ld.global.u64 	%rd56, [%rd55];
	mul.lo.s64 	%rd57, %rd56, %rd68;
	cvt.u32.u64 	%r35, %rd57;
	add.s32 	%r42, %r42, %r35;
	cvt.u32.u64 	%r41, %rd67;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd58, %r40;
	setp.lt.u64 	%p15, %rd58, %rd28;
	@%p15 bra 	$L__BB4_12;

	setp.eq.s64 	%p16, %rd29, 0;
	shl.b64 	%rd59, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd59;
	@%p16 bra 	$L__BB4_18;

	mul.wide.u32 	%rd60, %r42, 2;
	add.s64 	%rd61, %rd2, %rd60;
	ld.global.u16 	%rs15, [%rd61];
	bra.uni 	$L__BB4_19;

$L__BB4_18:
	ld.global.u16 	%rs15, [%rd22];

$L__BB4_19:
	// begin inline asm
	{.reg.b32         f, C;           
 .reg.b16         r,h;            
  mov.b16         h,%rs15;           
  cvt.f32.f16     f,h;            
  lg2.approx.ftz.f32  f,f;        
  mov.b32         C, 0x3f317218U;  
  mul.f32         f,f,C;          
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X160DU;
  mov.b16 ulp,0x9C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3BFEU;
  mov.b16 ulp,0x8010U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3C0BU;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X6051U;
  mov.b16 ulp,0x1C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs11,r;           
}
	// end inline asm
	st.global.u16 	[%rd22], %rs11;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd27;
	@%p17 bra 	$L__BB4_11;
	bra.uni 	$L__BB4_24;

$L__BB4_20:
	shl.b64 	%rd62, %rd7, 1;
	add.s64 	%rd25, %rd1, %rd62;
	setp.eq.s64 	%p18, %rd29, 0;
	@%p18 bra 	$L__BB4_22;

	ld.global.u16 	%rs16, [%rd2];
	bra.uni 	$L__BB4_23;

$L__BB4_22:
	ld.global.u16 	%rs16, [%rd25];

$L__BB4_23:
	// begin inline asm
	{.reg.b32         f, C;           
 .reg.b16         r,h;            
  mov.b16         h,%rs16;           
  cvt.f32.f16     f,h;            
  lg2.approx.ftz.f32  f,f;        
  mov.b32         C, 0x3f317218U;  
  mul.f32         f,f,C;          
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X160DU;
  mov.b16 ulp,0x9C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3BFEU;
  mov.b16 ulp,0x8010U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3C0BU;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X6051U;
  mov.b16 ulp,0x1C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs13,r;           
}
	// end inline asm
	st.global.u16 	[%rd25], %rs13;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd27;
	@%p19 bra 	$L__BB4_20;
	bra.uni 	$L__BB4_24;

$L__BB4_5:
	setp.ge.u64 	%p8, %rd7, %rd27;
	@%p8 bra 	$L__BB4_24;

	setp.eq.s64 	%p9, %rd29, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB4_8;

$L__BB4_7:
	shl.b64 	%rd43, %rd7, 1;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.u16 	%rs8, [%rd44];
	// begin inline asm
	{.reg.b32         f, C;           
 .reg.b16         r,h;            
  mov.b16         h,%rs8;           
  cvt.f32.f16     f,h;            
  lg2.approx.ftz.f32  f,f;        
  mov.b32         C, 0x3f317218U;  
  mul.f32         f,f,C;          
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X160DU;
  mov.b16 ulp,0x9C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3BFEU;
  mov.b16 ulp,0x8010U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3C0BU;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X6051U;
  mov.b16 ulp,0x1C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs7,r;           
}
	// end inline asm
	add.s64 	%rd45, %rd1, %rd43;
	st.global.u16 	[%rd45], %rs7;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd27;
	@%p10 bra 	$L__BB4_7;
	bra.uni 	$L__BB4_24;

$L__BB4_8:
	shl.b64 	%rd46, %rd7, 1;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.u16 	%rs10, [%rd47];
	// begin inline asm
	{.reg.b32         f, C;           
 .reg.b16         r,h;            
  mov.b16         h,%rs10;           
  cvt.f32.f16     f,h;            
  lg2.approx.ftz.f32  f,f;        
  mov.b32         C, 0x3f317218U;  
  mul.f32         f,f,C;          
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X160DU;
  mov.b16 ulp,0x9C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3BFEU;
  mov.b16 ulp,0x8010U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3C0BU;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X6051U;
  mov.b16 ulp,0x1C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs9,r;           
}
	// end inline asm
	st.global.u16 	[%rd47], %rs9;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd27;
	@%p11 bra 	$L__BB4_8;

$L__BB4_24:
	ret;

}
	// .globl	usin_f16
.visible .entry usin_f16(
	.param .u64 usin_f16_param_0,
	.param .u64 usin_f16_param_1,
	.param .u64 usin_f16_param_2,
	.param .u64 usin_f16_param_3,
	.param .u64 usin_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<89>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<73>;


	ld.param.u64 	%rd27, [usin_f16_param_0];
	ld.param.u64 	%rd28, [usin_f16_param_1];
	ld.param.u64 	%rd29, [usin_f16_param_2];
	ld.param.u64 	%rd30, [usin_f16_param_3];
	ld.param.u64 	%rd31, [usin_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB5_4;

	mov.u64 	%rd66, 1;
	mov.u32 	%r50, 0;

$L__BB5_2:
	not.b32 	%r22, %r50;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd5, %rd34, 4294967295;
	add.s64 	%rd35, %rd5, %rd28;
	shl.b64 	%rd36, %rd35, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	setp.ne.s64 	%p5, %rd66, %rd38;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB5_4;

	shl.b64 	%rd39, %rd5, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd66, %rd41, %rd66;
	add.s32 	%r50, %r50, 1;
	cvt.u64.u32 	%rd42, %r50;
	setp.lt.u64 	%p7, %rd42, %rd28;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB5_2;

$L__BB5_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p28 bra 	$L__BB5_9;
	bra.uni 	$L__BB5_5;

$L__BB5_9:
	setp.ge.u64 	%p16, %rd7, %rd27;
	@%p16 bra 	$L__BB5_25;

	@%p3 bra 	$L__BB5_20;

$L__BB5_11:
	mov.u32 	%r54, 0;
	mov.u32 	%r55, %r4;
	mov.u32 	%r56, %r54;

$L__BB5_12:
	not.b32 	%r34, %r54;
	cvt.u64.u32 	%rd48, %r34;
	add.s64 	%rd49, %rd48, %rd28;
	cvt.u64.u32 	%rd13, %r55;
	shl.b64 	%rd51, %rd49, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB5_14;

	div.u64 	%rd70, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd70, %rd15;
	sub.s64 	%rd71, %rd13, %rd54;
	bra.uni 	$L__BB5_15;

$L__BB5_14:
	cvt.u32.u64 	%r35, %rd15;
	cvt.u32.u64 	%r36, %rd13;
	div.u32 	%r37, %r36, %r35;
	mul.lo.s32 	%r38, %r37, %r35;
	sub.s32 	%r39, %r36, %r38;
	cvt.u64.u32 	%rd70, %r37;
	cvt.u64.u32 	%rd71, %r39;

$L__BB5_15:
	shl.b64 	%rd55, %rd28, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd71;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r56, %r56, %r40;
	cvt.u32.u64 	%r55, %rd70;
	add.s32 	%r54, %r54, 1;
	cvt.u64.u32 	%rd59, %r54;
	setp.lt.u64 	%p19, %rd59, %rd28;
	@%p19 bra 	$L__BB5_12;

	setp.eq.s64 	%p20, %rd30, 0;
	shl.b64 	%rd61, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd61;
	@%p20 bra 	$L__BB5_18;

	mul.wide.u32 	%rd63, %r56, 2;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.u16 	%rs27, [%rd64];
	bra.uni 	$L__BB5_19;

$L__BB5_18:
	ld.global.u16 	%rs27, [%rd22];

$L__BB5_19:
	// begin inline asm
	{  cvt.f32.f16 %f45, %rs27;}

	// end inline asm
	mov.f32 	%f47, 0f4B400000;
	mov.f32 	%f48, 0f3F22F983;
	fma.rn.f32 	%f49, %f45, %f48, %f47;
	mov.b32 	%r41, %f49;
	sub.rn.f32 	%f50, %f49, %f47;
	mov.f32 	%f51, 0fBFC90FDA;
	fma.rn.f32 	%f52, %f50, %f51, %f45;
	mov.f32 	%f53, 0fB3A22168;
	fma.rn.f32 	%f54, %f50, %f53, %f52;
	mul.f32 	%f55, %f54, %f54;
	and.b32  	%r42, %r41, 1;
	setp.eq.b32 	%p21, %r42, 1;
	selp.f32 	%f56, 0fBAB6061A, 0f3C08839E, %p21;
	selp.f32 	%f57, 0f37CCF5CE, 0fB94CA1F9, %p21;
	selp.f32 	%f58, 0f3D2AAAA5, 0fBE2AAAA3, %p21;
	selp.f32 	%f59, 0fBF000000, 0f00000000, %p21;
	selp.f32 	%f60, %f55, %f54, %p21;
	selp.f32 	%f61, 0f3F800000, %f54, %p21;
	fma.rn.f32 	%f62, %f57, %f55, %f56;
	fma.rn.f32 	%f63, %f62, %f55, %f58;
	fma.rn.f32 	%f64, %f63, %f55, %f59;
	fma.rn.f32 	%f65, %f64, %f60, %f61;
	and.b32  	%r43, %r41, 2;
	setp.eq.s32 	%p22, %r43, 0;
	neg.f32 	%f66, %f65;
	selp.f32 	%f46, %f65, %f66, %p22;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f46;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs19;      
	  mov.b16 i, %rs27;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs19, r;      
}

	// end inline asm
	st.global.u16 	[%rd22], %rs19;
	mov.u32 	%r44, %nctaid.x;
	mad.lo.s32 	%r4, %r3, %r44, %r4;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd27;
	@%p23 bra 	$L__BB5_11;
	bra.uni 	$L__BB5_25;

$L__BB5_5:
	setp.ge.u64 	%p8, %rd7, %rd27;
	@%p8 bra 	$L__BB5_25;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB5_8;

$L__BB5_7:
	shl.b64 	%rd43, %rd7, 1;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.u16 	%rs7, [%rd44];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs7;}

	// end inline asm
	mov.f32 	%f3, 0f4B400000;
	mov.f32 	%f4, 0f3F22F983;
	fma.rn.f32 	%f5, %f1, %f4, %f3;
	mov.b32 	%r26, %f5;
	sub.rn.f32 	%f6, %f5, %f3;
	mov.f32 	%f7, 0fBFC90FDA;
	fma.rn.f32 	%f8, %f6, %f7, %f1;
	mov.f32 	%f9, 0fB3A22168;
	fma.rn.f32 	%f10, %f6, %f9, %f8;
	mul.f32 	%f11, %f10, %f10;
	and.b32  	%r27, %r26, 1;
	setp.eq.b32 	%p10, %r27, 1;
	selp.f32 	%f12, 0fBAB6061A, 0f3C08839E, %p10;
	selp.f32 	%f13, 0f37CCF5CE, 0fB94CA1F9, %p10;
	selp.f32 	%f14, 0f3D2AAAA5, 0fBE2AAAA3, %p10;
	selp.f32 	%f15, 0fBF000000, 0f00000000, %p10;
	selp.f32 	%f16, %f11, %f10, %p10;
	selp.f32 	%f17, 0f3F800000, %f10, %p10;
	fma.rn.f32 	%f18, %f13, %f11, %f12;
	fma.rn.f32 	%f19, %f18, %f11, %f14;
	fma.rn.f32 	%f20, %f19, %f11, %f15;
	fma.rn.f32 	%f21, %f20, %f16, %f17;
	and.b32  	%r28, %r26, 2;
	setp.eq.s32 	%p11, %r28, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f2, %f21, %f22, %p11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f2;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs9;      
	  mov.b16 i, %rs7;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs9, r;      
}

	// end inline asm
	add.s64 	%rd45, %rd1, %rd43;
	st.global.u16 	[%rd45], %rs9;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p12, %rd7, %rd27;
	@%p12 bra 	$L__BB5_7;
	bra.uni 	$L__BB5_25;

$L__BB5_8:
	shl.b64 	%rd46, %rd7, 1;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.u16 	%rs12, [%rd47];
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs12;}

	// end inline asm
	mov.f32 	%f25, 0f4B400000;
	mov.f32 	%f26, 0f3F22F983;
	fma.rn.f32 	%f27, %f23, %f26, %f25;
	mov.b32 	%r29, %f27;
	sub.rn.f32 	%f28, %f27, %f25;
	mov.f32 	%f29, 0fBFC90FDA;
	fma.rn.f32 	%f30, %f28, %f29, %f23;
	mov.f32 	%f31, 0fB3A22168;
	fma.rn.f32 	%f32, %f28, %f31, %f30;
	mul.f32 	%f33, %f32, %f32;
	and.b32  	%r30, %r29, 1;
	setp.eq.b32 	%p13, %r30, 1;
	selp.f32 	%f34, 0fBAB6061A, 0f3C08839E, %p13;
	selp.f32 	%f35, 0f37CCF5CE, 0fB94CA1F9, %p13;
	selp.f32 	%f36, 0f3D2AAAA5, 0fBE2AAAA3, %p13;
	selp.f32 	%f37, 0fBF000000, 0f00000000, %p13;
	selp.f32 	%f38, %f33, %f32, %p13;
	selp.f32 	%f39, 0f3F800000, %f32, %p13;
	fma.rn.f32 	%f40, %f35, %f33, %f34;
	fma.rn.f32 	%f41, %f40, %f33, %f36;
	fma.rn.f32 	%f42, %f41, %f33, %f37;
	fma.rn.f32 	%f43, %f42, %f38, %f39;
	and.b32  	%r31, %r29, 2;
	setp.eq.s32 	%p14, %r31, 0;
	neg.f32 	%f44, %f43;
	selp.f32 	%f24, %f43, %f44, %p14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f24;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs14;      
	  mov.b16 i, %rs12;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs14, r;      
}

	// end inline asm
	st.global.u16 	[%rd47], %rs14;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd27;
	@%p15 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_25;

$L__BB5_20:
	mov.u32 	%r46, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r46;

$L__BB5_21:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd25, %rd1, %rd65;
	setp.eq.s64 	%p24, %rd30, 0;
	@%p24 bra 	$L__BB5_23;

	ld.global.u16 	%rs28, [%rd2];
	bra.uni 	$L__BB5_24;

$L__BB5_23:
	ld.global.u16 	%rs28, [%rd25];

$L__BB5_24:
	// begin inline asm
	{  cvt.f32.f16 %f67, %rs28;}

	// end inline asm
	mov.f32 	%f69, 0f4B400000;
	mov.f32 	%f70, 0f3F22F983;
	fma.rn.f32 	%f71, %f67, %f70, %f69;
	mov.b32 	%r47, %f71;
	sub.rn.f32 	%f72, %f71, %f69;
	mov.f32 	%f73, 0fBFC90FDA;
	fma.rn.f32 	%f74, %f72, %f73, %f67;
	mov.f32 	%f75, 0fB3A22168;
	fma.rn.f32 	%f76, %f72, %f75, %f74;
	mul.f32 	%f77, %f76, %f76;
	and.b32  	%r48, %r47, 1;
	setp.eq.b32 	%p25, %r48, 1;
	selp.f32 	%f78, 0fBAB6061A, 0f3C08839E, %p25;
	selp.f32 	%f79, 0f37CCF5CE, 0fB94CA1F9, %p25;
	selp.f32 	%f80, 0f3D2AAAA5, 0fBE2AAAA3, %p25;
	selp.f32 	%f81, 0fBF000000, 0f00000000, %p25;
	selp.f32 	%f82, %f77, %f76, %p25;
	selp.f32 	%f83, 0f3F800000, %f76, %p25;
	fma.rn.f32 	%f84, %f79, %f77, %f78;
	fma.rn.f32 	%f85, %f84, %f77, %f80;
	fma.rn.f32 	%f86, %f85, %f77, %f81;
	fma.rn.f32 	%f87, %f86, %f82, %f83;
	and.b32  	%r49, %r47, 2;
	setp.eq.s32 	%p26, %r49, 0;
	neg.f32 	%f88, %f87;
	selp.f32 	%f68, %f87, %f88, %p26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f68;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs24;      
	  mov.b16 i, %rs28;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs24, r;      
}

	// end inline asm
	st.global.u16 	[%rd25], %rs24;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p27, %rd7, %rd27;
	@%p27 bra 	$L__BB5_21;

$L__BB5_25:
	ret;

}
	// .globl	ucos_f16
.visible .entry ucos_f16(
	.param .u64 ucos_f16_param_0,
	.param .u64 ucos_f16_param_1,
	.param .u64 ucos_f16_param_2,
	.param .u64 ucos_f16_param_3,
	.param .u64 ucos_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<89>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<73>;


	ld.param.u64 	%rd27, [ucos_f16_param_0];
	ld.param.u64 	%rd28, [ucos_f16_param_1];
	ld.param.u64 	%rd29, [ucos_f16_param_2];
	ld.param.u64 	%rd30, [ucos_f16_param_3];
	ld.param.u64 	%rd31, [ucos_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB6_4;

	mov.u64 	%rd66, 1;
	mov.u32 	%r58, 0;

$L__BB6_2:
	not.b32 	%r22, %r58;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd5, %rd34, 4294967295;
	add.s64 	%rd35, %rd5, %rd28;
	shl.b64 	%rd36, %rd35, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	setp.ne.s64 	%p5, %rd66, %rd38;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB6_4;

	shl.b64 	%rd39, %rd5, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd66, %rd41, %rd66;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd42, %r58;
	setp.lt.u64 	%p7, %rd42, %rd28;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB6_2;

$L__BB6_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p28 bra 	$L__BB6_9;
	bra.uni 	$L__BB6_5;

$L__BB6_9:
	setp.ge.u64 	%p16, %rd7, %rd27;
	@%p16 bra 	$L__BB6_25;

	@%p3 bra 	$L__BB6_20;

$L__BB6_11:
	mov.u32 	%r62, 0;
	mov.u32 	%r63, %r4;
	mov.u32 	%r64, %r62;

$L__BB6_12:
	not.b32 	%r38, %r62;
	cvt.u64.u32 	%rd48, %r38;
	add.s64 	%rd49, %rd48, %rd28;
	cvt.u64.u32 	%rd13, %r63;
	shl.b64 	%rd51, %rd49, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB6_14;

	div.u64 	%rd70, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd70, %rd15;
	sub.s64 	%rd71, %rd13, %rd54;
	bra.uni 	$L__BB6_15;

$L__BB6_14:
	cvt.u32.u64 	%r39, %rd15;
	cvt.u32.u64 	%r40, %rd13;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd70, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB6_15:
	shl.b64 	%rd55, %rd28, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd71;
	cvt.u32.u64 	%r44, %rd58;
	add.s32 	%r64, %r64, %r44;
	cvt.u32.u64 	%r63, %rd70;
	add.s32 	%r62, %r62, 1;
	cvt.u64.u32 	%rd59, %r62;
	setp.lt.u64 	%p19, %rd59, %rd28;
	@%p19 bra 	$L__BB6_12;

	setp.eq.s64 	%p20, %rd30, 0;
	shl.b64 	%rd61, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd61;
	@%p20 bra 	$L__BB6_18;

	mul.wide.u32 	%rd63, %r64, 2;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.u16 	%rs27, [%rd64];
	bra.uni 	$L__BB6_19;

$L__BB6_18:
	ld.global.u16 	%rs27, [%rd22];

$L__BB6_19:
	// begin inline asm
	{  cvt.f32.f16 %f45, %rs27;}

	// end inline asm
	mov.f32 	%f47, 0f4B400000;
	mov.f32 	%f48, 0f3F22F983;
	fma.rn.f32 	%f49, %f45, %f48, %f47;
	mov.b32 	%r45, %f49;
	sub.rn.f32 	%f50, %f49, %f47;
	mov.f32 	%f51, 0fBFC90FDA;
	fma.rn.f32 	%f52, %f50, %f51, %f45;
	mov.f32 	%f53, 0fB3A22168;
	fma.rn.f32 	%f54, %f50, %f53, %f52;
	and.b32  	%r46, %r45, 3;
	add.s32 	%r47, %r46, 1;
	mul.f32 	%f55, %f54, %f54;
	and.b32  	%r48, %r47, 1;
	setp.eq.b32 	%p21, %r48, 1;
	selp.f32 	%f56, 0fBAB6061A, 0f3C08839E, %p21;
	selp.f32 	%f57, 0f37CCF5CE, 0fB94CA1F9, %p21;
	selp.f32 	%f58, 0f3D2AAAA5, 0fBE2AAAA3, %p21;
	selp.f32 	%f59, 0fBF000000, 0f00000000, %p21;
	selp.f32 	%f60, %f55, %f54, %p21;
	selp.f32 	%f61, 0f3F800000, %f54, %p21;
	fma.rn.f32 	%f62, %f57, %f55, %f56;
	fma.rn.f32 	%f63, %f62, %f55, %f58;
	fma.rn.f32 	%f64, %f63, %f55, %f59;
	fma.rn.f32 	%f65, %f64, %f60, %f61;
	and.b32  	%r49, %r47, 2;
	setp.eq.s32 	%p22, %r49, 0;
	neg.f32 	%f66, %f65;
	selp.f32 	%f46, %f65, %f66, %p22;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f46;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs19;       
	  mov.b16 i, %rs27;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs19, r;       
}

	// end inline asm
	st.global.u16 	[%rd22], %rs19;
	mov.u32 	%r50, %nctaid.x;
	mad.lo.s32 	%r4, %r3, %r50, %r4;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd27;
	@%p23 bra 	$L__BB6_11;
	bra.uni 	$L__BB6_25;

$L__BB6_5:
	setp.ge.u64 	%p8, %rd7, %rd27;
	@%p8 bra 	$L__BB6_25;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB6_8;

$L__BB6_7:
	shl.b64 	%rd43, %rd7, 1;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.u16 	%rs7, [%rd44];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs7;}

	// end inline asm
	mov.f32 	%f3, 0f4B400000;
	mov.f32 	%f4, 0f3F22F983;
	fma.rn.f32 	%f5, %f1, %f4, %f3;
	mov.b32 	%r26, %f5;
	sub.rn.f32 	%f6, %f5, %f3;
	mov.f32 	%f7, 0fBFC90FDA;
	fma.rn.f32 	%f8, %f6, %f7, %f1;
	mov.f32 	%f9, 0fB3A22168;
	fma.rn.f32 	%f10, %f6, %f9, %f8;
	and.b32  	%r27, %r26, 3;
	add.s32 	%r28, %r27, 1;
	mul.f32 	%f11, %f10, %f10;
	and.b32  	%r29, %r28, 1;
	setp.eq.b32 	%p10, %r29, 1;
	selp.f32 	%f12, 0fBAB6061A, 0f3C08839E, %p10;
	selp.f32 	%f13, 0f37CCF5CE, 0fB94CA1F9, %p10;
	selp.f32 	%f14, 0f3D2AAAA5, 0fBE2AAAA3, %p10;
	selp.f32 	%f15, 0fBF000000, 0f00000000, %p10;
	selp.f32 	%f16, %f11, %f10, %p10;
	selp.f32 	%f17, 0f3F800000, %f10, %p10;
	fma.rn.f32 	%f18, %f13, %f11, %f12;
	fma.rn.f32 	%f19, %f18, %f11, %f14;
	fma.rn.f32 	%f20, %f19, %f11, %f15;
	fma.rn.f32 	%f21, %f20, %f16, %f17;
	and.b32  	%r30, %r28, 2;
	setp.eq.s32 	%p11, %r30, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f2, %f21, %f22, %p11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f2;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs9;       
	  mov.b16 i, %rs7;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs9, r;       
}

	// end inline asm
	add.s64 	%rd45, %rd1, %rd43;
	st.global.u16 	[%rd45], %rs9;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p12, %rd7, %rd27;
	@%p12 bra 	$L__BB6_7;
	bra.uni 	$L__BB6_25;

$L__BB6_8:
	shl.b64 	%rd46, %rd7, 1;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.u16 	%rs12, [%rd47];
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs12;}

	// end inline asm
	mov.f32 	%f25, 0f4B400000;
	mov.f32 	%f26, 0f3F22F983;
	fma.rn.f32 	%f27, %f23, %f26, %f25;
	mov.b32 	%r31, %f27;
	sub.rn.f32 	%f28, %f27, %f25;
	mov.f32 	%f29, 0fBFC90FDA;
	fma.rn.f32 	%f30, %f28, %f29, %f23;
	mov.f32 	%f31, 0fB3A22168;
	fma.rn.f32 	%f32, %f28, %f31, %f30;
	and.b32  	%r32, %r31, 3;
	add.s32 	%r33, %r32, 1;
	mul.f32 	%f33, %f32, %f32;
	and.b32  	%r34, %r33, 1;
	setp.eq.b32 	%p13, %r34, 1;
	selp.f32 	%f34, 0fBAB6061A, 0f3C08839E, %p13;
	selp.f32 	%f35, 0f37CCF5CE, 0fB94CA1F9, %p13;
	selp.f32 	%f36, 0f3D2AAAA5, 0fBE2AAAA3, %p13;
	selp.f32 	%f37, 0fBF000000, 0f00000000, %p13;
	selp.f32 	%f38, %f33, %f32, %p13;
	selp.f32 	%f39, 0f3F800000, %f32, %p13;
	fma.rn.f32 	%f40, %f35, %f33, %f34;
	fma.rn.f32 	%f41, %f40, %f33, %f36;
	fma.rn.f32 	%f42, %f41, %f33, %f37;
	fma.rn.f32 	%f43, %f42, %f38, %f39;
	and.b32  	%r35, %r33, 2;
	setp.eq.s32 	%p14, %r35, 0;
	neg.f32 	%f44, %f43;
	selp.f32 	%f24, %f43, %f44, %p14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f24;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs14;       
	  mov.b16 i, %rs12;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs14, r;       
}

	// end inline asm
	st.global.u16 	[%rd47], %rs14;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd27;
	@%p15 bra 	$L__BB6_8;
	bra.uni 	$L__BB6_25;

$L__BB6_20:
	mov.u32 	%r52, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r52;

$L__BB6_21:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd25, %rd1, %rd65;
	setp.eq.s64 	%p24, %rd30, 0;
	@%p24 bra 	$L__BB6_23;

	ld.global.u16 	%rs28, [%rd2];
	bra.uni 	$L__BB6_24;

$L__BB6_23:
	ld.global.u16 	%rs28, [%rd25];

$L__BB6_24:
	// begin inline asm
	{  cvt.f32.f16 %f67, %rs28;}

	// end inline asm
	mov.f32 	%f69, 0f4B400000;
	mov.f32 	%f70, 0f3F22F983;
	fma.rn.f32 	%f71, %f67, %f70, %f69;
	mov.b32 	%r53, %f71;
	sub.rn.f32 	%f72, %f71, %f69;
	mov.f32 	%f73, 0fBFC90FDA;
	fma.rn.f32 	%f74, %f72, %f73, %f67;
	mov.f32 	%f75, 0fB3A22168;
	fma.rn.f32 	%f76, %f72, %f75, %f74;
	and.b32  	%r54, %r53, 3;
	add.s32 	%r55, %r54, 1;
	mul.f32 	%f77, %f76, %f76;
	and.b32  	%r56, %r55, 1;
	setp.eq.b32 	%p25, %r56, 1;
	selp.f32 	%f78, 0fBAB6061A, 0f3C08839E, %p25;
	selp.f32 	%f79, 0f37CCF5CE, 0fB94CA1F9, %p25;
	selp.f32 	%f80, 0f3D2AAAA5, 0fBE2AAAA3, %p25;
	selp.f32 	%f81, 0fBF000000, 0f00000000, %p25;
	selp.f32 	%f82, %f77, %f76, %p25;
	selp.f32 	%f83, 0f3F800000, %f76, %p25;
	fma.rn.f32 	%f84, %f79, %f77, %f78;
	fma.rn.f32 	%f85, %f84, %f77, %f80;
	fma.rn.f32 	%f86, %f85, %f77, %f81;
	fma.rn.f32 	%f87, %f86, %f82, %f83;
	and.b32  	%r57, %r55, 2;
	setp.eq.s32 	%p26, %r57, 0;
	neg.f32 	%f88, %f87;
	selp.f32 	%f68, %f87, %f88, %p26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f68;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs24;       
	  mov.b16 i, %rs28;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs24, r;       
}

	// end inline asm
	st.global.u16 	[%rd25], %rs24;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p27, %rd7, %rd27;
	@%p27 bra 	$L__BB6_21;

$L__BB6_25:
	ret;

}
	// .globl	utanh_f16
.visible .entry utanh_f16(
	.param .u64 utanh_f16_param_0,
	.param .u64 utanh_f16_param_1,
	.param .u64 utanh_f16_param_2,
	.param .u64 utanh_f16_param_3,
	.param .u64 utanh_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<105>;
	.reg .b32 	%r<60>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd28, [utanh_f16_param_0];
	ld.param.u64 	%rd29, [utanh_f16_param_1];
	ld.param.u64 	%rd31, [utanh_f16_param_2];
	ld.param.u64 	%rd30, [utanh_f16_param_3];
	ld.param.u64 	%rd32, [utanh_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB7_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r52, 0;

$L__BB7_2:
	not.b32 	%r22, %r52;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd64, %rd39;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB7_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd64, %rd42, %rd64;
	add.s32 	%r52, %r52, 1;
	cvt.u64.u32 	%rd43, %r52;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB7_2;

$L__BB7_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p28 bra 	$L__BB7_15;
	bra.uni 	$L__BB7_5;

$L__BB7_15:
	setp.ge.u64 	%p16, %rd7, %rd28;
	@%p16 bra 	$L__BB7_36;

	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r34;
	@%p3 bra 	$L__BB7_29;

$L__BB7_17:
	mov.u32 	%r56, 0;
	mov.u32 	%r57, %r4;
	mov.u32 	%r58, %r56;

$L__BB7_18:
	not.b32 	%r37, %r56;
	cvt.u64.u32 	%rd49, %r37;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd14, %r57;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd15, %rd3, %rd52;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd53, %rd16, -4294967296;
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB7_20;

	div.u64 	%rd68, %rd14, %rd16;
	mul.lo.s64 	%rd54, %rd68, %rd16;
	sub.s64 	%rd69, %rd14, %rd54;
	bra.uni 	$L__BB7_21;

$L__BB7_20:
	cvt.u32.u64 	%r38, %rd16;
	cvt.u32.u64 	%r39, %rd14;
	div.u32 	%r40, %r39, %r38;
	mul.lo.s32 	%r41, %r40, %r38;
	sub.s32 	%r42, %r39, %r41;
	cvt.u64.u32 	%rd68, %r40;
	cvt.u64.u32 	%rd69, %r42;

$L__BB7_21:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd15, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd69;
	cvt.u32.u64 	%r43, %rd58;
	add.s32 	%r58, %r58, %r43;
	cvt.u32.u64 	%r57, %rd68;
	add.s32 	%r56, %r56, 1;
	cvt.u64.u32 	%rd59, %r56;
	setp.lt.u64 	%p19, %rd59, %rd29;
	@%p19 bra 	$L__BB7_18;

	setp.eq.s64 	%p20, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd60;
	@%p20 bra 	$L__BB7_24;

	mul.wide.u32 	%rd61, %r58, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs15, [%rd62];
	bra.uni 	$L__BB7_25;

$L__BB7_24:
	ld.global.u16 	%rs15, [%rd23];

$L__BB7_25:
	// begin inline asm
	{  cvt.f32.f16 %f61, %rs15;}

	// end inline asm
	abs.f32 	%f12, %f61;
	setp.ltu.f32 	%p21, %f12, 0f3F19999A;
	@%p21 bra 	$L__BB7_27;
	bra.uni 	$L__BB7_26;

$L__BB7_27:
	mul.f32 	%f70, %f61, %f61;
	mov.f32 	%f71, 0fBD563CAE;
	mov.f32 	%f72, 0f3C80F082;
	fma.rn.f32 	%f73, %f72, %f70, %f71;
	mov.f32 	%f74, 0f3E085941;
	fma.rn.f32 	%f75, %f73, %f70, %f74;
	mov.f32 	%f76, 0fBEAAA9ED;
	fma.rn.f32 	%f77, %f75, %f70, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f79, %f77, %f70, %f78;
	fma.rn.f32 	%f103, %f79, %f61, %f61;
	bra.uni 	$L__BB7_28;

$L__BB7_26:
	mul.f32 	%f62, %f12, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f63, %f62;
	add.f32 	%f64, %f63, 0f3F800000;
	mov.f32 	%f65, 0f3F800000;
	rcp.approx.ftz.f32 	%f66, %f64;
	mov.f32 	%f67, 0fC0000000;
	fma.rn.f32 	%f68, %f66, %f67, %f65;
	setp.ge.f32 	%p22, %f12, 0f41102CB4;
	selp.f32 	%f69, 0f3F800000, %f68, %p22;
	mov.b32 	%r44, %f69;
	mov.b32 	%r45, %f61;
	and.b32  	%r46, %r45, -2147483648;
	or.b32  	%r47, %r46, %r44;
	mov.b32 	%f103, %r47;

$L__BB7_28:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f103;}

	// end inline asm
	st.global.u16 	[%rd23], %rs12;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd28;
	@%p23 bra 	$L__BB7_17;
	bra.uni 	$L__BB7_36;

$L__BB7_29:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd26, %rd1, %rd63;
	setp.eq.s64 	%p24, %rd30, 0;
	@%p24 bra 	$L__BB7_31;

	ld.global.u16 	%rs16, [%rd2];
	bra.uni 	$L__BB7_32;

$L__BB7_31:
	ld.global.u16 	%rs16, [%rd26];

$L__BB7_32:
	// begin inline asm
	{  cvt.f32.f16 %f81, %rs16;}

	// end inline asm
	abs.f32 	%f17, %f81;
	setp.ltu.f32 	%p25, %f17, 0f3F19999A;
	@%p25 bra 	$L__BB7_34;
	bra.uni 	$L__BB7_33;

$L__BB7_34:
	mul.f32 	%f90, %f81, %f81;
	mov.f32 	%f91, 0fBD563CAE;
	mov.f32 	%f92, 0f3C80F082;
	fma.rn.f32 	%f93, %f92, %f90, %f91;
	mov.f32 	%f94, 0f3E085941;
	fma.rn.f32 	%f95, %f93, %f90, %f94;
	mov.f32 	%f96, 0fBEAAA9ED;
	fma.rn.f32 	%f97, %f95, %f90, %f96;
	mov.f32 	%f98, 0f00000000;
	fma.rn.f32 	%f99, %f97, %f90, %f98;
	fma.rn.f32 	%f104, %f99, %f81, %f81;
	bra.uni 	$L__BB7_35;

$L__BB7_33:
	mul.f32 	%f82, %f17, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f83, %f82;
	add.f32 	%f84, %f83, 0f3F800000;
	mov.f32 	%f85, 0f3F800000;
	rcp.approx.ftz.f32 	%f86, %f84;
	mov.f32 	%f87, 0fC0000000;
	fma.rn.f32 	%f88, %f86, %f87, %f85;
	setp.ge.f32 	%p26, %f17, 0f41102CB4;
	selp.f32 	%f89, 0f3F800000, %f88, %p26;
	mov.b32 	%r48, %f89;
	mov.b32 	%r49, %f81;
	and.b32  	%r50, %r49, -2147483648;
	or.b32  	%r51, %r50, %r48;
	mov.b32 	%f104, %r51;

$L__BB7_35:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f104;}

	// end inline asm
	st.global.u16 	[%rd26], %rs14;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p27, %rd7, %rd28;
	@%p27 bra 	$L__BB7_29;
	bra.uni 	$L__BB7_36;

$L__BB7_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB7_36;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB7_11;

$L__BB7_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs7, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs7;}

	// end inline asm
	abs.f32 	%f2, %f21;
	setp.ltu.f32 	%p10, %f2, 0f3F19999A;
	@%p10 bra 	$L__BB7_9;
	bra.uni 	$L__BB7_8;

$L__BB7_9:
	mul.f32 	%f30, %f21, %f21;
	mov.f32 	%f31, 0fBD563CAE;
	mov.f32 	%f32, 0f3C80F082;
	fma.rn.f32 	%f33, %f32, %f30, %f31;
	mov.f32 	%f34, 0f3E085941;
	fma.rn.f32 	%f35, %f33, %f30, %f34;
	mov.f32 	%f36, 0fBEAAA9ED;
	fma.rn.f32 	%f37, %f35, %f30, %f36;
	mov.f32 	%f38, 0f00000000;
	fma.rn.f32 	%f39, %f37, %f30, %f38;
	fma.rn.f32 	%f101, %f39, %f21, %f21;
	bra.uni 	$L__BB7_10;

$L__BB7_8:
	mul.f32 	%f22, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f23, %f22;
	add.f32 	%f24, %f23, 0f3F800000;
	mov.f32 	%f25, 0f3F800000;
	rcp.approx.ftz.f32 	%f26, %f24;
	mov.f32 	%f27, 0fC0000000;
	fma.rn.f32 	%f28, %f26, %f27, %f25;
	setp.ge.f32 	%p11, %f2, 0f41102CB4;
	selp.f32 	%f29, 0f3F800000, %f28, %p11;
	mov.b32 	%r26, %f29;
	mov.b32 	%r27, %f21;
	and.b32  	%r28, %r27, -2147483648;
	or.b32  	%r29, %r28, %r26;
	mov.b32 	%f101, %r29;

$L__BB7_10:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f101;}

	// end inline asm
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB7_7;
	bra.uni 	$L__BB7_36;

$L__BB7_11:
	shl.b64 	%rd48, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd48;
	ld.global.u16 	%rs9, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f41, %rs9;}

	// end inline asm
	abs.f32 	%f7, %f41;
	setp.ltu.f32 	%p13, %f7, 0f3F19999A;
	@%p13 bra 	$L__BB7_13;
	bra.uni 	$L__BB7_12;

$L__BB7_13:
	mul.f32 	%f50, %f41, %f41;
	mov.f32 	%f51, 0fBD563CAE;
	mov.f32 	%f52, 0f3C80F082;
	fma.rn.f32 	%f53, %f52, %f50, %f51;
	mov.f32 	%f54, 0f3E085941;
	fma.rn.f32 	%f55, %f53, %f50, %f54;
	mov.f32 	%f56, 0fBEAAA9ED;
	fma.rn.f32 	%f57, %f55, %f50, %f56;
	mov.f32 	%f58, 0f00000000;
	fma.rn.f32 	%f59, %f57, %f50, %f58;
	fma.rn.f32 	%f102, %f59, %f41, %f41;
	bra.uni 	$L__BB7_14;

$L__BB7_12:
	mul.f32 	%f42, %f7, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f43, %f42;
	add.f32 	%f44, %f43, 0f3F800000;
	mov.f32 	%f45, 0f3F800000;
	rcp.approx.ftz.f32 	%f46, %f44;
	mov.f32 	%f47, 0fC0000000;
	fma.rn.f32 	%f48, %f46, %f47, %f45;
	setp.ge.f32 	%p14, %f7, 0f41102CB4;
	selp.f32 	%f49, 0f3F800000, %f48, %p14;
	mov.b32 	%r30, %f49;
	mov.b32 	%r31, %f41;
	and.b32  	%r32, %r31, -2147483648;
	or.b32  	%r33, %r32, %r30;
	mov.b32 	%f102, %r33;

$L__BB7_14:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f102;}

	// end inline asm
	st.global.u16 	[%rd11], %rs10;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd28;
	@%p15 bra 	$L__BB7_11;

$L__BB7_36:
	ret;

}
	// .globl	uerf_f16
.visible .entry uerf_f16(
	.param .u64 uerf_f16_param_0,
	.param .u64 uerf_f16_param_1,
	.param .u64 uerf_f16_param_2,
	.param .u64 uerf_f16_param_3,
	.param .u64 uerf_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<113>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd27, [uerf_f16_param_0];
	ld.param.u64 	%rd28, [uerf_f16_param_1];
	ld.param.u64 	%rd29, [uerf_f16_param_2];
	ld.param.u64 	%rd30, [uerf_f16_param_3];
	ld.param.u64 	%rd31, [uerf_f16_param_4];
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB8_4;

	mov.u64 	%rd71, 1;
	mov.u32 	%r56, 0;

$L__BB8_2:
	not.b32 	%r22, %r56;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	and.b64  	%rd2, %rd34, 4294967295;
	add.s64 	%rd35, %rd2, %rd28;
	cvta.to.global.u64 	%rd36, %rd29;
	shl.b64 	%rd37, %rd35, 3;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd71, %rd39;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB8_4;

	shl.b64 	%rd41, %rd2, 3;
	add.s64 	%rd42, %rd36, %rd41;
	ld.global.u64 	%rd43, [%rd42];
	mul.lo.s64 	%rd71, %rd43, %rd71;
	add.s32 	%r56, %r56, 1;
	cvt.u64.u32 	%rd44, %r56;
	setp.lt.u64 	%p7, %rd44, %rd28;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB8_2;

$L__BB8_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd4, %r4;
	@%p28 bra 	$L__BB8_15;
	bra.uni 	$L__BB8_5;

$L__BB8_15:
	setp.ge.u64 	%p16, %rd4, %rd27;
	@%p16 bra 	$L__BB8_36;

	@%p3 bra 	$L__BB8_29;

	cvta.to.global.u64 	%rd54, %rd29;

$L__BB8_18:
	mov.u32 	%r60, 0;
	mov.u32 	%r61, %r4;
	mov.u32 	%r62, %r60;

$L__BB8_19:
	not.b32 	%r38, %r60;
	cvt.u64.u32 	%rd52, %r38;
	add.s64 	%rd53, %rd52, %rd28;
	cvt.u64.u32 	%rd12, %r61;
	shl.b64 	%rd55, %rd53, 3;
	and.b64  	%rd56, %rd55, 34359738360;
	add.s64 	%rd13, %rd54, %rd56;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd57, %rd14, -4294967296;
	setp.eq.s64 	%p18, %rd57, 0;
	@%p18 bra 	$L__BB8_21;

	div.u64 	%rd75, %rd12, %rd14;
	mul.lo.s64 	%rd58, %rd75, %rd14;
	sub.s64 	%rd76, %rd12, %rd58;
	bra.uni 	$L__BB8_22;

$L__BB8_21:
	cvt.u32.u64 	%r39, %rd14;
	cvt.u32.u64 	%r40, %rd12;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd75, %r41;
	cvt.u64.u32 	%rd76, %r43;

$L__BB8_22:
	shl.b64 	%rd59, %rd28, 3;
	add.s64 	%rd60, %rd13, %rd59;
	ld.global.u64 	%rd61, [%rd60];
	mul.lo.s64 	%rd62, %rd61, %rd76;
	cvt.u32.u64 	%r44, %rd62;
	add.s32 	%r62, %r62, %r44;
	cvt.u32.u64 	%r61, %rd75;
	add.s32 	%r60, %r60, 1;
	cvt.u64.u32 	%rd63, %r60;
	setp.lt.u64 	%p19, %rd63, %rd28;
	@%p19 bra 	$L__BB8_19;

	setp.eq.s64 	%p20, %rd30, 0;
	cvta.to.global.u64 	%rd64, %rd31;
	shl.b64 	%rd65, %rd4, 1;
	add.s64 	%rd21, %rd64, %rd65;
	@%p20 bra 	$L__BB8_25;

	cvta.to.global.u64 	%rd66, %rd30;
	mul.wide.u32 	%rd67, %r62, 2;
	add.s64 	%rd68, %rd66, %rd67;
	ld.global.u16 	%rs15, [%rd68];
	bra.uni 	$L__BB8_26;

$L__BB8_25:
	ld.global.u16 	%rs15, [%rd21];

$L__BB8_26:
	// begin inline asm
	{  cvt.f32.f16 %f63, %rs15;}

	// end inline asm
	abs.f32 	%f64, %f63;
	setp.ltu.f32 	%p21, %f64, 0f3F8060FE;
	setp.ge.f32 	%p22, %f64, 0f3F8060FE;
	mul.f32 	%f65, %f63, %f63;
	selp.f32 	%f66, %f64, %f65, %p22;
	selp.f32 	%f67, 0f38EB4C3A, 0f38B1E96A, %p22;
	selp.f32 	%f68, 0fBAAE005B, 0fBA574D20, %p22;
	fma.rn.f32 	%f69, %f67, %f66, %f68;
	selp.f32 	%f70, 0f3C09919F, 0f3BAAD5EA, %p22;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	selp.f32 	%f72, 0fBD24D99A, 0fBCDC1BE7, %p22;
	fma.rn.f32 	%f73, %f71, %f66, %f72;
	selp.f32 	%f74, 0f3E235519, 0f3DE718AF, %p22;
	fma.rn.f32 	%f75, %f73, %f66, %f74;
	selp.f32 	%f76, 0f3F69B4F9, 0fBEC093AC, %p22;
	fma.rn.f32 	%f77, %f75, %f66, %f76;
	selp.f32 	%f78, 0f3F210A14, 0f3E0375D3, %p22;
	fma.rn.f32 	%f79, %f77, %f66, %f78;
	neg.f32 	%f80, %f64;
	selp.f32 	%f81, %f80, %f63, %p22;
	fma.rn.f32 	%f111, %f79, %f81, %f81;
	@%p21 bra 	$L__BB8_28;

	ex2.approx.ftz.f32 	%f82, %f111;
	mov.f32 	%f83, 0f3F800000;
	sub.f32 	%f84, %f83, %f82;
	mov.b32 	%r45, %f84;
	mov.b32 	%r46, %f63;
	and.b32  	%r47, %r46, -2147483648;
	or.b32  	%r48, %r47, %r45;
	mov.b32 	%f111, %r48;

$L__BB8_28:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f111;}

	// end inline asm
	st.global.u16 	[%rd21], %rs12;
	mov.u32 	%r50, %nctaid.x;
	mad.lo.s32 	%r4, %r3, %r50, %r4;
	cvt.u64.u32 	%rd4, %r4;
	setp.lt.u64 	%p23, %rd4, %rd27;
	@%p23 bra 	$L__BB8_18;
	bra.uni 	$L__BB8_36;

$L__BB8_5:
	setp.ge.u64 	%p8, %rd4, %rd27;
	@%p8 bra 	$L__BB8_36;

	setp.eq.s64 	%p9, %rd30, 0;
	@%p9 bra 	$L__BB8_11;

	cvta.to.global.u64 	%rd45, %rd30;
	cvta.to.global.u64 	%rd48, %rd31;
	mov.u32 	%r29, %nctaid.x;

$L__BB8_8:
	shl.b64 	%rd46, %rd4, 1;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.u16 	%rs7, [%rd47];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs7;}

	// end inline asm
	abs.f32 	%f18, %f17;
	setp.ltu.f32 	%p10, %f18, 0f3F8060FE;
	setp.ge.f32 	%p11, %f18, 0f3F8060FE;
	mul.f32 	%f19, %f17, %f17;
	selp.f32 	%f20, %f18, %f19, %p11;
	selp.f32 	%f21, 0f38EB4C3A, 0f38B1E96A, %p11;
	selp.f32 	%f22, 0fBAAE005B, 0fBA574D20, %p11;
	fma.rn.f32 	%f23, %f21, %f20, %f22;
	selp.f32 	%f24, 0f3C09919F, 0f3BAAD5EA, %p11;
	fma.rn.f32 	%f25, %f23, %f20, %f24;
	selp.f32 	%f26, 0fBD24D99A, 0fBCDC1BE7, %p11;
	fma.rn.f32 	%f27, %f25, %f20, %f26;
	selp.f32 	%f28, 0f3E235519, 0f3DE718AF, %p11;
	fma.rn.f32 	%f29, %f27, %f20, %f28;
	selp.f32 	%f30, 0f3F69B4F9, 0fBEC093AC, %p11;
	fma.rn.f32 	%f31, %f29, %f20, %f30;
	selp.f32 	%f32, 0f3F210A14, 0f3E0375D3, %p11;
	fma.rn.f32 	%f33, %f31, %f20, %f32;
	neg.f32 	%f34, %f18;
	selp.f32 	%f35, %f34, %f17, %p11;
	fma.rn.f32 	%f109, %f33, %f35, %f35;
	@%p10 bra 	$L__BB8_10;

	ex2.approx.ftz.f32 	%f36, %f109;
	mov.f32 	%f37, 0f3F800000;
	sub.f32 	%f38, %f37, %f36;
	mov.b32 	%r25, %f38;
	mov.b32 	%r26, %f17;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, %r25;
	mov.b32 	%f109, %r28;

$L__BB8_10:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f109;}

	// end inline asm
	add.s64 	%rd50, %rd48, %rd46;
	st.global.u16 	[%rd50], %rs8;
	mad.lo.s32 	%r4, %r3, %r29, %r4;
	cvt.u64.u32 	%rd4, %r4;
	setp.lt.u64 	%p12, %rd4, %rd27;
	@%p12 bra 	$L__BB8_8;
	bra.uni 	$L__BB8_36;

$L__BB8_29:
	cvta.to.global.u64 	%rd23, %rd31;
	mov.u32 	%r51, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r51;
	cvta.to.global.u64 	%rd70, %rd30;

$L__BB8_30:
	shl.b64 	%rd69, %rd4, 1;
	add.s64 	%rd25, %rd23, %rd69;
	setp.eq.s64 	%p24, %rd30, 0;
	@%p24 bra 	$L__BB8_32;

	ld.global.u16 	%rs16, [%rd70];
	bra.uni 	$L__BB8_33;

$L__BB8_32:
	ld.global.u16 	%rs16, [%rd25];

$L__BB8_33:
	// begin inline asm
	{  cvt.f32.f16 %f86, %rs16;}

	// end inline asm
	abs.f32 	%f87, %f86;
	setp.ltu.f32 	%p25, %f87, 0f3F8060FE;
	setp.ge.f32 	%p26, %f87, 0f3F8060FE;
	mul.f32 	%f88, %f86, %f86;
	selp.f32 	%f89, %f87, %f88, %p26;
	selp.f32 	%f90, 0f38EB4C3A, 0f38B1E96A, %p26;
	selp.f32 	%f91, 0fBAAE005B, 0fBA574D20, %p26;
	fma.rn.f32 	%f92, %f90, %f89, %f91;
	selp.f32 	%f93, 0f3C09919F, 0f3BAAD5EA, %p26;
	fma.rn.f32 	%f94, %f92, %f89, %f93;
	selp.f32 	%f95, 0fBD24D99A, 0fBCDC1BE7, %p26;
	fma.rn.f32 	%f96, %f94, %f89, %f95;
	selp.f32 	%f97, 0f3E235519, 0f3DE718AF, %p26;
	fma.rn.f32 	%f98, %f96, %f89, %f97;
	selp.f32 	%f99, 0f3F69B4F9, 0fBEC093AC, %p26;
	fma.rn.f32 	%f100, %f98, %f89, %f99;
	selp.f32 	%f101, 0f3F210A14, 0f3E0375D3, %p26;
	fma.rn.f32 	%f102, %f100, %f89, %f101;
	neg.f32 	%f103, %f87;
	selp.f32 	%f104, %f103, %f86, %p26;
	fma.rn.f32 	%f112, %f102, %f104, %f104;
	@%p25 bra 	$L__BB8_35;

	ex2.approx.ftz.f32 	%f105, %f112;
	mov.f32 	%f106, 0f3F800000;
	sub.f32 	%f107, %f106, %f105;
	mov.b32 	%r52, %f107;
	mov.b32 	%r53, %f86;
	and.b32  	%r54, %r53, -2147483648;
	or.b32  	%r55, %r54, %r52;
	mov.b32 	%f112, %r55;

$L__BB8_35:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f112;}

	// end inline asm
	st.global.u16 	[%rd25], %rs14;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd4, %r4;
	setp.lt.u64 	%p27, %rd4, %rd27;
	@%p27 bra 	$L__BB8_30;
	bra.uni 	$L__BB8_36;

$L__BB8_11:
	cvta.to.global.u64 	%rd7, %rd31;
	mov.u32 	%r31, %nctaid.x;
	mul.lo.s32 	%r7, %r3, %r31;

$L__BB8_12:
	shl.b64 	%rd51, %rd4, 1;
	add.s64 	%rd9, %rd7, %rd51;
	ld.global.u16 	%rs9, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f40, %rs9;}

	// end inline asm
	abs.f32 	%f41, %f40;
	setp.ltu.f32 	%p13, %f41, 0f3F8060FE;
	setp.ge.f32 	%p14, %f41, 0f3F8060FE;
	mul.f32 	%f42, %f40, %f40;
	selp.f32 	%f43, %f41, %f42, %p14;
	selp.f32 	%f44, 0f38EB4C3A, 0f38B1E96A, %p14;
	selp.f32 	%f45, 0fBAAE005B, 0fBA574D20, %p14;
	fma.rn.f32 	%f46, %f44, %f43, %f45;
	selp.f32 	%f47, 0f3C09919F, 0f3BAAD5EA, %p14;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	selp.f32 	%f49, 0fBD24D99A, 0fBCDC1BE7, %p14;
	fma.rn.f32 	%f50, %f48, %f43, %f49;
	selp.f32 	%f51, 0f3E235519, 0f3DE718AF, %p14;
	fma.rn.f32 	%f52, %f50, %f43, %f51;
	selp.f32 	%f53, 0f3F69B4F9, 0fBEC093AC, %p14;
	fma.rn.f32 	%f54, %f52, %f43, %f53;
	selp.f32 	%f55, 0f3F210A14, 0f3E0375D3, %p14;
	fma.rn.f32 	%f56, %f54, %f43, %f55;
	neg.f32 	%f57, %f41;
	selp.f32 	%f58, %f57, %f40, %p14;
	fma.rn.f32 	%f110, %f56, %f58, %f58;
	@%p13 bra 	$L__BB8_14;

	ex2.approx.ftz.f32 	%f59, %f110;
	mov.f32 	%f60, 0f3F800000;
	sub.f32 	%f61, %f60, %f59;
	mov.b32 	%r32, %f61;
	mov.b32 	%r33, %f40;
	and.b32  	%r34, %r33, -2147483648;
	or.b32  	%r35, %r34, %r32;
	mov.b32 	%f110, %r35;

$L__BB8_14:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f110;}

	// end inline asm
	st.global.u16 	[%rd9], %rs10;
	add.s32 	%r4, %r4, %r7;
	cvt.u64.u32 	%rd4, %r4;
	setp.lt.u64 	%p15, %rd4, %rd27;
	@%p15 bra 	$L__BB8_12;

$L__BB8_36:
	ret;

}
	// .globl	uceil_f16
.visible .entry uceil_f16(
	.param .u64 uceil_f16_param_0,
	.param .u64 uceil_f16_param_1,
	.param .u64 uceil_f16_param_2,
	.param .u64 uceil_f16_param_3,
	.param .u64 uceil_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<75>;


	ld.param.u64 	%rd28, [uceil_f16_param_0];
	ld.param.u64 	%rd29, [uceil_f16_param_1];
	ld.param.u64 	%rd31, [uceil_f16_param_2];
	ld.param.u64 	%rd30, [uceil_f16_param_3];
	ld.param.u64 	%rd32, [uceil_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB9_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r38, 0;

$L__BB9_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd34, %r24;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB9_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd43, %r38;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB9_2;

$L__BB9_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p21 bra 	$L__BB9_9;
	bra.uni 	$L__BB9_5;

$L__BB9_9:
	setp.ge.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB9_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB9_20;

$L__BB9_11:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB9_12:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd49, %r31;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd13, %r43;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB9_14;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB9_15;

$L__BB9_14:
	cvt.u32.u64 	%r32, %rd15;
	cvt.u32.u64 	%r33, %rd13;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd71, %r34;
	cvt.u64.u32 	%rd72, %r36;

$L__BB9_15:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r37, %rd58;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd71;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd59, %r42;
	setp.lt.u64 	%p15, %rd59, %rd29;
	@%p15 bra 	$L__BB9_12;

	setp.eq.s64 	%p16, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd60;
	@%p16 bra 	$L__BB9_18;

	mul.wide.u32 	%rd61, %r44, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs14, [%rd62];
	bra.uni 	$L__BB9_19;

$L__BB9_18:
	ld.global.u16 	%rs14, [%rd22];

$L__BB9_19:
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs14;}

	// end inline asm
	cvt.rpi.f32.f32 	%f6, %f5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f6;}

	// end inline asm
	st.global.u16 	[%rd22], %rs9;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd28;
	@%p17 bra 	$L__BB9_11;
	bra.uni 	$L__BB9_23;

$L__BB9_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB9_23;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB9_8;

$L__BB9_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs4, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs4;}

	// end inline asm
	cvt.rpi.f32.f32 	%f2, %f1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f2;}

	// end inline asm
	add.s64 	%rd46, %rd1, %rd44;
	st.global.u16 	[%rd46], %rs5;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd28;
	@%p10 bra 	$L__BB9_7;
	bra.uni 	$L__BB9_23;

$L__BB9_8:
	shl.b64 	%rd47, %rd7, 1;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u16 	%rs6, [%rd48];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs6;}

	// end inline asm
	cvt.rpi.f32.f32 	%f4, %f3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f4;}

	// end inline asm
	st.global.u16 	[%rd48], %rs7;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB9_8;
	bra.uni 	$L__BB9_23;

$L__BB9_20:
	setp.eq.s64 	%p18, %rd30, 0;
	@%p18 bra 	$L__BB9_22;

$L__BB9_21:
	ld.global.u16 	%rs10, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs10;}

	// end inline asm
	cvt.rpi.f32.f32 	%f8, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f8;}

	// end inline asm
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u16 	[%rd64], %rs11;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd28;
	@%p19 bra 	$L__BB9_21;
	bra.uni 	$L__BB9_23;

$L__BB9_22:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u16 	%rs12, [%rd66];
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs12;}

	// end inline asm
	cvt.rpi.f32.f32 	%f10, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f10;}

	// end inline asm
	st.global.u16 	[%rd66], %rs13;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB9_22;

$L__BB9_23:
	ret;

}
	// .globl	ufloor_f16
.visible .entry ufloor_f16(
	.param .u64 ufloor_f16_param_0,
	.param .u64 ufloor_f16_param_1,
	.param .u64 ufloor_f16_param_2,
	.param .u64 ufloor_f16_param_3,
	.param .u64 ufloor_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<75>;


	ld.param.u64 	%rd28, [ufloor_f16_param_0];
	ld.param.u64 	%rd29, [ufloor_f16_param_1];
	ld.param.u64 	%rd31, [ufloor_f16_param_2];
	ld.param.u64 	%rd30, [ufloor_f16_param_3];
	ld.param.u64 	%rd32, [ufloor_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB10_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r38, 0;

$L__BB10_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd34, %r24;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB10_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd43, %r38;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB10_2;

$L__BB10_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p21 bra 	$L__BB10_9;
	bra.uni 	$L__BB10_5;

$L__BB10_9:
	setp.ge.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB10_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB10_20;

$L__BB10_11:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB10_12:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd49, %r31;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd13, %r43;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB10_14;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB10_15;

$L__BB10_14:
	cvt.u32.u64 	%r32, %rd15;
	cvt.u32.u64 	%r33, %rd13;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd71, %r34;
	cvt.u64.u32 	%rd72, %r36;

$L__BB10_15:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r37, %rd58;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd71;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd59, %r42;
	setp.lt.u64 	%p15, %rd59, %rd29;
	@%p15 bra 	$L__BB10_12;

	setp.eq.s64 	%p16, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd60;
	@%p16 bra 	$L__BB10_18;

	mul.wide.u32 	%rd61, %r44, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs14, [%rd62];
	bra.uni 	$L__BB10_19;

$L__BB10_18:
	ld.global.u16 	%rs14, [%rd22];

$L__BB10_19:
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs14;}

	// end inline asm
	cvt.rmi.f32.f32 	%f6, %f5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f6;}

	// end inline asm
	st.global.u16 	[%rd22], %rs9;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd28;
	@%p17 bra 	$L__BB10_11;
	bra.uni 	$L__BB10_23;

$L__BB10_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB10_23;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB10_8;

$L__BB10_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs4, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs4;}

	// end inline asm
	cvt.rmi.f32.f32 	%f2, %f1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f2;}

	// end inline asm
	add.s64 	%rd46, %rd1, %rd44;
	st.global.u16 	[%rd46], %rs5;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd28;
	@%p10 bra 	$L__BB10_7;
	bra.uni 	$L__BB10_23;

$L__BB10_8:
	shl.b64 	%rd47, %rd7, 1;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u16 	%rs6, [%rd48];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs6;}

	// end inline asm
	cvt.rmi.f32.f32 	%f4, %f3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f4;}

	// end inline asm
	st.global.u16 	[%rd48], %rs7;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB10_8;
	bra.uni 	$L__BB10_23;

$L__BB10_20:
	setp.eq.s64 	%p18, %rd30, 0;
	@%p18 bra 	$L__BB10_22;

$L__BB10_21:
	ld.global.u16 	%rs10, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs10;}

	// end inline asm
	cvt.rmi.f32.f32 	%f8, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f8;}

	// end inline asm
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u16 	[%rd64], %rs11;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd28;
	@%p19 bra 	$L__BB10_21;
	bra.uni 	$L__BB10_23;

$L__BB10_22:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u16 	%rs12, [%rd66];
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs12;}

	// end inline asm
	cvt.rmi.f32.f32 	%f10, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f10;}

	// end inline asm
	st.global.u16 	[%rd66], %rs13;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB10_22;

$L__BB10_23:
	ret;

}
	// .globl	uround_f16
.visible .entry uround_f16(
	.param .u64 uround_f16_param_0,
	.param .u64 uround_f16_param_1,
	.param .u64 uround_f16_param_2,
	.param .u64 uround_f16_param_3,
	.param .u64 uround_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<21>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<75>;


	ld.param.u64 	%rd28, [uround_f16_param_0];
	ld.param.u64 	%rd29, [uround_f16_param_1];
	ld.param.u64 	%rd31, [uround_f16_param_2];
	ld.param.u64 	%rd30, [uround_f16_param_3];
	ld.param.u64 	%rd32, [uround_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB11_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r53, 0;

$L__BB11_2:
	not.b32 	%r24, %r53;
	cvt.u64.u32 	%rd34, %r24;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB11_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r53, %r53, 1;
	cvt.u64.u32 	%rd43, %r53;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB11_2;

$L__BB11_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p21 bra 	$L__BB11_9;
	bra.uni 	$L__BB11_5;

$L__BB11_9:
	setp.ge.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB11_23;

	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r34;
	@%p3 bra 	$L__BB11_20;

$L__BB11_11:
	mov.u32 	%r57, 0;
	mov.u32 	%r58, %r4;
	mov.u32 	%r59, %r57;

$L__BB11_12:
	not.b32 	%r37, %r57;
	cvt.u64.u32 	%rd49, %r37;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd13, %r58;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB11_14;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB11_15;

$L__BB11_14:
	cvt.u32.u64 	%r38, %rd15;
	cvt.u32.u64 	%r39, %rd13;
	div.u32 	%r40, %r39, %r38;
	mul.lo.s32 	%r41, %r40, %r38;
	sub.s32 	%r42, %r39, %r41;
	cvt.u64.u32 	%rd71, %r40;
	cvt.u64.u32 	%rd72, %r42;

$L__BB11_15:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r43, %rd58;
	add.s32 	%r59, %r59, %r43;
	cvt.u32.u64 	%r58, %rd71;
	add.s32 	%r57, %r57, 1;
	cvt.u64.u32 	%rd59, %r57;
	setp.lt.u64 	%p15, %rd59, %rd29;
	@%p15 bra 	$L__BB11_12;

	setp.eq.s64 	%p16, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd60;
	@%p16 bra 	$L__BB11_18;

	mul.wide.u32 	%rd61, %r59, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs14, [%rd62];
	bra.uni 	$L__BB11_19;

$L__BB11_18:
	ld.global.u16 	%rs14, [%rd22];

$L__BB11_19:
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs14;}

	// end inline asm
	mov.b32 	%r44, %f9;
	and.b32  	%r45, %r44, -2147483648;
	or.b32  	%r46, %r45, 1056964608;
	mov.b32 	%f11, %r46;
	add.rz.f32 	%f12, %f9, %f11;
	cvt.rzi.f32.f32 	%f10, %f12;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f10;}

	// end inline asm
	st.global.u16 	[%rd22], %rs9;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd28;
	@%p17 bra 	$L__BB11_11;
	bra.uni 	$L__BB11_23;

$L__BB11_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB11_23;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB11_8;

$L__BB11_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs4, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs4;}

	// end inline asm
	mov.b32 	%r28, %f1;
	and.b32  	%r29, %r28, -2147483648;
	or.b32  	%r30, %r29, 1056964608;
	mov.b32 	%f3, %r30;
	add.rz.f32 	%f4, %f1, %f3;
	cvt.rzi.f32.f32 	%f2, %f4;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f2;}

	// end inline asm
	add.s64 	%rd46, %rd1, %rd44;
	st.global.u16 	[%rd46], %rs5;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd28;
	@%p10 bra 	$L__BB11_7;
	bra.uni 	$L__BB11_23;

$L__BB11_8:
	shl.b64 	%rd47, %rd7, 1;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u16 	%rs6, [%rd48];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs6;}

	// end inline asm
	mov.b32 	%r31, %f5;
	and.b32  	%r32, %r31, -2147483648;
	or.b32  	%r33, %r32, 1056964608;
	mov.b32 	%f7, %r33;
	add.rz.f32 	%f8, %f5, %f7;
	cvt.rzi.f32.f32 	%f6, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f6;}

	// end inline asm
	st.global.u16 	[%rd48], %rs7;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB11_8;
	bra.uni 	$L__BB11_23;

$L__BB11_20:
	setp.eq.s64 	%p18, %rd30, 0;
	@%p18 bra 	$L__BB11_22;

$L__BB11_21:
	ld.global.u16 	%rs10, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs10;}

	// end inline asm
	mov.b32 	%r47, %f13;
	and.b32  	%r48, %r47, -2147483648;
	or.b32  	%r49, %r48, 1056964608;
	mov.b32 	%f15, %r49;
	add.rz.f32 	%f16, %f13, %f15;
	cvt.rzi.f32.f32 	%f14, %f16;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f14;}

	// end inline asm
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u16 	[%rd64], %rs11;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd28;
	@%p19 bra 	$L__BB11_21;
	bra.uni 	$L__BB11_23;

$L__BB11_22:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u16 	%rs12, [%rd66];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs12;}

	// end inline asm
	mov.b32 	%r50, %f17;
	and.b32  	%r51, %r50, -2147483648;
	or.b32  	%r52, %r51, 1056964608;
	mov.b32 	%f19, %r52;
	add.rz.f32 	%f20, %f17, %f19;
	cvt.rzi.f32.f32 	%f18, %f20;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f18;}

	// end inline asm
	st.global.u16 	[%rd66], %rs13;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB11_22;

$L__BB11_23:
	ret;

}
	// .globl	unormcdf_f16
.visible .entry unormcdf_f16(
	.param .u64 unormcdf_f16_param_0,
	.param .u64 unormcdf_f16_param_1,
	.param .u64 unormcdf_f16_param_2,
	.param .u64 unormcdf_f16_param_3,
	.param .u64 unormcdf_f16_param_4
)
{
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<337>;
	.reg .b32 	%r<76>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd28, [unormcdf_f16_param_0];
	ld.param.u64 	%rd29, [unormcdf_f16_param_1];
	ld.param.u64 	%rd31, [unormcdf_f16_param_2];
	ld.param.u64 	%rd30, [unormcdf_f16_param_3];
	ld.param.u64 	%rd32, [unormcdf_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p40, %p2;
	@%p3 bra 	$L__BB12_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r68, 0;

$L__BB12_2:
	not.b32 	%r22, %r68;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd64, %rd39;
	mov.pred 	%p40, -1;
	@%p5 bra 	$L__BB12_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd64, %rd42, %rd64;
	add.s32 	%r68, %r68, 1;
	cvt.u64.u32 	%rd43, %r68;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p40, %p2;
	@%p7 bra 	$L__BB12_2;

$L__BB12_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p40 bra 	$L__BB12_13;
	bra.uni 	$L__BB12_5;

$L__BB12_13:
	setp.ge.u64 	%p22, %rd7, %rd28;
	@%p22 bra 	$L__BB12_32;

	mov.u32 	%r42, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r42;
	@%p3 bra 	$L__BB12_26;

$L__BB12_15:
	mov.u32 	%r72, 0;
	mov.u32 	%r73, %r4;
	mov.u32 	%r74, %r72;

$L__BB12_16:
	not.b32 	%r45, %r72;
	cvt.u64.u32 	%rd49, %r45;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd14, %r73;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd15, %rd3, %rd52;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd53, %rd16, -4294967296;
	setp.eq.s64 	%p24, %rd53, 0;
	@%p24 bra 	$L__BB12_18;

	div.u64 	%rd68, %rd14, %rd16;
	mul.lo.s64 	%rd54, %rd68, %rd16;
	sub.s64 	%rd69, %rd14, %rd54;
	bra.uni 	$L__BB12_19;

$L__BB12_18:
	cvt.u32.u64 	%r46, %rd16;
	cvt.u32.u64 	%r47, %rd14;
	div.u32 	%r48, %r47, %r46;
	mul.lo.s32 	%r49, %r48, %r46;
	sub.s32 	%r50, %r47, %r49;
	cvt.u64.u32 	%rd68, %r48;
	cvt.u64.u32 	%rd69, %r50;

$L__BB12_19:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd15, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd69;
	cvt.u32.u64 	%r51, %rd58;
	add.s32 	%r74, %r74, %r51;
	cvt.u32.u64 	%r73, %rd68;
	add.s32 	%r72, %r72, 1;
	cvt.u64.u32 	%rd59, %r72;
	setp.lt.u64 	%p25, %rd59, %rd29;
	@%p25 bra 	$L__BB12_16;

	setp.eq.s64 	%p26, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd60;
	@%p26 bra 	$L__BB12_22;

	mul.wide.u32 	%rd61, %r74, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs15, [%rd62];
	bra.uni 	$L__BB12_23;

$L__BB12_22:
	ld.global.u16 	%rs15, [%rd23];

$L__BB12_23:
	// begin inline asm
	{  cvt.f32.f16 %f179, %rs15;}

	// end inline asm
	abs.f32 	%f180, %f179;
	setp.gt.f32 	%p27, %f180, 0f41680000;
	mov.b32 	%r52, %f179;
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r53, 1097334784;
	mov.b32 	%f181, %r54;
	selp.f32 	%f182, %f181, %f179, %p27;
	mov.f32 	%f183, 0fBF3504F3;
	mul.rn.f32 	%f13, %f182, %f183;
	neg.f32 	%f184, %f13;
	fma.rn.f32 	%f185, %f182, %f183, %f184;
	mov.f32 	%f186, 0fB24FE77A;
	fma.rn.f32 	%f14, %f182, %f186, %f185;
	add.rn.f32 	%f15, %f13, %f14;
	abs.f32 	%f187, %f15;
	add.f32 	%f188, %f187, 0fC0800000;
	mov.f32 	%f189, 0fC0800000;
	add.f32 	%f190, %f187, 0f40800000;
	rcp.approx.ftz.f32 	%f191, %f190;
	mul.rn.f32 	%f192, %f188, %f191;
	add.f32 	%f193, %f192, 0f3F800000;
	mov.f32 	%f194, 0f3F800000;
	fma.rn.f32 	%f195, %f189, %f193, %f187;
	neg.f32 	%f196, %f192;
	fma.rn.f32 	%f197, %f196, %f187, %f195;
	fma.rn.f32 	%f198, %f191, %f197, %f192;
	mov.f32 	%f199, 0f3BE6E05B;
	mov.f32 	%f200, 0f3A69A091;
	fma.rn.f32 	%f201, %f200, %f198, %f199;
	mov.f32 	%f202, 0fBC81FB4B;
	fma.rn.f32 	%f203, %f201, %f198, %f202;
	mov.f32 	%f204, 0f3D15373B;
	fma.rn.f32 	%f205, %f203, %f198, %f204;
	mov.f32 	%f206, 0fBD887C5A;
	fma.rn.f32 	%f207, %f205, %f198, %f206;
	mov.f32 	%f208, 0f3DC021D5;
	fma.rn.f32 	%f209, %f207, %f198, %f208;
	mov.f32 	%f210, 0fBDCED424;
	fma.rn.f32 	%f211, %f209, %f198, %f210;
	mov.f32 	%f212, 0f3D8B74DE;
	fma.rn.f32 	%f213, %f211, %f198, %f212;
	mov.f32 	%f214, 0f3C7BF170;
	fma.rn.f32 	%f215, %f213, %f198, %f214;
	mov.f32 	%f216, 0fBE0EF8D4;
	fma.rn.f32 	%f217, %f215, %f198, %f216;
	mov.f32 	%f218, 0f3F9DD2C9;
	fma.rn.f32 	%f219, %f217, %f198, %f218;
	mov.f32 	%f220, 0f40000000;
	fma.rn.f32 	%f221, %f220, %f187, %f194;
	rcp.approx.ftz.f32 	%f222, %f221;
	mul.rn.f32 	%f223, %f219, %f222;
	mul.f32 	%f224, %f223, 0fC0000000;
	fma.rn.f32 	%f225, %f187, %f224, %f219;
	sub.f32 	%f226, %f225, %f223;
	fma.rn.f32 	%f227, %f226, %f222, %f223;
	mul.f32 	%f228, %f187, %f187;
	neg.f32 	%f229, %f228;
	mov.f32 	%f230, 0f3FB8AA3B;
	mul.rn.f32 	%f231, %f229, %f230;
	cvt.rzi.f32.f32 	%f232, %f231;
	abs.f32 	%f233, %f232;
	setp.gt.f32 	%p28, %f233, 0f42FC0000;
	mov.b32 	%r55, %f232;
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r56, 1123811328;
	mov.b32 	%f234, %r57;
	selp.f32 	%f235, %f234, %f232, %p28;
	mov.f32 	%f236, 0fBF317218;
	fma.rn.f32 	%f237, %f235, %f236, %f229;
	mov.f32 	%f238, 0f3102E308;
	fma.rn.f32 	%f239, %f235, %f238, %f237;
	mul.f32 	%f240, %f239, 0f3FB8AA3B;
	add.f32 	%f241, %f235, 0f4B40007F;
	mov.b32 	%r58, %f241;
	shl.b32 	%r59, %r58, 23;
	mov.b32 	%f242, %r59;
	ex2.approx.ftz.f32 	%f243, %f240;
	mul.f32 	%f244, %f243, %f242;
	neg.f32 	%f245, %f187;
	fma.rn.f32 	%f246, %f245, %f187, %f228;
	fma.rn.f32 	%f247, %f244, %f246, %f244;
	mul.f32 	%f248, %f227, %f247;
	setp.gt.f32 	%p29, %f187, 0f4120E148;
	selp.f32 	%f249, 0f00000000, %f248, %p29;
	setp.lt.f32 	%p30, %f15, 0f00000000;
	sub.f32 	%f250, %f220, %f249;
	selp.f32 	%f335, %f250, %f249, %p30;
	setp.geu.f32 	%p31, %f182, 0fBF800000;
	@%p31 bra 	$L__BB12_25;

	sub.f32 	%f251, %f13, %f15;
	add.rn.f32 	%f252, %f251, %f14;
	mul.f32 	%f253, %f15, 0fC0000000;
	mul.f32 	%f254, %f253, %f335;
	fma.rn.f32 	%f335, %f254, %f252, %f335;

$L__BB12_25:
	mul.f32 	%f255, %f335, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f255;}

	// end inline asm
	st.global.u16 	[%rd23], %rs12;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p32, %rd7, %rd28;
	@%p32 bra 	$L__BB12_15;
	bra.uni 	$L__BB12_32;

$L__BB12_26:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd26, %rd1, %rd63;
	setp.eq.s64 	%p33, %rd30, 0;
	@%p33 bra 	$L__BB12_28;

	ld.global.u16 	%rs16, [%rd2];
	bra.uni 	$L__BB12_29;

$L__BB12_28:
	ld.global.u16 	%rs16, [%rd26];

$L__BB12_29:
	// begin inline asm
	{  cvt.f32.f16 %f256, %rs16;}

	// end inline asm
	abs.f32 	%f257, %f256;
	setp.gt.f32 	%p34, %f257, 0f41680000;
	mov.b32 	%r60, %f256;
	and.b32  	%r61, %r60, -2147483648;
	or.b32  	%r62, %r61, 1097334784;
	mov.b32 	%f258, %r62;
	selp.f32 	%f259, %f258, %f256, %p34;
	mov.f32 	%f260, 0fBF3504F3;
	mul.rn.f32 	%f19, %f259, %f260;
	neg.f32 	%f261, %f19;
	fma.rn.f32 	%f262, %f259, %f260, %f261;
	mov.f32 	%f263, 0fB24FE77A;
	fma.rn.f32 	%f20, %f259, %f263, %f262;
	add.rn.f32 	%f21, %f19, %f20;
	abs.f32 	%f264, %f21;
	add.f32 	%f265, %f264, 0fC0800000;
	mov.f32 	%f266, 0fC0800000;
	add.f32 	%f267, %f264, 0f40800000;
	rcp.approx.ftz.f32 	%f268, %f267;
	mul.rn.f32 	%f269, %f265, %f268;
	add.f32 	%f270, %f269, 0f3F800000;
	mov.f32 	%f271, 0f3F800000;
	fma.rn.f32 	%f272, %f266, %f270, %f264;
	neg.f32 	%f273, %f269;
	fma.rn.f32 	%f274, %f273, %f264, %f272;
	fma.rn.f32 	%f275, %f268, %f274, %f269;
	mov.f32 	%f276, 0f3BE6E05B;
	mov.f32 	%f277, 0f3A69A091;
	fma.rn.f32 	%f278, %f277, %f275, %f276;
	mov.f32 	%f279, 0fBC81FB4B;
	fma.rn.f32 	%f280, %f278, %f275, %f279;
	mov.f32 	%f281, 0f3D15373B;
	fma.rn.f32 	%f282, %f280, %f275, %f281;
	mov.f32 	%f283, 0fBD887C5A;
	fma.rn.f32 	%f284, %f282, %f275, %f283;
	mov.f32 	%f285, 0f3DC021D5;
	fma.rn.f32 	%f286, %f284, %f275, %f285;
	mov.f32 	%f287, 0fBDCED424;
	fma.rn.f32 	%f288, %f286, %f275, %f287;
	mov.f32 	%f289, 0f3D8B74DE;
	fma.rn.f32 	%f290, %f288, %f275, %f289;
	mov.f32 	%f291, 0f3C7BF170;
	fma.rn.f32 	%f292, %f290, %f275, %f291;
	mov.f32 	%f293, 0fBE0EF8D4;
	fma.rn.f32 	%f294, %f292, %f275, %f293;
	mov.f32 	%f295, 0f3F9DD2C9;
	fma.rn.f32 	%f296, %f294, %f275, %f295;
	mov.f32 	%f297, 0f40000000;
	fma.rn.f32 	%f298, %f297, %f264, %f271;
	rcp.approx.ftz.f32 	%f299, %f298;
	mul.rn.f32 	%f300, %f296, %f299;
	mul.f32 	%f301, %f300, 0fC0000000;
	fma.rn.f32 	%f302, %f264, %f301, %f296;
	sub.f32 	%f303, %f302, %f300;
	fma.rn.f32 	%f304, %f303, %f299, %f300;
	mul.f32 	%f305, %f264, %f264;
	neg.f32 	%f306, %f305;
	mov.f32 	%f307, 0f3FB8AA3B;
	mul.rn.f32 	%f308, %f306, %f307;
	cvt.rzi.f32.f32 	%f309, %f308;
	abs.f32 	%f310, %f309;
	setp.gt.f32 	%p35, %f310, 0f42FC0000;
	mov.b32 	%r63, %f309;
	and.b32  	%r64, %r63, -2147483648;
	or.b32  	%r65, %r64, 1123811328;
	mov.b32 	%f311, %r65;
	selp.f32 	%f312, %f311, %f309, %p35;
	mov.f32 	%f313, 0fBF317218;
	fma.rn.f32 	%f314, %f312, %f313, %f306;
	mov.f32 	%f315, 0f3102E308;
	fma.rn.f32 	%f316, %f312, %f315, %f314;
	mul.f32 	%f317, %f316, 0f3FB8AA3B;
	add.f32 	%f318, %f312, 0f4B40007F;
	mov.b32 	%r66, %f318;
	shl.b32 	%r67, %r66, 23;
	mov.b32 	%f319, %r67;
	ex2.approx.ftz.f32 	%f320, %f317;
	mul.f32 	%f321, %f320, %f319;
	neg.f32 	%f322, %f264;
	fma.rn.f32 	%f323, %f322, %f264, %f305;
	fma.rn.f32 	%f324, %f321, %f323, %f321;
	mul.f32 	%f325, %f304, %f324;
	setp.gt.f32 	%p36, %f264, 0f4120E148;
	selp.f32 	%f326, 0f00000000, %f325, %p36;
	setp.lt.f32 	%p37, %f21, 0f00000000;
	sub.f32 	%f327, %f297, %f326;
	selp.f32 	%f336, %f327, %f326, %p37;
	setp.geu.f32 	%p38, %f259, 0fBF800000;
	@%p38 bra 	$L__BB12_31;

	sub.f32 	%f328, %f19, %f21;
	add.rn.f32 	%f329, %f328, %f20;
	mul.f32 	%f330, %f21, 0fC0000000;
	mul.f32 	%f331, %f330, %f336;
	fma.rn.f32 	%f336, %f331, %f329, %f336;

$L__BB12_31:
	mul.f32 	%f332, %f336, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f332;}

	// end inline asm
	st.global.u16 	[%rd26], %rs14;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p39, %rd7, %rd28;
	@%p39 bra 	$L__BB12_26;
	bra.uni 	$L__BB12_32;

$L__BB12_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB12_32;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB12_10;

$L__BB12_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs7, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs7;}

	// end inline asm
	abs.f32 	%f26, %f25;
	setp.gt.f32 	%p10, %f26, 0f41680000;
	mov.b32 	%r26, %f25;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1097334784;
	mov.b32 	%f27, %r28;
	selp.f32 	%f28, %f27, %f25, %p10;
	mov.f32 	%f29, 0fBF3504F3;
	mul.rn.f32 	%f1, %f28, %f29;
	neg.f32 	%f30, %f1;
	fma.rn.f32 	%f31, %f28, %f29, %f30;
	mov.f32 	%f32, 0fB24FE77A;
	fma.rn.f32 	%f2, %f28, %f32, %f31;
	add.rn.f32 	%f3, %f1, %f2;
	abs.f32 	%f33, %f3;
	add.f32 	%f34, %f33, 0fC0800000;
	mov.f32 	%f35, 0fC0800000;
	add.f32 	%f36, %f33, 0f40800000;
	rcp.approx.ftz.f32 	%f37, %f36;
	mul.rn.f32 	%f38, %f34, %f37;
	add.f32 	%f39, %f38, 0f3F800000;
	mov.f32 	%f40, 0f3F800000;
	fma.rn.f32 	%f41, %f35, %f39, %f33;
	neg.f32 	%f42, %f38;
	fma.rn.f32 	%f43, %f42, %f33, %f41;
	fma.rn.f32 	%f44, %f37, %f43, %f38;
	mov.f32 	%f45, 0f3BE6E05B;
	mov.f32 	%f46, 0f3A69A091;
	fma.rn.f32 	%f47, %f46, %f44, %f45;
	mov.f32 	%f48, 0fBC81FB4B;
	fma.rn.f32 	%f49, %f47, %f44, %f48;
	mov.f32 	%f50, 0f3D15373B;
	fma.rn.f32 	%f51, %f49, %f44, %f50;
	mov.f32 	%f52, 0fBD887C5A;
	fma.rn.f32 	%f53, %f51, %f44, %f52;
	mov.f32 	%f54, 0f3DC021D5;
	fma.rn.f32 	%f55, %f53, %f44, %f54;
	mov.f32 	%f56, 0fBDCED424;
	fma.rn.f32 	%f57, %f55, %f44, %f56;
	mov.f32 	%f58, 0f3D8B74DE;
	fma.rn.f32 	%f59, %f57, %f44, %f58;
	mov.f32 	%f60, 0f3C7BF170;
	fma.rn.f32 	%f61, %f59, %f44, %f60;
	mov.f32 	%f62, 0fBE0EF8D4;
	fma.rn.f32 	%f63, %f61, %f44, %f62;
	mov.f32 	%f64, 0f3F9DD2C9;
	fma.rn.f32 	%f65, %f63, %f44, %f64;
	mov.f32 	%f66, 0f40000000;
	fma.rn.f32 	%f67, %f66, %f33, %f40;
	rcp.approx.ftz.f32 	%f68, %f67;
	mul.rn.f32 	%f69, %f65, %f68;
	mul.f32 	%f70, %f69, 0fC0000000;
	fma.rn.f32 	%f71, %f33, %f70, %f65;
	sub.f32 	%f72, %f71, %f69;
	fma.rn.f32 	%f73, %f72, %f68, %f69;
	mul.f32 	%f74, %f33, %f33;
	neg.f32 	%f75, %f74;
	mov.f32 	%f76, 0f3FB8AA3B;
	mul.rn.f32 	%f77, %f75, %f76;
	cvt.rzi.f32.f32 	%f78, %f77;
	abs.f32 	%f79, %f78;
	setp.gt.f32 	%p11, %f79, 0f42FC0000;
	mov.b32 	%r29, %f78;
	and.b32  	%r30, %r29, -2147483648;
	or.b32  	%r31, %r30, 1123811328;
	mov.b32 	%f80, %r31;
	selp.f32 	%f81, %f80, %f78, %p11;
	mov.f32 	%f82, 0fBF317218;
	fma.rn.f32 	%f83, %f81, %f82, %f75;
	mov.f32 	%f84, 0f3102E308;
	fma.rn.f32 	%f85, %f81, %f84, %f83;
	mul.f32 	%f86, %f85, 0f3FB8AA3B;
	add.f32 	%f87, %f81, 0f4B40007F;
	mov.b32 	%r32, %f87;
	shl.b32 	%r33, %r32, 23;
	mov.b32 	%f88, %r33;
	ex2.approx.ftz.f32 	%f89, %f86;
	mul.f32 	%f90, %f89, %f88;
	neg.f32 	%f91, %f33;
	fma.rn.f32 	%f92, %f91, %f33, %f74;
	fma.rn.f32 	%f93, %f90, %f92, %f90;
	mul.f32 	%f94, %f73, %f93;
	setp.gt.f32 	%p12, %f33, 0f4120E148;
	selp.f32 	%f95, 0f00000000, %f94, %p12;
	setp.lt.f32 	%p13, %f3, 0f00000000;
	sub.f32 	%f96, %f66, %f95;
	selp.f32 	%f333, %f96, %f95, %p13;
	setp.geu.f32 	%p14, %f28, 0fBF800000;
	@%p14 bra 	$L__BB12_9;

	sub.f32 	%f97, %f1, %f3;
	add.rn.f32 	%f98, %f97, %f2;
	mul.f32 	%f99, %f3, 0fC0000000;
	mul.f32 	%f100, %f99, %f333;
	fma.rn.f32 	%f333, %f100, %f98, %f333;

$L__BB12_9:
	mul.f32 	%f101, %f333, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f101;}

	// end inline asm
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd28;
	@%p15 bra 	$L__BB12_7;
	bra.uni 	$L__BB12_32;

$L__BB12_10:
	shl.b64 	%rd48, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd48;
	ld.global.u16 	%rs9, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f102, %rs9;}

	// end inline asm
	abs.f32 	%f103, %f102;
	setp.gt.f32 	%p16, %f103, 0f41680000;
	mov.b32 	%r34, %f102;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1097334784;
	mov.b32 	%f104, %r36;
	selp.f32 	%f105, %f104, %f102, %p16;
	mov.f32 	%f106, 0fBF3504F3;
	mul.rn.f32 	%f7, %f105, %f106;
	neg.f32 	%f107, %f7;
	fma.rn.f32 	%f108, %f105, %f106, %f107;
	mov.f32 	%f109, 0fB24FE77A;
	fma.rn.f32 	%f8, %f105, %f109, %f108;
	add.rn.f32 	%f9, %f7, %f8;
	abs.f32 	%f110, %f9;
	add.f32 	%f111, %f110, 0fC0800000;
	mov.f32 	%f112, 0fC0800000;
	add.f32 	%f113, %f110, 0f40800000;
	rcp.approx.ftz.f32 	%f114, %f113;
	mul.rn.f32 	%f115, %f111, %f114;
	add.f32 	%f116, %f115, 0f3F800000;
	mov.f32 	%f117, 0f3F800000;
	fma.rn.f32 	%f118, %f112, %f116, %f110;
	neg.f32 	%f119, %f115;
	fma.rn.f32 	%f120, %f119, %f110, %f118;
	fma.rn.f32 	%f121, %f114, %f120, %f115;
	mov.f32 	%f122, 0f3BE6E05B;
	mov.f32 	%f123, 0f3A69A091;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0fBC81FB4B;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3D15373B;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0fBD887C5A;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3DC021D5;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0fBDCED424;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3D8B74DE;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3C7BF170;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0fBE0EF8D4;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F9DD2C9;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mov.f32 	%f143, 0f40000000;
	fma.rn.f32 	%f144, %f143, %f110, %f117;
	rcp.approx.ftz.f32 	%f145, %f144;
	mul.rn.f32 	%f146, %f142, %f145;
	mul.f32 	%f147, %f146, 0fC0000000;
	fma.rn.f32 	%f148, %f110, %f147, %f142;
	sub.f32 	%f149, %f148, %f146;
	fma.rn.f32 	%f150, %f149, %f145, %f146;
	mul.f32 	%f151, %f110, %f110;
	neg.f32 	%f152, %f151;
	mov.f32 	%f153, 0f3FB8AA3B;
	mul.rn.f32 	%f154, %f152, %f153;
	cvt.rzi.f32.f32 	%f155, %f154;
	abs.f32 	%f156, %f155;
	setp.gt.f32 	%p17, %f156, 0f42FC0000;
	mov.b32 	%r37, %f155;
	and.b32  	%r38, %r37, -2147483648;
	or.b32  	%r39, %r38, 1123811328;
	mov.b32 	%f157, %r39;
	selp.f32 	%f158, %f157, %f155, %p17;
	mov.f32 	%f159, 0fBF317218;
	fma.rn.f32 	%f160, %f158, %f159, %f152;
	mov.f32 	%f161, 0f3102E308;
	fma.rn.f32 	%f162, %f158, %f161, %f160;
	mul.f32 	%f163, %f162, 0f3FB8AA3B;
	add.f32 	%f164, %f158, 0f4B40007F;
	mov.b32 	%r40, %f164;
	shl.b32 	%r41, %r40, 23;
	mov.b32 	%f165, %r41;
	ex2.approx.ftz.f32 	%f166, %f163;
	mul.f32 	%f167, %f166, %f165;
	neg.f32 	%f168, %f110;
	fma.rn.f32 	%f169, %f168, %f110, %f151;
	fma.rn.f32 	%f170, %f167, %f169, %f167;
	mul.f32 	%f171, %f150, %f170;
	setp.gt.f32 	%p18, %f110, 0f4120E148;
	selp.f32 	%f172, 0f00000000, %f171, %p18;
	setp.lt.f32 	%p19, %f9, 0f00000000;
	sub.f32 	%f173, %f143, %f172;
	selp.f32 	%f334, %f173, %f172, %p19;
	setp.geu.f32 	%p20, %f105, 0fBF800000;
	@%p20 bra 	$L__BB12_12;

	sub.f32 	%f174, %f7, %f9;
	add.rn.f32 	%f175, %f174, %f8;
	mul.f32 	%f176, %f9, 0fC0000000;
	mul.f32 	%f177, %f176, %f334;
	fma.rn.f32 	%f334, %f177, %f175, %f334;

$L__BB12_12:
	mul.f32 	%f178, %f334, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f178;}

	// end inline asm
	st.global.u16 	[%rd11], %rs10;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p21, %rd7, %rd28;
	@%p21 bra 	$L__BB12_10;

$L__BB12_32:
	ret;

}
	// .globl	uabs_f16
.visible .entry uabs_f16(
	.param .u64 uabs_f16_param_0,
	.param .u64 uabs_f16_param_1,
	.param .u64 uabs_f16_param_2,
	.param .u64 uabs_f16_param_3,
	.param .u64 uabs_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<75>;


	ld.param.u64 	%rd28, [uabs_f16_param_0];
	ld.param.u64 	%rd29, [uabs_f16_param_1];
	ld.param.u64 	%rd31, [uabs_f16_param_2];
	ld.param.u64 	%rd30, [uabs_f16_param_3];
	ld.param.u64 	%rd32, [uabs_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB13_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r38, 0;

$L__BB13_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd34, %r24;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB13_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd43, %r38;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB13_2;

$L__BB13_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p21 bra 	$L__BB13_9;
	bra.uni 	$L__BB13_5;

$L__BB13_9:
	setp.ge.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB13_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB13_20;

$L__BB13_11:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB13_12:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd49, %r31;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd13, %r43;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB13_14;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB13_15;

$L__BB13_14:
	cvt.u32.u64 	%r32, %rd15;
	cvt.u32.u64 	%r33, %rd13;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd71, %r34;
	cvt.u64.u32 	%rd72, %r36;

$L__BB13_15:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r37, %rd58;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd71;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd59, %r42;
	setp.lt.u64 	%p15, %rd59, %rd29;
	@%p15 bra 	$L__BB13_12;

	setp.eq.s64 	%p16, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd60;
	@%p16 bra 	$L__BB13_18;

	mul.wide.u32 	%rd61, %r44, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs14, [%rd62];
	bra.uni 	$L__BB13_19;

$L__BB13_18:
	ld.global.u16 	%rs14, [%rd22];

$L__BB13_19:
	// begin inline asm
	{abs.f16 %rs8,%rs14;
}
	// end inline asm
	st.global.u16 	[%rd22], %rs8;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd28;
	@%p17 bra 	$L__BB13_11;
	bra.uni 	$L__BB13_23;

$L__BB13_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB13_23;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB13_8;

$L__BB13_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs5, [%rd45];
	// begin inline asm
	{abs.f16 %rs4,%rs5;
}
	// end inline asm
	add.s64 	%rd46, %rd1, %rd44;
	st.global.u16 	[%rd46], %rs4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd28;
	@%p10 bra 	$L__BB13_7;
	bra.uni 	$L__BB13_23;

$L__BB13_8:
	shl.b64 	%rd47, %rd7, 1;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u16 	%rs7, [%rd48];
	// begin inline asm
	{abs.f16 %rs6,%rs7;
}
	// end inline asm
	st.global.u16 	[%rd48], %rs6;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB13_8;
	bra.uni 	$L__BB13_23;

$L__BB13_20:
	setp.eq.s64 	%p18, %rd30, 0;
	@%p18 bra 	$L__BB13_22;

$L__BB13_21:
	ld.global.u16 	%rs11, [%rd2];
	// begin inline asm
	{abs.f16 %rs10,%rs11;
}
	// end inline asm
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u16 	[%rd64], %rs10;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd28;
	@%p19 bra 	$L__BB13_21;
	bra.uni 	$L__BB13_23;

$L__BB13_22:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u16 	%rs13, [%rd66];
	// begin inline asm
	{abs.f16 %rs12,%rs13;
}
	// end inline asm
	st.global.u16 	[%rd66], %rs12;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB13_22;

$L__BB13_23:
	ret;

}
	// .globl	usqr_f16
.visible .entry usqr_f16(
	.param .u64 usqr_f16_param_0,
	.param .u64 usqr_f16_param_1,
	.param .u64 usqr_f16_param_2,
	.param .u64 usqr_f16_param_3,
	.param .u64 usqr_f16_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [usqr_f16_param_0];
	ld.param.u64 	%rd27, [usqr_f16_param_1];
	ld.param.u64 	%rd29, [usqr_f16_param_2];
	ld.param.u64 	%rd28, [usqr_f16_param_3];
	ld.param.u64 	%rd30, [usqr_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB14_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB14_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB14_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB14_2;

$L__BB14_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB14_8;
	bra.uni 	$L__BB14_5;

$L__BB14_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB14_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB14_16;

$L__BB14_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB14_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB14_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB14_14;

$L__BB14_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB14_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB14_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 2;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 1;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.u16 	%rs5, [%rd60];
	// begin inline asm
	{mul.f16 %rs4,%rs5,%rs5;
}
	// end inline asm
	st.global.u16 	[%rd59], %rs4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_19;

$L__BB14_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB14_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB14_7:
	shl.b64 	%rd42, %rd7, 1;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.u16 	%rs2, [%rd43];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	add.s64 	%rd44, %rd1, %rd42;
	st.global.u16 	[%rd44], %rs1;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB14_7;

$L__BB14_19:
	ret;

$L__BB14_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB14_18;

$L__BB14_17:
	ld.global.u16 	%rs8, [%rd2];
	// begin inline asm
	{mul.f16 %rs7,%rs8,%rs8;
}
	// end inline asm
	shl.b64 	%rd61, %rd7, 1;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.u16 	[%rd62], %rs7;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB14_17;
	bra.uni 	$L__BB14_19;

$L__BB14_18:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.u16 	%rs11, [%rd64];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs11;
}
	// end inline asm
	st.global.u16 	[%rd64], %rs10;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB14_18;
	bra.uni 	$L__BB14_19;

}
	// .globl	usqrt_f16
.visible .entry usqrt_f16(
	.param .u64 usqrt_f16_param_0,
	.param .u64 usqrt_f16_param_1,
	.param .u64 usqrt_f16_param_2,
	.param .u64 usqrt_f16_param_3,
	.param .u64 usqrt_f16_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<75>;


	ld.param.u64 	%rd28, [usqrt_f16_param_0];
	ld.param.u64 	%rd29, [usqrt_f16_param_1];
	ld.param.u64 	%rd31, [usqrt_f16_param_2];
	ld.param.u64 	%rd30, [usqrt_f16_param_3];
	ld.param.u64 	%rd32, [usqrt_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p21, %p2;
	@%p3 bra 	$L__BB15_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r38, 0;

$L__BB15_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd34, %r24;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p21, -1;
	@%p5 bra 	$L__BB15_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd43, %r38;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p21, %p2;
	@%p7 bra 	$L__BB15_2;

$L__BB15_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p21 bra 	$L__BB15_9;
	bra.uni 	$L__BB15_5;

$L__BB15_9:
	setp.ge.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB15_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB15_20;

$L__BB15_11:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB15_12:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd49, %r31;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd13, %r43;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd14, %rd3, %rd52;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd53, %rd15, -4294967296;
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB15_14;

	div.u64 	%rd71, %rd13, %rd15;
	mul.lo.s64 	%rd54, %rd71, %rd15;
	sub.s64 	%rd72, %rd13, %rd54;
	bra.uni 	$L__BB15_15;

$L__BB15_14:
	cvt.u32.u64 	%r32, %rd15;
	cvt.u32.u64 	%r33, %rd13;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd71, %r34;
	cvt.u64.u32 	%rd72, %r36;

$L__BB15_15:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd14, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd72;
	cvt.u32.u64 	%r37, %rd58;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd71;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd59, %r42;
	setp.lt.u64 	%p15, %rd59, %rd29;
	@%p15 bra 	$L__BB15_12;

	setp.eq.s64 	%p16, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd22, %rd1, %rd60;
	@%p16 bra 	$L__BB15_18;

	mul.wide.u32 	%rd61, %r44, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs14, [%rd62];
	bra.uni 	$L__BB15_19;

$L__BB15_18:
	ld.global.u16 	%rs14, [%rd22];

$L__BB15_19:
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs14;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs8,r;     
}
	// end inline asm
	st.global.u16 	[%rd22], %rs8;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p17, %rd7, %rd28;
	@%p17 bra 	$L__BB15_11;
	bra.uni 	$L__BB15_23;

$L__BB15_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB15_23;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB15_8;

$L__BB15_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs5, [%rd45];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs5;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs4,r;     
}
	// end inline asm
	add.s64 	%rd46, %rd1, %rd44;
	st.global.u16 	[%rd46], %rs4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd28;
	@%p10 bra 	$L__BB15_7;
	bra.uni 	$L__BB15_23;

$L__BB15_8:
	shl.b64 	%rd47, %rd7, 1;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u16 	%rs7, [%rd48];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs7;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs6,r;     
}
	// end inline asm
	st.global.u16 	[%rd48], %rs6;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB15_8;
	bra.uni 	$L__BB15_23;

$L__BB15_20:
	setp.eq.s64 	%p18, %rd30, 0;
	@%p18 bra 	$L__BB15_22;

$L__BB15_21:
	ld.global.u16 	%rs11, [%rd2];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs11;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs10,r;     
}
	// end inline asm
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u16 	[%rd64], %rs10;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd28;
	@%p19 bra 	$L__BB15_21;
	bra.uni 	$L__BB15_23;

$L__BB15_22:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u16 	%rs13, [%rd66];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs13;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs12,r;     
}
	// end inline asm
	st.global.u16 	[%rd66], %rs12;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB15_22;

$L__BB15_23:
	ret;

}
	// .globl	ugelu_f16
.visible .entry ugelu_f16(
	.param .u64 ugelu_f16_param_0,
	.param .u64 ugelu_f16_param_1,
	.param .u64 ugelu_f16_param_2,
	.param .u64 ugelu_f16_param_3,
	.param .u64 ugelu_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<137>;
	.reg .f32 	%f<105>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<17>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd28, [ugelu_f16_param_0];
	ld.param.u64 	%rd29, [ugelu_f16_param_1];
	ld.param.u64 	%rd31, [ugelu_f16_param_2];
	ld.param.u64 	%rd30, [ugelu_f16_param_3];
	ld.param.u64 	%rd32, [ugelu_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB16_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r52, 0;

$L__BB16_2:
	not.b32 	%r22, %r52;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd65, %rd39;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB16_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd65, %rd42, %rd65;
	add.s32 	%r52, %r52, 1;
	cvt.u64.u32 	%rd43, %r52;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB16_2;

$L__BB16_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p28 bra 	$L__BB16_15;
	bra.uni 	$L__BB16_5;

$L__BB16_15:
	setp.ge.u64 	%p16, %rd7, %rd28;
	@%p16 bra 	$L__BB16_36;

	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r34;
	@%p3 bra 	$L__BB16_29;

$L__BB16_17:
	mov.u32 	%r56, 0;
	mov.u32 	%r57, %r4;
	mov.u32 	%r58, %r56;

$L__BB16_18:
	not.b32 	%r37, %r56;
	cvt.u64.u32 	%rd49, %r37;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd14, %r57;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd15, %rd3, %rd52;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd53, %rd16, -4294967296;
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB16_20;

	div.u64 	%rd69, %rd14, %rd16;
	mul.lo.s64 	%rd54, %rd69, %rd16;
	sub.s64 	%rd70, %rd14, %rd54;
	bra.uni 	$L__BB16_21;

$L__BB16_20:
	cvt.u32.u64 	%r38, %rd16;
	cvt.u32.u64 	%r39, %rd14;
	div.u32 	%r40, %r39, %r38;
	mul.lo.s32 	%r41, %r40, %r38;
	sub.s32 	%r42, %r39, %r41;
	cvt.u64.u32 	%rd69, %r40;
	cvt.u64.u32 	%rd70, %r42;

$L__BB16_21:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd15, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd70;
	cvt.u32.u64 	%r43, %rd58;
	add.s32 	%r58, %r58, %r43;
	cvt.u32.u64 	%r57, %rd69;
	add.s32 	%r56, %r56, 1;
	cvt.u64.u32 	%rd59, %r56;
	setp.lt.u64 	%p19, %rd59, %rd29;
	@%p19 bra 	$L__BB16_18;

	setp.eq.s64 	%p20, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd60;
	@%p20 bra 	$L__BB16_24;

	mul.wide.u32 	%rd62, %r58, 2;
	add.s64 	%rd63, %rd2, %rd62;
	ld.global.u16 	%rs135, [%rd63];
	bra.uni 	$L__BB16_25;

$L__BB16_24:
	ld.global.u16 	%rs135, [%rd23];

$L__BB16_25:
	// begin inline asm
	{mul.f16 %rs75,%rs135,%rs135;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs78,%rs75,%rs135;
}
	// end inline asm
	mov.f64 	%fd9, 0d3FA6E4E26D4801F7;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs81, %fd9;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs82,%rs81,%rs78;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs85,%rs135,%rs82;
}
	// end inline asm
	mov.f64 	%fd10, 0d3FE0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs88, %fd10;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs89,%rs88,%rs135;
}
	// end inline asm
	mov.f64 	%fd11, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs92, %fd11;}

	// end inline asm
	mov.f64 	%fd12, 0d3FE9884533D43651;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs93, %fd12;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs94,%rs93,%rs85;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f61, %rs94;}

	// end inline asm
	abs.f32 	%f12, %f61;
	setp.ltu.f32 	%p21, %f12, 0f3F19999A;
	@%p21 bra 	$L__BB16_27;
	bra.uni 	$L__BB16_26;

$L__BB16_27:
	mul.f32 	%f70, %f61, %f61;
	mov.f32 	%f71, 0fBD563CAE;
	mov.f32 	%f72, 0f3C80F082;
	fma.rn.f32 	%f73, %f72, %f70, %f71;
	mov.f32 	%f74, 0f3E085941;
	fma.rn.f32 	%f75, %f73, %f70, %f74;
	mov.f32 	%f76, 0fBEAAA9ED;
	fma.rn.f32 	%f77, %f75, %f70, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f79, %f77, %f70, %f78;
	fma.rn.f32 	%f103, %f79, %f61, %f61;
	bra.uni 	$L__BB16_28;

$L__BB16_26:
	mul.f32 	%f62, %f12, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f63, %f62;
	add.f32 	%f64, %f63, 0f3F800000;
	mov.f32 	%f65, 0f3F800000;
	rcp.approx.ftz.f32 	%f66, %f64;
	mov.f32 	%f67, 0fC0000000;
	fma.rn.f32 	%f68, %f66, %f67, %f65;
	setp.ge.f32 	%p22, %f12, 0f41102CB4;
	selp.f32 	%f69, 0f3F800000, %f68, %p22;
	mov.b32 	%r44, %f69;
	mov.b32 	%r45, %f61;
	and.b32  	%r46, %r45, -2147483648;
	or.b32  	%r47, %r46, %r44;
	mov.b32 	%f103, %r47;

$L__BB16_28:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs98, %f103;}

	// end inline asm
	// begin inline asm
	{add.f16 %rs99,%rs92,%rs98;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs102,%rs89,%rs99;
}
	// end inline asm
	st.global.u16 	[%rd23], %rs102;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd28;
	@%p23 bra 	$L__BB16_17;
	bra.uni 	$L__BB16_36;

$L__BB16_29:
	shl.b64 	%rd64, %rd7, 1;
	add.s64 	%rd26, %rd1, %rd64;
	setp.eq.s64 	%p24, %rd30, 0;
	@%p24 bra 	$L__BB16_31;

	ld.global.u16 	%rs136, [%rd2];
	bra.uni 	$L__BB16_32;

$L__BB16_31:
	ld.global.u16 	%rs136, [%rd26];

$L__BB16_32:
	// begin inline asm
	{mul.f16 %rs105,%rs136,%rs136;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs108,%rs105,%rs136;
}
	// end inline asm
	mov.f64 	%fd13, 0d3FA6E4E26D4801F7;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs111, %fd13;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs112,%rs111,%rs108;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs115,%rs136,%rs112;
}
	// end inline asm
	mov.f64 	%fd14, 0d3FE0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs118, %fd14;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs119,%rs118,%rs136;
}
	// end inline asm
	mov.f64 	%fd15, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs122, %fd15;}

	// end inline asm
	mov.f64 	%fd16, 0d3FE9884533D43651;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs123, %fd16;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs124,%rs123,%rs115;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f81, %rs124;}

	// end inline asm
	abs.f32 	%f17, %f81;
	setp.ltu.f32 	%p25, %f17, 0f3F19999A;
	@%p25 bra 	$L__BB16_34;
	bra.uni 	$L__BB16_33;

$L__BB16_34:
	mul.f32 	%f90, %f81, %f81;
	mov.f32 	%f91, 0fBD563CAE;
	mov.f32 	%f92, 0f3C80F082;
	fma.rn.f32 	%f93, %f92, %f90, %f91;
	mov.f32 	%f94, 0f3E085941;
	fma.rn.f32 	%f95, %f93, %f90, %f94;
	mov.f32 	%f96, 0fBEAAA9ED;
	fma.rn.f32 	%f97, %f95, %f90, %f96;
	mov.f32 	%f98, 0f00000000;
	fma.rn.f32 	%f99, %f97, %f90, %f98;
	fma.rn.f32 	%f104, %f99, %f81, %f81;
	bra.uni 	$L__BB16_35;

$L__BB16_33:
	mul.f32 	%f82, %f17, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f83, %f82;
	add.f32 	%f84, %f83, 0f3F800000;
	mov.f32 	%f85, 0f3F800000;
	rcp.approx.ftz.f32 	%f86, %f84;
	mov.f32 	%f87, 0fC0000000;
	fma.rn.f32 	%f88, %f86, %f87, %f85;
	setp.ge.f32 	%p26, %f17, 0f41102CB4;
	selp.f32 	%f89, 0f3F800000, %f88, %p26;
	mov.b32 	%r48, %f89;
	mov.b32 	%r49, %f81;
	and.b32  	%r50, %r49, -2147483648;
	or.b32  	%r51, %r50, %r48;
	mov.b32 	%f104, %r51;

$L__BB16_35:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs128, %f104;}

	// end inline asm
	// begin inline asm
	{add.f16 %rs129,%rs122,%rs128;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs132,%rs119,%rs129;
}
	// end inline asm
	st.global.u16 	[%rd26], %rs132;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p27, %rd7, %rd28;
	@%p27 bra 	$L__BB16_29;
	bra.uni 	$L__BB16_36;

$L__BB16_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB16_36;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB16_11;

$L__BB16_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs16, [%rd45];
	// begin inline asm
	{mul.f16 %rs15,%rs16,%rs16;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs18,%rs15,%rs16;
}
	// end inline asm
	mov.f64 	%fd1, 0d3FA6E4E26D4801F7;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs21, %fd1;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs22,%rs21,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs16,%rs22;
}
	// end inline asm
	mov.f64 	%fd2, 0d3FE0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs28, %fd2;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs29,%rs28,%rs16;
}
	// end inline asm
	mov.f64 	%fd3, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs32, %fd3;}

	// end inline asm
	mov.f64 	%fd4, 0d3FE9884533D43651;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs33, %fd4;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs34,%rs33,%rs25;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs34;}

	// end inline asm
	abs.f32 	%f2, %f21;
	setp.ltu.f32 	%p10, %f2, 0f3F19999A;
	@%p10 bra 	$L__BB16_9;
	bra.uni 	$L__BB16_8;

$L__BB16_9:
	mul.f32 	%f30, %f21, %f21;
	mov.f32 	%f31, 0fBD563CAE;
	mov.f32 	%f32, 0f3C80F082;
	fma.rn.f32 	%f33, %f32, %f30, %f31;
	mov.f32 	%f34, 0f3E085941;
	fma.rn.f32 	%f35, %f33, %f30, %f34;
	mov.f32 	%f36, 0fBEAAA9ED;
	fma.rn.f32 	%f37, %f35, %f30, %f36;
	mov.f32 	%f38, 0f00000000;
	fma.rn.f32 	%f39, %f37, %f30, %f38;
	fma.rn.f32 	%f101, %f39, %f21, %f21;
	bra.uni 	$L__BB16_10;

$L__BB16_8:
	mul.f32 	%f22, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f23, %f22;
	add.f32 	%f24, %f23, 0f3F800000;
	mov.f32 	%f25, 0f3F800000;
	rcp.approx.ftz.f32 	%f26, %f24;
	mov.f32 	%f27, 0fC0000000;
	fma.rn.f32 	%f28, %f26, %f27, %f25;
	setp.ge.f32 	%p11, %f2, 0f41102CB4;
	selp.f32 	%f29, 0f3F800000, %f28, %p11;
	mov.b32 	%r26, %f29;
	mov.b32 	%r27, %f21;
	and.b32  	%r28, %r27, -2147483648;
	or.b32  	%r29, %r28, %r26;
	mov.b32 	%f101, %r29;

$L__BB16_10:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs38, %f101;}

	// end inline asm
	// begin inline asm
	{add.f16 %rs39,%rs32,%rs38;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs42,%rs29,%rs39;
}
	// end inline asm
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs42;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p12, %rd7, %rd28;
	@%p12 bra 	$L__BB16_7;
	bra.uni 	$L__BB16_36;

$L__BB16_11:
	shl.b64 	%rd48, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd48;
	ld.global.u16 	%rs46, [%rd11];
	// begin inline asm
	{mul.f16 %rs45,%rs46,%rs46;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs48,%rs45,%rs46;
}
	// end inline asm
	mov.f64 	%fd5, 0d3FA6E4E26D4801F7;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs51, %fd5;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs52,%rs51,%rs48;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs55,%rs46,%rs52;
}
	// end inline asm
	mov.f64 	%fd6, 0d3FE0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs58, %fd6;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs59,%rs58,%rs46;
}
	// end inline asm
	mov.f64 	%fd7, 0d3FF0000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs62, %fd7;}

	// end inline asm
	mov.f64 	%fd8, 0d3FE9884533D43651;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs63, %fd8;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs64,%rs63,%rs55;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f41, %rs64;}

	// end inline asm
	abs.f32 	%f7, %f41;
	setp.ltu.f32 	%p13, %f7, 0f3F19999A;
	@%p13 bra 	$L__BB16_13;
	bra.uni 	$L__BB16_12;

$L__BB16_13:
	mul.f32 	%f50, %f41, %f41;
	mov.f32 	%f51, 0fBD563CAE;
	mov.f32 	%f52, 0f3C80F082;
	fma.rn.f32 	%f53, %f52, %f50, %f51;
	mov.f32 	%f54, 0f3E085941;
	fma.rn.f32 	%f55, %f53, %f50, %f54;
	mov.f32 	%f56, 0fBEAAA9ED;
	fma.rn.f32 	%f57, %f55, %f50, %f56;
	mov.f32 	%f58, 0f00000000;
	fma.rn.f32 	%f59, %f57, %f50, %f58;
	fma.rn.f32 	%f102, %f59, %f41, %f41;
	bra.uni 	$L__BB16_14;

$L__BB16_12:
	mul.f32 	%f42, %f7, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f43, %f42;
	add.f32 	%f44, %f43, 0f3F800000;
	mov.f32 	%f45, 0f3F800000;
	rcp.approx.ftz.f32 	%f46, %f44;
	mov.f32 	%f47, 0fC0000000;
	fma.rn.f32 	%f48, %f46, %f47, %f45;
	setp.ge.f32 	%p14, %f7, 0f41102CB4;
	selp.f32 	%f49, 0f3F800000, %f48, %p14;
	mov.b32 	%r30, %f49;
	mov.b32 	%r31, %f41;
	and.b32  	%r32, %r31, -2147483648;
	or.b32  	%r33, %r32, %r30;
	mov.b32 	%f102, %r33;

$L__BB16_14:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f102;}

	// end inline asm
	// begin inline asm
	{add.f16 %rs69,%rs62,%rs68;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs72,%rs59,%rs69;
}
	// end inline asm
	st.global.u16 	[%rd11], %rs72;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd28;
	@%p15 bra 	$L__BB16_11;

$L__BB16_36:
	ret;

}
	// .globl	ugelu_erf_f16
.visible .entry ugelu_erf_f16(
	.param .u64 ugelu_erf_f16_param_0,
	.param .u64 ugelu_erf_f16_param_1,
	.param .u64 ugelu_erf_f16_param_2,
	.param .u64 ugelu_erf_f16_param_3,
	.param .u64 ugelu_erf_f16_param_4
)
{
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<33>;
	.reg .f32 	%f<337>;
	.reg .b32 	%r<76>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd28, [ugelu_erf_f16_param_0];
	ld.param.u64 	%rd29, [ugelu_erf_f16_param_1];
	ld.param.u64 	%rd31, [ugelu_erf_f16_param_2];
	ld.param.u64 	%rd30, [ugelu_erf_f16_param_3];
	ld.param.u64 	%rd32, [ugelu_erf_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p40, %p2;
	@%p3 bra 	$L__BB17_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r68, 0;

$L__BB17_2:
	not.b32 	%r22, %r68;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd64, %rd39;
	mov.pred 	%p40, -1;
	@%p5 bra 	$L__BB17_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd64, %rd42, %rd64;
	add.s32 	%r68, %r68, 1;
	cvt.u64.u32 	%rd43, %r68;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p40, %p2;
	@%p7 bra 	$L__BB17_2;

$L__BB17_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p40 bra 	$L__BB17_13;
	bra.uni 	$L__BB17_5;

$L__BB17_13:
	setp.ge.u64 	%p22, %rd7, %rd28;
	@%p22 bra 	$L__BB17_32;

	mov.u32 	%r42, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r42;
	@%p3 bra 	$L__BB17_26;

$L__BB17_15:
	mov.u32 	%r72, 0;
	mov.u32 	%r73, %r4;
	mov.u32 	%r74, %r72;

$L__BB17_16:
	not.b32 	%r45, %r72;
	cvt.u64.u32 	%rd49, %r45;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd14, %r73;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd15, %rd3, %rd52;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd53, %rd16, -4294967296;
	setp.eq.s64 	%p24, %rd53, 0;
	@%p24 bra 	$L__BB17_18;

	div.u64 	%rd68, %rd14, %rd16;
	mul.lo.s64 	%rd54, %rd68, %rd16;
	sub.s64 	%rd69, %rd14, %rd54;
	bra.uni 	$L__BB17_19;

$L__BB17_18:
	cvt.u32.u64 	%r46, %rd16;
	cvt.u32.u64 	%r47, %rd14;
	div.u32 	%r48, %r47, %r46;
	mul.lo.s32 	%r49, %r48, %r46;
	sub.s32 	%r50, %r47, %r49;
	cvt.u64.u32 	%rd68, %r48;
	cvt.u64.u32 	%rd69, %r50;

$L__BB17_19:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd15, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd69;
	cvt.u32.u64 	%r51, %rd58;
	add.s32 	%r74, %r74, %r51;
	cvt.u32.u64 	%r73, %rd68;
	add.s32 	%r72, %r72, 1;
	cvt.u64.u32 	%rd59, %r72;
	setp.lt.u64 	%p25, %rd59, %rd29;
	@%p25 bra 	$L__BB17_16;

	setp.eq.s64 	%p26, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd60;
	@%p26 bra 	$L__BB17_22;

	mul.wide.u32 	%rd61, %r74, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs31, [%rd62];
	bra.uni 	$L__BB17_23;

$L__BB17_22:
	ld.global.u16 	%rs31, [%rd23];

$L__BB17_23:
	// begin inline asm
	{  cvt.f32.f16 %f179, %rs31;}

	// end inline asm
	abs.f32 	%f180, %f179;
	setp.gt.f32 	%p27, %f180, 0f41680000;
	mov.b32 	%r52, %f179;
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r53, 1097334784;
	mov.b32 	%f181, %r54;
	selp.f32 	%f182, %f181, %f179, %p27;
	mov.f32 	%f183, 0fBF3504F3;
	mul.rn.f32 	%f13, %f182, %f183;
	neg.f32 	%f184, %f13;
	fma.rn.f32 	%f185, %f182, %f183, %f184;
	mov.f32 	%f186, 0fB24FE77A;
	fma.rn.f32 	%f14, %f182, %f186, %f185;
	add.rn.f32 	%f15, %f13, %f14;
	abs.f32 	%f187, %f15;
	add.f32 	%f188, %f187, 0fC0800000;
	mov.f32 	%f189, 0fC0800000;
	add.f32 	%f190, %f187, 0f40800000;
	rcp.approx.ftz.f32 	%f191, %f190;
	mul.rn.f32 	%f192, %f188, %f191;
	add.f32 	%f193, %f192, 0f3F800000;
	mov.f32 	%f194, 0f3F800000;
	fma.rn.f32 	%f195, %f189, %f193, %f187;
	neg.f32 	%f196, %f192;
	fma.rn.f32 	%f197, %f196, %f187, %f195;
	fma.rn.f32 	%f198, %f191, %f197, %f192;
	mov.f32 	%f199, 0f3BE6E05B;
	mov.f32 	%f200, 0f3A69A091;
	fma.rn.f32 	%f201, %f200, %f198, %f199;
	mov.f32 	%f202, 0fBC81FB4B;
	fma.rn.f32 	%f203, %f201, %f198, %f202;
	mov.f32 	%f204, 0f3D15373B;
	fma.rn.f32 	%f205, %f203, %f198, %f204;
	mov.f32 	%f206, 0fBD887C5A;
	fma.rn.f32 	%f207, %f205, %f198, %f206;
	mov.f32 	%f208, 0f3DC021D5;
	fma.rn.f32 	%f209, %f207, %f198, %f208;
	mov.f32 	%f210, 0fBDCED424;
	fma.rn.f32 	%f211, %f209, %f198, %f210;
	mov.f32 	%f212, 0f3D8B74DE;
	fma.rn.f32 	%f213, %f211, %f198, %f212;
	mov.f32 	%f214, 0f3C7BF170;
	fma.rn.f32 	%f215, %f213, %f198, %f214;
	mov.f32 	%f216, 0fBE0EF8D4;
	fma.rn.f32 	%f217, %f215, %f198, %f216;
	mov.f32 	%f218, 0f3F9DD2C9;
	fma.rn.f32 	%f219, %f217, %f198, %f218;
	mov.f32 	%f220, 0f40000000;
	fma.rn.f32 	%f221, %f220, %f187, %f194;
	rcp.approx.ftz.f32 	%f222, %f221;
	mul.rn.f32 	%f223, %f219, %f222;
	mul.f32 	%f224, %f223, 0fC0000000;
	fma.rn.f32 	%f225, %f187, %f224, %f219;
	sub.f32 	%f226, %f225, %f223;
	fma.rn.f32 	%f227, %f226, %f222, %f223;
	mul.f32 	%f228, %f187, %f187;
	neg.f32 	%f229, %f228;
	mov.f32 	%f230, 0f3FB8AA3B;
	mul.rn.f32 	%f231, %f229, %f230;
	cvt.rzi.f32.f32 	%f232, %f231;
	abs.f32 	%f233, %f232;
	setp.gt.f32 	%p28, %f233, 0f42FC0000;
	mov.b32 	%r55, %f232;
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r56, 1123811328;
	mov.b32 	%f234, %r57;
	selp.f32 	%f235, %f234, %f232, %p28;
	mov.f32 	%f236, 0fBF317218;
	fma.rn.f32 	%f237, %f235, %f236, %f229;
	mov.f32 	%f238, 0f3102E308;
	fma.rn.f32 	%f239, %f235, %f238, %f237;
	mul.f32 	%f240, %f239, 0f3FB8AA3B;
	add.f32 	%f241, %f235, 0f4B40007F;
	mov.b32 	%r58, %f241;
	shl.b32 	%r59, %r58, 23;
	mov.b32 	%f242, %r59;
	ex2.approx.ftz.f32 	%f243, %f240;
	mul.f32 	%f244, %f243, %f242;
	neg.f32 	%f245, %f187;
	fma.rn.f32 	%f246, %f245, %f187, %f228;
	fma.rn.f32 	%f247, %f244, %f246, %f244;
	mul.f32 	%f248, %f227, %f247;
	setp.gt.f32 	%p29, %f187, 0f4120E148;
	selp.f32 	%f249, 0f00000000, %f248, %p29;
	setp.lt.f32 	%p30, %f15, 0f00000000;
	sub.f32 	%f250, %f220, %f249;
	selp.f32 	%f335, %f250, %f249, %p30;
	setp.geu.f32 	%p31, %f182, 0fBF800000;
	@%p31 bra 	$L__BB17_25;

	sub.f32 	%f251, %f13, %f15;
	add.rn.f32 	%f252, %f251, %f14;
	mul.f32 	%f253, %f15, 0fC0000000;
	mul.f32 	%f254, %f253, %f335;
	fma.rn.f32 	%f335, %f254, %f252, %f335;

$L__BB17_25:
	mul.f32 	%f255, %f335, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f255;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs23,%rs31,%rs22;
}
	// end inline asm
	st.global.u16 	[%rd23], %rs23;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p32, %rd7, %rd28;
	@%p32 bra 	$L__BB17_15;
	bra.uni 	$L__BB17_32;

$L__BB17_26:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd26, %rd1, %rd63;
	setp.eq.s64 	%p33, %rd30, 0;
	@%p33 bra 	$L__BB17_28;

	ld.global.u16 	%rs32, [%rd2];
	bra.uni 	$L__BB17_29;

$L__BB17_28:
	ld.global.u16 	%rs32, [%rd26];

$L__BB17_29:
	// begin inline asm
	{  cvt.f32.f16 %f256, %rs32;}

	// end inline asm
	abs.f32 	%f257, %f256;
	setp.gt.f32 	%p34, %f257, 0f41680000;
	mov.b32 	%r60, %f256;
	and.b32  	%r61, %r60, -2147483648;
	or.b32  	%r62, %r61, 1097334784;
	mov.b32 	%f258, %r62;
	selp.f32 	%f259, %f258, %f256, %p34;
	mov.f32 	%f260, 0fBF3504F3;
	mul.rn.f32 	%f19, %f259, %f260;
	neg.f32 	%f261, %f19;
	fma.rn.f32 	%f262, %f259, %f260, %f261;
	mov.f32 	%f263, 0fB24FE77A;
	fma.rn.f32 	%f20, %f259, %f263, %f262;
	add.rn.f32 	%f21, %f19, %f20;
	abs.f32 	%f264, %f21;
	add.f32 	%f265, %f264, 0fC0800000;
	mov.f32 	%f266, 0fC0800000;
	add.f32 	%f267, %f264, 0f40800000;
	rcp.approx.ftz.f32 	%f268, %f267;
	mul.rn.f32 	%f269, %f265, %f268;
	add.f32 	%f270, %f269, 0f3F800000;
	mov.f32 	%f271, 0f3F800000;
	fma.rn.f32 	%f272, %f266, %f270, %f264;
	neg.f32 	%f273, %f269;
	fma.rn.f32 	%f274, %f273, %f264, %f272;
	fma.rn.f32 	%f275, %f268, %f274, %f269;
	mov.f32 	%f276, 0f3BE6E05B;
	mov.f32 	%f277, 0f3A69A091;
	fma.rn.f32 	%f278, %f277, %f275, %f276;
	mov.f32 	%f279, 0fBC81FB4B;
	fma.rn.f32 	%f280, %f278, %f275, %f279;
	mov.f32 	%f281, 0f3D15373B;
	fma.rn.f32 	%f282, %f280, %f275, %f281;
	mov.f32 	%f283, 0fBD887C5A;
	fma.rn.f32 	%f284, %f282, %f275, %f283;
	mov.f32 	%f285, 0f3DC021D5;
	fma.rn.f32 	%f286, %f284, %f275, %f285;
	mov.f32 	%f287, 0fBDCED424;
	fma.rn.f32 	%f288, %f286, %f275, %f287;
	mov.f32 	%f289, 0f3D8B74DE;
	fma.rn.f32 	%f290, %f288, %f275, %f289;
	mov.f32 	%f291, 0f3C7BF170;
	fma.rn.f32 	%f292, %f290, %f275, %f291;
	mov.f32 	%f293, 0fBE0EF8D4;
	fma.rn.f32 	%f294, %f292, %f275, %f293;
	mov.f32 	%f295, 0f3F9DD2C9;
	fma.rn.f32 	%f296, %f294, %f275, %f295;
	mov.f32 	%f297, 0f40000000;
	fma.rn.f32 	%f298, %f297, %f264, %f271;
	rcp.approx.ftz.f32 	%f299, %f298;
	mul.rn.f32 	%f300, %f296, %f299;
	mul.f32 	%f301, %f300, 0fC0000000;
	fma.rn.f32 	%f302, %f264, %f301, %f296;
	sub.f32 	%f303, %f302, %f300;
	fma.rn.f32 	%f304, %f303, %f299, %f300;
	mul.f32 	%f305, %f264, %f264;
	neg.f32 	%f306, %f305;
	mov.f32 	%f307, 0f3FB8AA3B;
	mul.rn.f32 	%f308, %f306, %f307;
	cvt.rzi.f32.f32 	%f309, %f308;
	abs.f32 	%f310, %f309;
	setp.gt.f32 	%p35, %f310, 0f42FC0000;
	mov.b32 	%r63, %f309;
	and.b32  	%r64, %r63, -2147483648;
	or.b32  	%r65, %r64, 1123811328;
	mov.b32 	%f311, %r65;
	selp.f32 	%f312, %f311, %f309, %p35;
	mov.f32 	%f313, 0fBF317218;
	fma.rn.f32 	%f314, %f312, %f313, %f306;
	mov.f32 	%f315, 0f3102E308;
	fma.rn.f32 	%f316, %f312, %f315, %f314;
	mul.f32 	%f317, %f316, 0f3FB8AA3B;
	add.f32 	%f318, %f312, 0f4B40007F;
	mov.b32 	%r66, %f318;
	shl.b32 	%r67, %r66, 23;
	mov.b32 	%f319, %r67;
	ex2.approx.ftz.f32 	%f320, %f317;
	mul.f32 	%f321, %f320, %f319;
	neg.f32 	%f322, %f264;
	fma.rn.f32 	%f323, %f322, %f264, %f305;
	fma.rn.f32 	%f324, %f321, %f323, %f321;
	mul.f32 	%f325, %f304, %f324;
	setp.gt.f32 	%p36, %f264, 0f4120E148;
	selp.f32 	%f326, 0f00000000, %f325, %p36;
	setp.lt.f32 	%p37, %f21, 0f00000000;
	sub.f32 	%f327, %f297, %f326;
	selp.f32 	%f336, %f327, %f326, %p37;
	setp.geu.f32 	%p38, %f259, 0fBF800000;
	@%p38 bra 	$L__BB17_31;

	sub.f32 	%f328, %f19, %f21;
	add.rn.f32 	%f329, %f328, %f20;
	mul.f32 	%f330, %f21, 0fC0000000;
	mul.f32 	%f331, %f330, %f336;
	fma.rn.f32 	%f336, %f331, %f329, %f336;

$L__BB17_31:
	mul.f32 	%f332, %f336, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f332;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs28,%rs32,%rs27;
}
	// end inline asm
	st.global.u16 	[%rd26], %rs28;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p39, %rd7, %rd28;
	@%p39 bra 	$L__BB17_26;
	bra.uni 	$L__BB17_32;

$L__BB17_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB17_32;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB17_10;

$L__BB17_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs1, [%rd45];
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs1;}

	// end inline asm
	abs.f32 	%f26, %f25;
	setp.gt.f32 	%p10, %f26, 0f41680000;
	mov.b32 	%r26, %f25;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1097334784;
	mov.b32 	%f27, %r28;
	selp.f32 	%f28, %f27, %f25, %p10;
	mov.f32 	%f29, 0fBF3504F3;
	mul.rn.f32 	%f1, %f28, %f29;
	neg.f32 	%f30, %f1;
	fma.rn.f32 	%f31, %f28, %f29, %f30;
	mov.f32 	%f32, 0fB24FE77A;
	fma.rn.f32 	%f2, %f28, %f32, %f31;
	add.rn.f32 	%f3, %f1, %f2;
	abs.f32 	%f33, %f3;
	add.f32 	%f34, %f33, 0fC0800000;
	mov.f32 	%f35, 0fC0800000;
	add.f32 	%f36, %f33, 0f40800000;
	rcp.approx.ftz.f32 	%f37, %f36;
	mul.rn.f32 	%f38, %f34, %f37;
	add.f32 	%f39, %f38, 0f3F800000;
	mov.f32 	%f40, 0f3F800000;
	fma.rn.f32 	%f41, %f35, %f39, %f33;
	neg.f32 	%f42, %f38;
	fma.rn.f32 	%f43, %f42, %f33, %f41;
	fma.rn.f32 	%f44, %f37, %f43, %f38;
	mov.f32 	%f45, 0f3BE6E05B;
	mov.f32 	%f46, 0f3A69A091;
	fma.rn.f32 	%f47, %f46, %f44, %f45;
	mov.f32 	%f48, 0fBC81FB4B;
	fma.rn.f32 	%f49, %f47, %f44, %f48;
	mov.f32 	%f50, 0f3D15373B;
	fma.rn.f32 	%f51, %f49, %f44, %f50;
	mov.f32 	%f52, 0fBD887C5A;
	fma.rn.f32 	%f53, %f51, %f44, %f52;
	mov.f32 	%f54, 0f3DC021D5;
	fma.rn.f32 	%f55, %f53, %f44, %f54;
	mov.f32 	%f56, 0fBDCED424;
	fma.rn.f32 	%f57, %f55, %f44, %f56;
	mov.f32 	%f58, 0f3D8B74DE;
	fma.rn.f32 	%f59, %f57, %f44, %f58;
	mov.f32 	%f60, 0f3C7BF170;
	fma.rn.f32 	%f61, %f59, %f44, %f60;
	mov.f32 	%f62, 0fBE0EF8D4;
	fma.rn.f32 	%f63, %f61, %f44, %f62;
	mov.f32 	%f64, 0f3F9DD2C9;
	fma.rn.f32 	%f65, %f63, %f44, %f64;
	mov.f32 	%f66, 0f40000000;
	fma.rn.f32 	%f67, %f66, %f33, %f40;
	rcp.approx.ftz.f32 	%f68, %f67;
	mul.rn.f32 	%f69, %f65, %f68;
	mul.f32 	%f70, %f69, 0fC0000000;
	fma.rn.f32 	%f71, %f33, %f70, %f65;
	sub.f32 	%f72, %f71, %f69;
	fma.rn.f32 	%f73, %f72, %f68, %f69;
	mul.f32 	%f74, %f33, %f33;
	neg.f32 	%f75, %f74;
	mov.f32 	%f76, 0f3FB8AA3B;
	mul.rn.f32 	%f77, %f75, %f76;
	cvt.rzi.f32.f32 	%f78, %f77;
	abs.f32 	%f79, %f78;
	setp.gt.f32 	%p11, %f79, 0f42FC0000;
	mov.b32 	%r29, %f78;
	and.b32  	%r30, %r29, -2147483648;
	or.b32  	%r31, %r30, 1123811328;
	mov.b32 	%f80, %r31;
	selp.f32 	%f81, %f80, %f78, %p11;
	mov.f32 	%f82, 0fBF317218;
	fma.rn.f32 	%f83, %f81, %f82, %f75;
	mov.f32 	%f84, 0f3102E308;
	fma.rn.f32 	%f85, %f81, %f84, %f83;
	mul.f32 	%f86, %f85, 0f3FB8AA3B;
	add.f32 	%f87, %f81, 0f4B40007F;
	mov.b32 	%r32, %f87;
	shl.b32 	%r33, %r32, 23;
	mov.b32 	%f88, %r33;
	ex2.approx.ftz.f32 	%f89, %f86;
	mul.f32 	%f90, %f89, %f88;
	neg.f32 	%f91, %f33;
	fma.rn.f32 	%f92, %f91, %f33, %f74;
	fma.rn.f32 	%f93, %f90, %f92, %f90;
	mul.f32 	%f94, %f73, %f93;
	setp.gt.f32 	%p12, %f33, 0f4120E148;
	selp.f32 	%f95, 0f00000000, %f94, %p12;
	setp.lt.f32 	%p13, %f3, 0f00000000;
	sub.f32 	%f96, %f66, %f95;
	selp.f32 	%f333, %f96, %f95, %p13;
	setp.geu.f32 	%p14, %f28, 0fBF800000;
	@%p14 bra 	$L__BB17_9;

	sub.f32 	%f97, %f1, %f3;
	add.rn.f32 	%f98, %f97, %f2;
	mul.f32 	%f99, %f3, 0fC0000000;
	mul.f32 	%f100, %f99, %f333;
	fma.rn.f32 	%f333, %f100, %f98, %f333;

$L__BB17_9:
	mul.f32 	%f101, %f333, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f101;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs13,%rs1,%rs12;
}
	// end inline asm
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs13;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd28;
	@%p15 bra 	$L__BB17_7;
	bra.uni 	$L__BB17_32;

$L__BB17_10:
	shl.b64 	%rd48, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd48;
	ld.global.u16 	%rs2, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f102, %rs2;}

	// end inline asm
	abs.f32 	%f103, %f102;
	setp.gt.f32 	%p16, %f103, 0f41680000;
	mov.b32 	%r34, %f102;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1097334784;
	mov.b32 	%f104, %r36;
	selp.f32 	%f105, %f104, %f102, %p16;
	mov.f32 	%f106, 0fBF3504F3;
	mul.rn.f32 	%f7, %f105, %f106;
	neg.f32 	%f107, %f7;
	fma.rn.f32 	%f108, %f105, %f106, %f107;
	mov.f32 	%f109, 0fB24FE77A;
	fma.rn.f32 	%f8, %f105, %f109, %f108;
	add.rn.f32 	%f9, %f7, %f8;
	abs.f32 	%f110, %f9;
	add.f32 	%f111, %f110, 0fC0800000;
	mov.f32 	%f112, 0fC0800000;
	add.f32 	%f113, %f110, 0f40800000;
	rcp.approx.ftz.f32 	%f114, %f113;
	mul.rn.f32 	%f115, %f111, %f114;
	add.f32 	%f116, %f115, 0f3F800000;
	mov.f32 	%f117, 0f3F800000;
	fma.rn.f32 	%f118, %f112, %f116, %f110;
	neg.f32 	%f119, %f115;
	fma.rn.f32 	%f120, %f119, %f110, %f118;
	fma.rn.f32 	%f121, %f114, %f120, %f115;
	mov.f32 	%f122, 0f3BE6E05B;
	mov.f32 	%f123, 0f3A69A091;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0fBC81FB4B;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3D15373B;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0fBD887C5A;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3DC021D5;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0fBDCED424;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3D8B74DE;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3C7BF170;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0fBE0EF8D4;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F9DD2C9;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mov.f32 	%f143, 0f40000000;
	fma.rn.f32 	%f144, %f143, %f110, %f117;
	rcp.approx.ftz.f32 	%f145, %f144;
	mul.rn.f32 	%f146, %f142, %f145;
	mul.f32 	%f147, %f146, 0fC0000000;
	fma.rn.f32 	%f148, %f110, %f147, %f142;
	sub.f32 	%f149, %f148, %f146;
	fma.rn.f32 	%f150, %f149, %f145, %f146;
	mul.f32 	%f151, %f110, %f110;
	neg.f32 	%f152, %f151;
	mov.f32 	%f153, 0f3FB8AA3B;
	mul.rn.f32 	%f154, %f152, %f153;
	cvt.rzi.f32.f32 	%f155, %f154;
	abs.f32 	%f156, %f155;
	setp.gt.f32 	%p17, %f156, 0f42FC0000;
	mov.b32 	%r37, %f155;
	and.b32  	%r38, %r37, -2147483648;
	or.b32  	%r39, %r38, 1123811328;
	mov.b32 	%f157, %r39;
	selp.f32 	%f158, %f157, %f155, %p17;
	mov.f32 	%f159, 0fBF317218;
	fma.rn.f32 	%f160, %f158, %f159, %f152;
	mov.f32 	%f161, 0f3102E308;
	fma.rn.f32 	%f162, %f158, %f161, %f160;
	mul.f32 	%f163, %f162, 0f3FB8AA3B;
	add.f32 	%f164, %f158, 0f4B40007F;
	mov.b32 	%r40, %f164;
	shl.b32 	%r41, %r40, 23;
	mov.b32 	%f165, %r41;
	ex2.approx.ftz.f32 	%f166, %f163;
	mul.f32 	%f167, %f166, %f165;
	neg.f32 	%f168, %f110;
	fma.rn.f32 	%f169, %f168, %f110, %f151;
	fma.rn.f32 	%f170, %f167, %f169, %f167;
	mul.f32 	%f171, %f150, %f170;
	setp.gt.f32 	%p18, %f110, 0f4120E148;
	selp.f32 	%f172, 0f00000000, %f171, %p18;
	setp.lt.f32 	%p19, %f9, 0f00000000;
	sub.f32 	%f173, %f143, %f172;
	selp.f32 	%f334, %f173, %f172, %p19;
	setp.geu.f32 	%p20, %f105, 0fBF800000;
	@%p20 bra 	$L__BB17_12;

	sub.f32 	%f174, %f7, %f9;
	add.rn.f32 	%f175, %f174, %f8;
	mul.f32 	%f176, %f9, 0fC0000000;
	mul.f32 	%f177, %f176, %f334;
	fma.rn.f32 	%f334, %f177, %f175, %f334;

$L__BB17_12:
	mul.f32 	%f178, %f334, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f178;}

	// end inline asm
	// begin inline asm
	{mul.f16 %rs18,%rs2,%rs17;
}
	// end inline asm
	st.global.u16 	[%rd11], %rs18;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p21, %rd7, %rd28;
	@%p21 bra 	$L__BB17_10;

$L__BB17_32:
	ret;

}
	// .globl	urelu_f16
.visible .entry urelu_f16(
	.param .u64 urelu_f16_param_0,
	.param .u64 urelu_f16_param_1,
	.param .u64 urelu_f16_param_2,
	.param .u64 urelu_f16_param_3,
	.param .u64 urelu_f16_param_4
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<90>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<76>;


	ld.param.u64 	%rd30, [urelu_f16_param_0];
	ld.param.u64 	%rd31, [urelu_f16_param_1];
	ld.param.u64 	%rd33, [urelu_f16_param_2];
	ld.param.u64 	%rd32, [urelu_f16_param_3];
	ld.param.u64 	%rd34, [urelu_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd34;
	cvta.to.global.u64 	%rd2, %rd32;
	cvta.to.global.u64 	%rd3, %rd33;
	setp.eq.s64 	%p3, %rd31, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p31, %p2;
	@%p3 bra 	$L__BB18_4;

	mov.u64 	%rd68, 1;
	mov.u32 	%r38, 0;

$L__BB18_2:
	not.b32 	%r24, %r38;
	cvt.u64.u32 	%rd36, %r24;
	add.s64 	%rd37, %rd36, %rd31;
	and.b64  	%rd5, %rd37, 4294967295;
	add.s64 	%rd38, %rd5, %rd31;
	shl.b64 	%rd39, %rd38, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	setp.ne.s64 	%p5, %rd68, %rd41;
	mov.pred 	%p31, -1;
	@%p5 bra 	$L__BB18_4;

	shl.b64 	%rd42, %rd5, 3;
	add.s64 	%rd43, %rd3, %rd42;
	ld.global.u64 	%rd44, [%rd43];
	mul.lo.s64 	%rd68, %rd44, %rd68;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd45, %r38;
	setp.lt.u64 	%p7, %rd45, %rd31;
	mov.pred 	%p31, %p2;
	@%p7 bra 	$L__BB18_2;

$L__BB18_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd7, %r4;
	@%p31 bra 	$L__BB18_15;
	bra.uni 	$L__BB18_5;

$L__BB18_15:
	setp.ge.u64 	%p16, %rd7, %rd30;
	@%p16 bra 	$L__BB18_38;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r28;
	@%p3 bra 	$L__BB18_29;

$L__BB18_17:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB18_18:
	not.b32 	%r31, %r42;
	cvt.u64.u32 	%rd51, %r31;
	add.s64 	%rd52, %rd51, %rd31;
	cvt.u64.u32 	%rd14, %r43;
	shl.b64 	%rd53, %rd52, 3;
	and.b64  	%rd54, %rd53, 34359738360;
	add.s64 	%rd15, %rd3, %rd54;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd55, %rd16, -4294967296;
	setp.eq.s64 	%p18, %rd55, 0;
	@%p18 bra 	$L__BB18_20;

	div.u64 	%rd72, %rd14, %rd16;
	mul.lo.s64 	%rd56, %rd72, %rd16;
	sub.s64 	%rd73, %rd14, %rd56;
	bra.uni 	$L__BB18_21;

$L__BB18_20:
	cvt.u32.u64 	%r32, %rd16;
	cvt.u32.u64 	%r33, %rd14;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd72, %r34;
	cvt.u64.u32 	%rd73, %r36;

$L__BB18_21:
	shl.b64 	%rd57, %rd31, 3;
	add.s64 	%rd58, %rd15, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	mul.lo.s64 	%rd60, %rd59, %rd73;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r44, %r44, %r37;
	cvt.u32.u64 	%r43, %rd72;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd61, %r42;
	setp.lt.u64 	%p19, %rd61, %rd31;
	@%p19 bra 	$L__BB18_18;

	setp.eq.s64 	%p20, %rd32, 0;
	shl.b64 	%rd62, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd62;
	@%p20 bra 	$L__BB18_24;

	mul.wide.u32 	%rd63, %r44, 2;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.u16 	%rs86, [%rd64];
	bra.uni 	$L__BB18_25;

$L__BB18_24:
	ld.global.u16 	%rs86, [%rd23];

$L__BB18_25:
	mov.f64 	%fd3, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs48, %fd3;}

	// end inline asm
	// begin inline asm
	{set.nan.f16.f16 %rs49,%rs86,%rs86;
}
	// end inline asm
	setp.ne.s16 	%p21, %rs49, 0;
	mov.u16 	%rs87, 32767;
	@%p21 bra 	$L__BB18_28;

	// begin inline asm
	{set.nan.f16.f16 %rs53,%rs48,%rs48;
}
	// end inline asm
	setp.ne.s16 	%p22, %rs53, 0;
	@%p22 bra 	$L__BB18_28;

	// begin inline asm
	{  cvt.f32.f16 %f13, %rs86;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs48;}

	// end inline asm
	// begin inline asm
	{max.f32 %f15,%f13,%f14;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f15;}

	// end inline asm

$L__BB18_28:
	st.global.u16 	[%rd23], %rs87;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd30;
	@%p23 bra 	$L__BB18_17;
	bra.uni 	$L__BB18_38;

$L__BB18_5:
	setp.ge.u64 	%p8, %rd7, %rd30;
	@%p8 bra 	$L__BB18_38;

	setp.eq.s64 	%p9, %rd32, 0;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p9 bra 	$L__BB18_11;

$L__BB18_7:
	shl.b64 	%rd46, %rd7, 1;
	add.s64 	%rd47, %rd2, %rd46;
	ld.global.u16 	%rs1, [%rd47];
	mov.f64 	%fd1, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs24, %fd1;}

	// end inline asm
	// begin inline asm
	{set.nan.f16.f16 %rs25,%rs1,%rs1;
}
	// end inline asm
	setp.ne.s16 	%p10, %rs25, 0;
	mov.u16 	%rs84, 32767;
	@%p10 bra 	$L__BB18_10;

	// begin inline asm
	{set.nan.f16.f16 %rs29,%rs24,%rs24;
}
	// end inline asm
	setp.ne.s16 	%p11, %rs29, 0;
	@%p11 bra 	$L__BB18_10;

	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs24;}

	// end inline asm
	// begin inline asm
	{max.f32 %f3,%f1,%f2;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f3;}

	// end inline asm

$L__BB18_10:
	add.s64 	%rd49, %rd1, %rd46;
	st.global.u16 	[%rd49], %rs84;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p12, %rd7, %rd30;
	@%p12 bra 	$L__BB18_7;
	bra.uni 	$L__BB18_38;

$L__BB18_11:
	shl.b64 	%rd50, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd50;
	ld.global.u16 	%rs5, [%rd11];
	mov.f64 	%fd2, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs36, %fd2;}

	// end inline asm
	// begin inline asm
	{set.nan.f16.f16 %rs37,%rs5,%rs5;
}
	// end inline asm
	setp.ne.s16 	%p13, %rs37, 0;
	mov.u16 	%rs85, 32767;
	@%p13 bra 	$L__BB18_14;

	// begin inline asm
	{set.nan.f16.f16 %rs41,%rs36,%rs36;
}
	// end inline asm
	setp.ne.s16 	%p14, %rs41, 0;
	@%p14 bra 	$L__BB18_14;

	// begin inline asm
	{  cvt.f32.f16 %f7, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs36;}

	// end inline asm
	// begin inline asm
	{max.f32 %f9,%f7,%f8;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs85, %f9;}

	// end inline asm

$L__BB18_14:
	st.global.u16 	[%rd11], %rs85;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p15, %rd7, %rd30;
	@%p15 bra 	$L__BB18_11;
	bra.uni 	$L__BB18_38;

$L__BB18_29:
	setp.eq.s64 	%p24, %rd32, 0;
	@%p24 bra 	$L__BB18_34;

$L__BB18_30:
	ld.global.u16 	%rs16, [%rd2];
	mov.f64 	%fd4, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs60, %fd4;}

	// end inline asm
	// begin inline asm
	{set.nan.f16.f16 %rs61,%rs16,%rs16;
}
	// end inline asm
	setp.ne.s16 	%p25, %rs61, 0;
	mov.u16 	%rs88, 32767;
	@%p25 bra 	$L__BB18_33;

	// begin inline asm
	{set.nan.f16.f16 %rs65,%rs60,%rs60;
}
	// end inline asm
	setp.ne.s16 	%p26, %rs65, 0;
	@%p26 bra 	$L__BB18_33;

	// begin inline asm
	{  cvt.f32.f16 %f19, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs60;}

	// end inline asm
	// begin inline asm
	{max.f32 %f21,%f19,%f20;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs88, %f21;}

	// end inline asm

$L__BB18_33:
	shl.b64 	%rd65, %rd7, 1;
	add.s64 	%rd66, %rd1, %rd65;
	st.global.u16 	[%rd66], %rs88;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p27, %rd7, %rd30;
	@%p27 bra 	$L__BB18_30;
	bra.uni 	$L__BB18_38;

$L__BB18_34:
	shl.b64 	%rd67, %rd7, 1;
	add.s64 	%rd28, %rd1, %rd67;
	ld.global.u16 	%rs20, [%rd28];
	mov.f64 	%fd5, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs72, %fd5;}

	// end inline asm
	// begin inline asm
	{set.nan.f16.f16 %rs73,%rs20,%rs20;
}
	// end inline asm
	setp.ne.s16 	%p28, %rs73, 0;
	mov.u16 	%rs89, 32767;
	@%p28 bra 	$L__BB18_37;

	// begin inline asm
	{set.nan.f16.f16 %rs77,%rs72,%rs72;
}
	// end inline asm
	setp.ne.s16 	%p29, %rs77, 0;
	@%p29 bra 	$L__BB18_37;

	// begin inline asm
	{  cvt.f32.f16 %f25, %rs20;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs72;}

	// end inline asm
	// begin inline asm
	{max.f32 %f27,%f25,%f26;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f27;}

	// end inline asm

$L__BB18_37:
	st.global.u16 	[%rd28], %rs89;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p30, %rd7, %rd30;
	@%p30 bra 	$L__BB18_34;

$L__BB18_38:
	ret;

}
	// .globl	uelu_f16
.visible .entry uelu_f16(
	.param .u64 uelu_f16_param_0,
	.param .u64 uelu_f16_param_1,
	.param .u64 uelu_f16_param_2,
	.param .align 2 .b8 uelu_f16_param_3[2],
	.param .u64 uelu_f16_param_4,
	.param .u64 uelu_f16_param_5
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<84>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<71>;


	ld.param.u16 	%rs25, [uelu_f16_param_3];
	ld.param.u64 	%rd28, [uelu_f16_param_0];
	ld.param.u64 	%rd29, [uelu_f16_param_1];
	ld.param.u64 	%rd31, [uelu_f16_param_2];
	ld.param.u64 	%rd30, [uelu_f16_param_4];
	ld.param.u64 	%rd32, [uelu_f16_param_5];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p24, %p2;
	@%p3 bra 	$L__BB19_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r44, 0;

$L__BB19_2:
	not.b32 	%r22, %r44;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd64, %rd39;
	mov.pred 	%p24, -1;
	@%p5 bra 	$L__BB19_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd64, %rd42, %rd64;
	add.s32 	%r44, %r44, 1;
	cvt.u64.u32 	%rd43, %r44;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p24, %p2;
	@%p7 bra 	$L__BB19_2;

$L__BB19_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p24 bra 	$L__BB19_13;
	bra.uni 	$L__BB19_5;

$L__BB19_13:
	setp.ge.u64 	%p14, %rd7, %rd28;
	@%p14 bra 	$L__BB19_32;

	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r30;
	@%p3 bra 	$L__BB19_26;

$L__BB19_15:
	mov.u32 	%r48, 0;
	mov.u32 	%r49, %r4;
	mov.u32 	%r50, %r48;

$L__BB19_16:
	not.b32 	%r33, %r48;
	cvt.u64.u32 	%rd49, %r33;
	add.s64 	%rd50, %rd49, %rd29;
	cvt.u64.u32 	%rd14, %r49;
	shl.b64 	%rd51, %rd50, 3;
	and.b64  	%rd52, %rd51, 34359738360;
	add.s64 	%rd15, %rd3, %rd52;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd53, %rd16, -4294967296;
	setp.eq.s64 	%p16, %rd53, 0;
	@%p16 bra 	$L__BB19_18;

	div.u64 	%rd68, %rd14, %rd16;
	mul.lo.s64 	%rd54, %rd68, %rd16;
	sub.s64 	%rd69, %rd14, %rd54;
	bra.uni 	$L__BB19_19;

$L__BB19_18:
	cvt.u32.u64 	%r34, %rd16;
	cvt.u32.u64 	%r35, %rd14;
	div.u32 	%r36, %r35, %r34;
	mul.lo.s32 	%r37, %r36, %r34;
	sub.s32 	%r38, %r35, %r37;
	cvt.u64.u32 	%rd68, %r36;
	cvt.u64.u32 	%rd69, %r38;

$L__BB19_19:
	shl.b64 	%rd55, %rd29, 3;
	add.s64 	%rd56, %rd15, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd69;
	cvt.u32.u64 	%r39, %rd58;
	add.s32 	%r50, %r50, %r39;
	cvt.u32.u64 	%r49, %rd68;
	add.s32 	%r48, %r48, 1;
	cvt.u64.u32 	%rd59, %r48;
	setp.lt.u64 	%p17, %rd59, %rd29;
	@%p17 bra 	$L__BB19_16;

	setp.eq.s64 	%p18, %rd30, 0;
	shl.b64 	%rd60, %rd7, 1;
	add.s64 	%rd23, %rd1, %rd60;
	@%p18 bra 	$L__BB19_22;

	mul.wide.u32 	%rd61, %r50, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs81, [%rd62];
	bra.uni 	$L__BB19_23;

$L__BB19_22:
	ld.global.u16 	%rs81, [%rd23];

$L__BB19_23:
	mov.u32 	%r40, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs52, %r40;
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs81, %rs52;
  selp.u16 %rs53, 1, 0, __$temp3;}
	// end inline asm
	setp.ne.s16 	%p19, %rs53, 0;
	@%p19 bra 	$L__BB19_25;

	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs81;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs56,r;           
}
	// end inline asm
	mov.u32 	%r41, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs58, %r41;
	// end inline asm
	// begin inline asm
	{sub.f16 %rs59,%rs56,%rs58;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs81,%rs25,%rs59;
}
	// end inline asm

$L__BB19_25:
	st.global.u16 	[%rd23], %rs81;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p20, %rd7, %rd28;
	@%p20 bra 	$L__BB19_15;
	bra.uni 	$L__BB19_32;

$L__BB19_26:
	shl.b64 	%rd63, %rd7, 1;
	add.s64 	%rd26, %rd1, %rd63;
	setp.eq.s64 	%p21, %rd30, 0;
	@%p21 bra 	$L__BB19_28;

	ld.global.u16 	%rs83, [%rd2];
	bra.uni 	$L__BB19_29;

$L__BB19_28:
	ld.global.u16 	%rs83, [%rd26];

$L__BB19_29:
	mov.u32 	%r42, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs65, %r42;
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs83, %rs65;
  selp.u16 %rs66, 1, 0, __$temp3;}
	// end inline asm
	setp.ne.s16 	%p22, %rs66, 0;
	@%p22 bra 	$L__BB19_31;

	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs83;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs69,r;           
}
	// end inline asm
	mov.u32 	%r43, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs71, %r43;
	// end inline asm
	// begin inline asm
	{sub.f16 %rs72,%rs69,%rs71;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs83,%rs25,%rs72;
}
	// end inline asm

$L__BB19_31:
	st.global.u16 	[%rd26], %rs83;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p23, %rd7, %rd28;
	@%p23 bra 	$L__BB19_26;
	bra.uni 	$L__BB19_32;

$L__BB19_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB19_32;

	setp.eq.s64 	%p9, %rd30, 0;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p9 bra 	$L__BB19_10;

$L__BB19_7:
	shl.b64 	%rd44, %rd7, 1;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u16 	%rs78, [%rd45];
	mov.u32 	%r26, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs26, %r26;
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs78, %rs26;
  selp.u16 %rs27, 1, 0, __$temp3;}
	// end inline asm
	setp.ne.s16 	%p10, %rs27, 0;
	@%p10 bra 	$L__BB19_9;

	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs78;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs30,r;           
}
	// end inline asm
	mov.u32 	%r27, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs32, %r27;
	// end inline asm
	// begin inline asm
	{sub.f16 %rs33,%rs30,%rs32;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs78,%rs25,%rs33;
}
	// end inline asm

$L__BB19_9:
	add.s64 	%rd47, %rd1, %rd44;
	st.global.u16 	[%rd47], %rs78;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p11, %rd7, %rd28;
	@%p11 bra 	$L__BB19_7;
	bra.uni 	$L__BB19_32;

$L__BB19_10:
	shl.b64 	%rd48, %rd7, 1;
	add.s64 	%rd11, %rd1, %rd48;
	ld.global.u16 	%rs79, [%rd11];
	mov.u32 	%r28, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs39, %r28;
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs79, %rs39;
  selp.u16 %rs40, 1, 0, __$temp3;}
	// end inline asm
	setp.ne.s16 	%p12, %rs40, 0;
	@%p12 bra 	$L__BB19_12;

	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs79;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs43,r;           
}
	// end inline asm
	mov.u32 	%r29, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs45, %r29;
	// end inline asm
	// begin inline asm
	{sub.f16 %rs46,%rs43,%rs45;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs79,%rs25,%rs46;
}
	// end inline asm

$L__BB19_12:
	st.global.u16 	[%rd11], %rs79;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p13, %rd7, %rd28;
	@%p13 bra 	$L__BB19_10;

$L__BB19_32:
	ret;

}
	// .globl	usilu_f16
.visible .entry usilu_f16(
	.param .u64 usilu_f16_param_0,
	.param .u64 usilu_f16_param_1,
	.param .u64 usilu_f16_param_2,
	.param .u64 usilu_f16_param_3,
	.param .u64 usilu_f16_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<113>;
	.reg .f32 	%f<53>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<77>;


	ld.param.u64 	%rd26, [usilu_f16_param_0];
	ld.param.u64 	%rd27, [usilu_f16_param_1];
	ld.param.u64 	%rd28, [usilu_f16_param_2];
	ld.param.u64 	%rd29, [usilu_f16_param_3];
	ld.param.u64 	%rd30, [usilu_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB20_4;

	mov.u64 	%rd70, 1;
	mov.u32 	%r44, 0;

$L__BB20_2:
	not.b32 	%r19, %r44;
	cvt.u64.u32 	%rd32, %r19;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd3, %rd33, 4294967295;
	add.s64 	%rd34, %rd3, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd70, %rd37;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB20_4;

	shl.b64 	%rd38, %rd3, 3;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd70, %rd40, %rd70;
	add.s32 	%r44, %r44, 1;
	cvt.u64.u32 	%rd41, %r44;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB20_2;

$L__BB20_4:
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r3, %r21, %r20, %r22;
	cvt.u64.u32 	%rd5, %r3;
	@%p28 bra 	$L__BB20_17;
	bra.uni 	$L__BB20_5;

$L__BB20_17:
	setp.ge.u64 	%p16, %rd5, %rd26;
	@%p16 bra 	$L__BB20_39;

	@%p3 bra 	$L__BB20_31;

$L__BB20_19:
	mov.u32 	%r48, 0;
	mov.u32 	%r49, %r3;
	mov.u32 	%r50, %r48;

$L__BB20_20:
	not.b32 	%r31, %r48;
	cvt.u64.u32 	%rd50, %r31;
	add.s64 	%rd51, %rd50, %rd27;
	cvt.u64.u32 	%rd12, %r49;
	shl.b64 	%rd53, %rd51, 3;
	and.b64  	%rd54, %rd53, 34359738360;
	add.s64 	%rd13, %rd1, %rd54;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd55, %rd14, -4294967296;
	setp.eq.s64 	%p18, %rd55, 0;
	@%p18 bra 	$L__BB20_22;

	div.u64 	%rd74, %rd12, %rd14;
	mul.lo.s64 	%rd56, %rd74, %rd14;
	sub.s64 	%rd75, %rd12, %rd56;
	bra.uni 	$L__BB20_23;

$L__BB20_22:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd74, %r34;
	cvt.u64.u32 	%rd75, %r36;

$L__BB20_23:
	shl.b64 	%rd57, %rd27, 3;
	add.s64 	%rd58, %rd13, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	mul.lo.s64 	%rd60, %rd59, %rd75;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r50, %r50, %r37;
	cvt.u32.u64 	%r49, %rd74;
	add.s32 	%r48, %r48, 1;
	cvt.u64.u32 	%rd61, %r48;
	setp.lt.u64 	%p19, %rd61, %rd27;
	@%p19 bra 	$L__BB20_20;

	setp.eq.s64 	%p20, %rd29, 0;
	cvta.to.global.u64 	%rd62, %rd30;
	shl.b64 	%rd63, %rd5, 1;
	add.s64 	%rd21, %rd62, %rd63;
	@%p20 bra 	$L__BB20_26;

	cvta.to.global.u64 	%rd64, %rd29;
	mul.wide.u32 	%rd65, %r50, 2;
	add.s64 	%rd66, %rd64, %rd65;
	ld.global.u16 	%rs109, [%rd66];
	bra.uni 	$L__BB20_27;

$L__BB20_26:
	ld.global.u16 	%rs109, [%rd21];

$L__BB20_27:
	mov.u32 	%r38, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs65, %r38;
	// end inline asm
	// begin inline asm
	{neg.f16 %rs66,%rs109;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs66;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs68,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs70,%rs65,%rs68;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f35, %rs109;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f36, %rs70;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f37, %f36;
}
	// end inline asm
	mul.f32 	%f39, %f35, %f37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f39;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs76,%rs110;
}
	// end inline asm
	mov.u16 	%rs80, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs76, %rs80;
  selp.u16 %rs78, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p21, %rs78, 0;
	@%p21 bra 	$L__BB20_30;

	mov.f32 	%f40, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f40;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs81, %rs76;
  selp.u16 %rs82, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p22, %rs82, 0;
	@%p22 bra 	$L__BB20_30;

	neg.f32 	%f42, %f36;
	fma.rn.f32 	%f43, %f42, %f39, %f35;
	fma.rn.f32 	%f41, %f37, %f43, %f39;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f41;}

	// end inline asm

$L__BB20_30:
	st.global.u16 	[%rd21], %rs110;
	mov.u32 	%r39, %nctaid.x;
	mad.lo.s32 	%r3, %r20, %r39, %r3;
	cvt.u64.u32 	%rd5, %r3;
	setp.lt.u64 	%p23, %rd5, %rd26;
	@%p23 bra 	$L__BB20_19;
	bra.uni 	$L__BB20_39;

$L__BB20_5:
	setp.ge.u64 	%p8, %rd5, %rd26;
	@%p8 bra 	$L__BB20_39;

	setp.eq.s64 	%p9, %rd29, 0;
	@%p9 bra 	$L__BB20_12;

	cvta.to.global.u64 	%rd42, %rd29;
	cvta.to.global.u64 	%rd45, %rd30;
	mov.u32 	%r24, %nctaid.x;

$L__BB20_8:
	shl.b64 	%rd43, %rd5, 1;
	add.s64 	%rd44, %rd42, %rd43;
	ld.global.u16 	%rs25, [%rd44];
	mov.u32 	%r23, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs23, %r23;
	// end inline asm
	// begin inline asm
	{neg.f16 %rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs24;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs26,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs28,%rs23,%rs26;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs28;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f19, %f18;
}
	// end inline asm
	mul.f32 	%f21, %f17, %f19;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs107, %f21;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs34,%rs107;
}
	// end inline asm
	mov.u16 	%rs38, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs34, %rs38;
  selp.u16 %rs36, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p10, %rs36, 0;
	@%p10 bra 	$L__BB20_11;

	mov.f32 	%f22, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f22;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs39, %rs34;
  selp.u16 %rs40, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p11, %rs40, 0;
	@%p11 bra 	$L__BB20_11;

	neg.f32 	%f24, %f18;
	fma.rn.f32 	%f25, %f24, %f21, %f17;
	fma.rn.f32 	%f23, %f19, %f25, %f21;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs107, %f23;}

	// end inline asm

$L__BB20_11:
	add.s64 	%rd47, %rd45, %rd43;
	st.global.u16 	[%rd47], %rs107;
	mad.lo.s32 	%r3, %r20, %r24, %r3;
	cvt.u64.u32 	%rd5, %r3;
	setp.lt.u64 	%p12, %rd5, %rd26;
	@%p12 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_39;

$L__BB20_31:
	cvta.to.global.u64 	%rd67, %rd30;
	cvta.to.global.u64 	%rd69, %rd29;
	mov.u32 	%r42, %nctaid.x;

$L__BB20_32:
	shl.b64 	%rd68, %rd5, 1;
	add.s64 	%rd24, %rd67, %rd68;
	setp.eq.s64 	%p24, %rd29, 0;
	@%p24 bra 	$L__BB20_34;

	ld.global.u16 	%rs111, [%rd69];
	bra.uni 	$L__BB20_35;

$L__BB20_34:
	ld.global.u16 	%rs111, [%rd24];

$L__BB20_35:
	mov.u32 	%r41, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs86, %r41;
	// end inline asm
	// begin inline asm
	{neg.f16 %rs87,%rs111;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs87;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs89,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs91,%rs86,%rs89;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f44, %rs111;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f45, %rs91;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f46, %f45;
}
	// end inline asm
	mul.f32 	%f48, %f44, %f46;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs112, %f48;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs97,%rs112;
}
	// end inline asm
	mov.u16 	%rs101, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs97, %rs101;
  selp.u16 %rs99, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p25, %rs99, 0;
	@%p25 bra 	$L__BB20_38;

	mov.f32 	%f49, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f49;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs102, %rs97;
  selp.u16 %rs103, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p26, %rs103, 0;
	@%p26 bra 	$L__BB20_38;

	neg.f32 	%f51, %f45;
	fma.rn.f32 	%f52, %f51, %f48, %f44;
	fma.rn.f32 	%f50, %f46, %f52, %f48;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs112, %f50;}

	// end inline asm

$L__BB20_38:
	st.global.u16 	[%rd24], %rs112;
	mad.lo.s32 	%r3, %r20, %r42, %r3;
	cvt.u64.u32 	%rd5, %r3;
	setp.lt.u64 	%p27, %rd5, %rd26;
	@%p27 bra 	$L__BB20_32;
	bra.uni 	$L__BB20_39;

$L__BB20_12:
	cvta.to.global.u64 	%rd48, %rd30;
	mov.u32 	%r27, %nctaid.x;

$L__BB20_13:
	shl.b64 	%rd49, %rd5, 1;
	add.s64 	%rd9, %rd48, %rd49;
	ld.global.u16 	%rs46, [%rd9];
	mov.u32 	%r26, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs44, %r26;
	// end inline asm
	// begin inline asm
	{neg.f16 %rs45,%rs46;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs45;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs47,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs49,%rs44,%rs47;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs49;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f28, %f27;
}
	// end inline asm
	mul.f32 	%f30, %f26, %f28;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f30;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs55,%rs108;
}
	// end inline asm
	mov.u16 	%rs59, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs55, %rs59;
  selp.u16 %rs57, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p13, %rs57, 0;
	@%p13 bra 	$L__BB20_16;

	mov.f32 	%f31, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f31;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs60, %rs55;
  selp.u16 %rs61, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p14, %rs61, 0;
	@%p14 bra 	$L__BB20_16;

	neg.f32 	%f33, %f27;
	fma.rn.f32 	%f34, %f33, %f30, %f26;
	fma.rn.f32 	%f32, %f28, %f34, %f30;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f32;}

	// end inline asm

$L__BB20_16:
	st.global.u16 	[%rd9], %rs108;
	mad.lo.s32 	%r3, %r20, %r27, %r3;
	cvt.u64.u32 	%rd5, %r3;
	setp.lt.u64 	%p15, %rd5, %rd26;
	@%p15 bra 	$L__BB20_13;

$L__BB20_39:
	ret;

}
	// .globl	upowf_f16
.visible .entry upowf_f16(
	.param .u64 upowf_f16_param_0,
	.param .u64 upowf_f16_param_1,
	.param .u64 upowf_f16_param_2,
	.param .align 2 .b8 upowf_f16_param_3[2],
	.param .u64 upowf_f16_param_4,
	.param .u64 upowf_f16_param_5
)
{
	.reg .pred 	%p<79>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<271>;
	.reg .b32 	%r<85>;
	.reg .b64 	%rd<66>;


	ld.param.u16 	%rs6, [upowf_f16_param_3];
	ld.param.u64 	%rd25, [upowf_f16_param_0];
	ld.param.u64 	%rd26, [upowf_f16_param_1];
	ld.param.u64 	%rd28, [upowf_f16_param_2];
	ld.param.u64 	%rd27, [upowf_f16_param_4];
	ld.param.u64 	%rd29, [upowf_f16_param_5];
	cvta.to.global.u64 	%rd1, %rd29;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd28;
	setp.eq.s64 	%p3, %rd26, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p78, %p2;
	@%p3 bra 	$L__BB21_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r78, 0;

$L__BB21_2:
	not.b32 	%r21, %r78;
	cvt.u64.u32 	%rd31, %r21;
	add.s64 	%rd32, %rd31, %rd26;
	and.b64  	%rd5, %rd32, 4294967295;
	add.s64 	%rd33, %rd5, %rd26;
	shl.b64 	%rd34, %rd33, 3;
	add.s64 	%rd35, %rd3, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p5, %rd60, %rd36;
	mov.pred 	%p78, -1;
	@%p5 bra 	$L__BB21_4;

	shl.b64 	%rd37, %rd5, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	mul.lo.s64 	%rd60, %rd39, %rd60;
	add.s32 	%r78, %r78, 1;
	cvt.u64.u32 	%rd40, %r78;
	setp.lt.u64 	%p7, %rd40, %rd26;
	mov.pred 	%p78, %p2;
	@%p7 bra 	$L__BB21_2;

$L__BB21_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r79, %r22, %r3, %r23;
	cvt.u64.u32 	%rd61, %r79;
	@%p78 bra 	$L__BB21_29;
	bra.uni 	$L__BB21_5;

$L__BB21_29:
	setp.ge.u64 	%p52, %rd61, %rd25;
	@%p52 bra 	$L__BB21_49;

	mov.u32 	%r53, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r53;
	// begin inline asm
	{  cvt.f32.f16 %f190, %rs6;}

	// end inline asm
	mul.f32 	%f192, %f190, 0f3F000000;
	cvt.rzi.f32.f32 	%f193, %f192;
	add.f32 	%f194, %f193, %f193;
	sub.f32 	%f195, %f190, %f194;
	abs.f32 	%f23, %f195;
	bra.uni 	$L__BB21_31;

$L__BB21_42:
	setp.eq.f32 	%p66, %f189, 0f00000000;
	setp.eq.f32 	%p67, %f24, 0f7F800000;
	or.pred  	%p68, %p66, %p67;
	@%p68 bra 	$L__BB21_46;
	bra.uni 	$L__BB21_43;

$L__BB21_46:
	setp.eq.f32 	%p75, %f23, 0f3F800000;
	add.f32 	%f266, %f189, %f189;
	mov.b32 	%r73, %f266;
	xor.b32  	%r74, %r73, 2139095040;
	setp.lt.f32 	%p76, %f190, 0f00000000;
	selp.b32 	%r75, %r74, %r73, %p76;
	and.b32  	%r76, %r75, 2147483647;
	selp.b32 	%r77, %r75, %r76, %p75;
	mov.b32 	%f270, %r77;
	bra.uni 	$L__BB21_48;

$L__BB21_43:
	setp.eq.f32 	%p69, %f189, 0fBF800000;
	setp.eq.f32 	%p70, %f26, 0f7F800000;
	and.pred  	%p71, %p69, %p70;
	@%p71 bra 	$L__BB21_48;

	setp.geu.f32 	%p72, %f189, 0f00000000;
	mov.f32 	%f270, %f25;
	@%p72 bra 	$L__BB21_48;

	setp.eq.f32 	%p73, %f23, 0f3F800000;
	neg.f32 	%f263, %f25;
	selp.f32 	%f264, %f263, %f25, %p73;
	cvt.rmi.f32.f32 	%f265, %f190;
	setp.neu.f32 	%p74, %f265, %f190;
	selp.f32 	%f270, 0f7FFFFFFF, %f264, %p74;
	bra.uni 	$L__BB21_48;

$L__BB21_31:
	mov.u32 	%r82, 0;
	mov.u32 	%r83, %r79;
	mov.u32 	%r84, %r82;
	@%p3 bra 	$L__BB21_36;

$L__BB21_32:
	not.b32 	%r57, %r82;
	cvt.u64.u32 	%rd46, %r57;
	add.s64 	%rd47, %rd46, %rd26;
	cvt.u64.u32 	%rd14, %r83;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd15, %rd3, %rd49;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd50, %rd16, -4294967296;
	setp.eq.s64 	%p54, %rd50, 0;
	@%p54 bra 	$L__BB21_34;

	div.u64 	%rd64, %rd14, %rd16;
	mul.lo.s64 	%rd51, %rd64, %rd16;
	sub.s64 	%rd65, %rd14, %rd51;
	bra.uni 	$L__BB21_35;

$L__BB21_34:
	cvt.u32.u64 	%r58, %rd16;
	cvt.u32.u64 	%r59, %rd14;
	div.u32 	%r60, %r59, %r58;
	mul.lo.s32 	%r61, %r60, %r58;
	sub.s32 	%r62, %r59, %r61;
	cvt.u64.u32 	%rd64, %r60;
	cvt.u64.u32 	%rd65, %r62;

$L__BB21_35:
	shl.b64 	%rd52, %rd26, 3;
	add.s64 	%rd53, %rd15, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd65;
	cvt.u32.u64 	%r63, %rd55;
	add.s32 	%r84, %r84, %r63;
	cvt.u32.u64 	%r83, %rd64;
	add.s32 	%r82, %r82, 1;
	cvt.u64.u32 	%rd56, %r82;
	setp.lt.u64 	%p55, %rd56, %rd26;
	@%p55 bra 	$L__BB21_32;

$L__BB21_36:
	shl.b64 	%rd57, %rd61, 1;
	add.s64 	%rd23, %rd1, %rd57;
	setp.eq.s64 	%p56, %rd27, 0;
	@%p56 bra 	$L__BB21_38;

	mul.wide.u32 	%rd58, %r84, 2;
	add.s64 	%rd59, %rd2, %rd58;
	ld.global.u16 	%rs16, [%rd59];
	bra.uni 	$L__BB21_39;

$L__BB21_38:
	ld.global.u16 	%rs16, [%rd23];

$L__BB21_39:
	// begin inline asm
	{  cvt.f32.f16 %f189, %rs16;}

	// end inline asm
	abs.f32 	%f24, %f189;
	setp.lt.f32 	%p57, %f24, 0f00800000;
	mul.f32 	%f196, %f24, 0f4B800000;
	selp.f32 	%f197, %f196, %f24, %p57;
	selp.f32 	%f198, 0fC1C00000, 0f00000000, %p57;
	mov.b32 	%r64, %f197;
	add.s32 	%r65, %r64, -1060439283;
	and.b32  	%r66, %r65, -8388608;
	sub.s32 	%r67, %r64, %r66;
	mov.b32 	%f199, %r67;
	cvt.rn.f32.s32 	%f200, %r66;
	mov.f32 	%f201, 0f34000000;
	fma.rn.f32 	%f202, %f200, %f201, %f198;
	add.f32 	%f203, %f199, 0fBF800000;
	add.f32 	%f204, %f199, 0f3F800000;
	mov.f32 	%f270, 0f3F800000;
	rcp.approx.ftz.f32 	%f205, %f204;
	add.f32 	%f206, %f203, %f203;
	mul.f32 	%f207, %f206, %f205;
	mul.f32 	%f208, %f207, %f207;
	sub.f32 	%f209, %f203, %f207;
	add.f32 	%f210, %f209, %f209;
	neg.f32 	%f211, %f207;
	fma.rn.f32 	%f212, %f211, %f203, %f210;
	mul.rn.f32 	%f213, %f205, %f212;
	mov.f32 	%f214, 0f3B52E7DB;
	mov.f32 	%f215, 0f3A2C32E4;
	fma.rn.f32 	%f216, %f215, %f208, %f214;
	mov.f32 	%f217, 0f3C93BB73;
	fma.rn.f32 	%f218, %f216, %f208, %f217;
	mov.f32 	%f219, 0f3DF6384F;
	fma.rn.f32 	%f220, %f218, %f208, %f219;
	mul.rn.f32 	%f221, %f220, %f208;
	mov.f32 	%f222, 0f3FB8AA3B;
	fma.rn.f32 	%f223, %f207, %f222, %f202;
	sub.f32 	%f224, %f202, %f223;
	fma.rn.f32 	%f225, %f207, %f222, %f224;
	fma.rn.f32 	%f226, %f213, %f222, %f225;
	mov.f32 	%f227, 0f32A55E34;
	fma.rn.f32 	%f228, %f207, %f227, %f226;
	mul.f32 	%f229, %f221, 0f40400000;
	fma.rn.f32 	%f230, %f229, %f213, %f228;
	fma.rn.f32 	%f231, %f221, %f207, %f230;
	add.rn.f32 	%f232, %f223, %f231;
	neg.f32 	%f233, %f223;
	add.rn.f32 	%f234, %f232, %f233;
	neg.f32 	%f235, %f234;
	add.rn.f32 	%f236, %f231, %f235;
	mul.rn.f32 	%f237, %f232, %f190;
	neg.f32 	%f238, %f237;
	fma.rn.f32 	%f239, %f232, %f190, %f238;
	fma.rn.f32 	%f240, %f236, %f190, %f239;
	cvt.rni.f32.f32 	%f241, %f237;
	sub.f32 	%f242, %f237, %f241;
	add.f32 	%f243, %f240, %f242;
	mov.f32 	%f244, 0f3AAF85ED;
	mov.f32 	%f245, 0f391FCB8E;
	fma.rn.f32 	%f246, %f245, %f243, %f244;
	mov.f32 	%f247, 0f3C1D9856;
	fma.rn.f32 	%f248, %f246, %f243, %f247;
	mov.f32 	%f249, 0f3D6357BB;
	fma.rn.f32 	%f250, %f248, %f243, %f249;
	mov.f32 	%f251, 0f3E75FDEC;
	fma.rn.f32 	%f252, %f250, %f243, %f251;
	mov.f32 	%f253, 0f3F317218;
	fma.rn.f32 	%f254, %f252, %f243, %f253;
	fma.rn.f32 	%f255, %f254, %f243, %f270;
	cvt.rzi.s32.f32 	%r68, %f241;
	setp.gt.f32 	%p58, %f241, 0f00000000;
	selp.b32 	%r69, 0, -2097152000, %p58;
	add.s32 	%r70, %r69, 2130706432;
	mov.b32 	%f256, %r70;
	mul.f32 	%f257, %f255, %f256;
	shl.b32 	%r71, %r68, 23;
	sub.s32 	%r72, %r71, %r69;
	mov.b32 	%f258, %r72;
	mul.f32 	%f259, %f257, %f258;
	abs.f32 	%f260, %f237;
	setp.gt.f32 	%p59, %f260, 0f43180000;
	setp.lt.f32 	%p60, %f237, 0f00000000;
	selp.f32 	%f261, 0f00000000, 0f7F800000, %p60;
	selp.f32 	%f25, %f261, %f259, %p59;
	setp.eq.f32 	%p61, %f189, 0f3F800000;
	setp.eq.f32 	%p62, %f190, 0f00000000;
	or.pred  	%p63, %p61, %p62;
	@%p63 bra 	$L__BB21_48;

	setp.gtu.f32 	%p64, %f24, 0f7F800000;
	@%p64 bra 	$L__BB21_47;

	abs.f32 	%f26, %f190;
	setp.gtu.f32 	%p65, %f26, 0f7F800000;
	@%p65 bra 	$L__BB21_47;
	bra.uni 	$L__BB21_42;

$L__BB21_47:
	add.rn.f32 	%f270, %f189, %f190;

$L__BB21_48:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f270;}

	// end inline asm
	st.global.u16 	[%rd23], %rs15;
	add.s32 	%r79, %r79, %r10;
	cvt.u64.u32 	%rd61, %r79;
	setp.lt.u64 	%p77, %rd61, %rd25;
	@%p77 bra 	$L__BB21_31;
	bra.uni 	$L__BB21_49;

$L__BB21_5:
	setp.ge.u64 	%p8, %rd61, %rd25;
	@%p8 bra 	$L__BB21_49;

	setp.eq.s64 	%p9, %rd27, 0;
	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r24;
	@%p9 bra 	$L__BB21_18;

	// begin inline asm
	{  cvt.f32.f16 %f32, %rs6;}

	// end inline asm
	mul.f32 	%f34, %f32, 0f3F000000;
	cvt.rzi.f32.f32 	%f35, %f34;
	add.f32 	%f36, %f35, %f35;
	sub.f32 	%f37, %f32, %f36;
	abs.f32 	%f3, %f37;
	bra.uni 	$L__BB21_8;

$L__BB21_11:
	setp.eq.f32 	%p19, %f31, 0f00000000;
	setp.eq.f32 	%p20, %f4, 0f7F800000;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB21_15;
	bra.uni 	$L__BB21_12;

$L__BB21_15:
	setp.eq.f32 	%p28, %f3, 0f3F800000;
	add.f32 	%f108, %f31, %f31;
	mov.b32 	%r34, %f108;
	xor.b32  	%r35, %r34, 2139095040;
	setp.lt.f32 	%p29, %f32, 0f00000000;
	selp.b32 	%r36, %r35, %r34, %p29;
	and.b32  	%r37, %r36, 2147483647;
	selp.b32 	%r38, %r36, %r37, %p28;
	mov.b32 	%f268, %r38;
	bra.uni 	$L__BB21_17;

$L__BB21_12:
	setp.eq.f32 	%p22, %f31, 0fBF800000;
	setp.eq.f32 	%p23, %f6, 0f7F800000;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	$L__BB21_17;

	setp.geu.f32 	%p25, %f31, 0f00000000;
	mov.f32 	%f268, %f5;
	@%p25 bra 	$L__BB21_17;

	setp.eq.f32 	%p26, %f3, 0f3F800000;
	neg.f32 	%f105, %f5;
	selp.f32 	%f106, %f105, %f5, %p26;
	cvt.rmi.f32.f32 	%f107, %f32;
	setp.neu.f32 	%p27, %f107, %f32;
	selp.f32 	%f268, 0f7FFFFFFF, %f106, %p27;
	bra.uni 	$L__BB21_17;

$L__BB21_8:
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.u16 	%rs7, [%rd42];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs7;}

	// end inline asm
	abs.f32 	%f4, %f31;
	setp.lt.f32 	%p10, %f4, 0f00800000;
	mul.f32 	%f38, %f4, 0f4B800000;
	selp.f32 	%f39, %f38, %f4, %p10;
	selp.f32 	%f40, 0fC1C00000, 0f00000000, %p10;
	mov.b32 	%r25, %f39;
	add.s32 	%r26, %r25, -1060439283;
	and.b32  	%r27, %r26, -8388608;
	sub.s32 	%r28, %r25, %r27;
	mov.b32 	%f41, %r28;
	cvt.rn.f32.s32 	%f42, %r27;
	mov.f32 	%f43, 0f34000000;
	fma.rn.f32 	%f44, %f42, %f43, %f40;
	add.f32 	%f45, %f41, 0fBF800000;
	add.f32 	%f46, %f41, 0f3F800000;
	mov.f32 	%f268, 0f3F800000;
	rcp.approx.ftz.f32 	%f47, %f46;
	add.f32 	%f48, %f45, %f45;
	mul.f32 	%f49, %f48, %f47;
	mul.f32 	%f50, %f49, %f49;
	sub.f32 	%f51, %f45, %f49;
	add.f32 	%f52, %f51, %f51;
	neg.f32 	%f53, %f49;
	fma.rn.f32 	%f54, %f53, %f45, %f52;
	mul.rn.f32 	%f55, %f47, %f54;
	mov.f32 	%f56, 0f3B52E7DB;
	mov.f32 	%f57, 0f3A2C32E4;
	fma.rn.f32 	%f58, %f57, %f50, %f56;
	mov.f32 	%f59, 0f3C93BB73;
	fma.rn.f32 	%f60, %f58, %f50, %f59;
	mov.f32 	%f61, 0f3DF6384F;
	fma.rn.f32 	%f62, %f60, %f50, %f61;
	mul.rn.f32 	%f63, %f62, %f50;
	mov.f32 	%f64, 0f3FB8AA3B;
	fma.rn.f32 	%f65, %f49, %f64, %f44;
	sub.f32 	%f66, %f44, %f65;
	fma.rn.f32 	%f67, %f49, %f64, %f66;
	fma.rn.f32 	%f68, %f55, %f64, %f67;
	mov.f32 	%f69, 0f32A55E34;
	fma.rn.f32 	%f70, %f49, %f69, %f68;
	mul.f32 	%f71, %f63, 0f40400000;
	fma.rn.f32 	%f72, %f71, %f55, %f70;
	fma.rn.f32 	%f73, %f63, %f49, %f72;
	add.rn.f32 	%f74, %f65, %f73;
	neg.f32 	%f75, %f65;
	add.rn.f32 	%f76, %f74, %f75;
	neg.f32 	%f77, %f76;
	add.rn.f32 	%f78, %f73, %f77;
	mul.rn.f32 	%f79, %f74, %f32;
	neg.f32 	%f80, %f79;
	fma.rn.f32 	%f81, %f74, %f32, %f80;
	fma.rn.f32 	%f82, %f78, %f32, %f81;
	cvt.rni.f32.f32 	%f83, %f79;
	sub.f32 	%f84, %f79, %f83;
	add.f32 	%f85, %f82, %f84;
	mov.f32 	%f86, 0f3AAF85ED;
	mov.f32 	%f87, 0f391FCB8E;
	fma.rn.f32 	%f88, %f87, %f85, %f86;
	mov.f32 	%f89, 0f3C1D9856;
	fma.rn.f32 	%f90, %f88, %f85, %f89;
	mov.f32 	%f91, 0f3D6357BB;
	fma.rn.f32 	%f92, %f90, %f85, %f91;
	mov.f32 	%f93, 0f3E75FDEC;
	fma.rn.f32 	%f94, %f92, %f85, %f93;
	mov.f32 	%f95, 0f3F317218;
	fma.rn.f32 	%f96, %f94, %f85, %f95;
	fma.rn.f32 	%f97, %f96, %f85, %f268;
	cvt.rzi.s32.f32 	%r29, %f83;
	setp.gt.f32 	%p11, %f83, 0f00000000;
	selp.b32 	%r30, 0, -2097152000, %p11;
	add.s32 	%r31, %r30, 2130706432;
	mov.b32 	%f98, %r31;
	mul.f32 	%f99, %f97, %f98;
	shl.b32 	%r32, %r29, 23;
	sub.s32 	%r33, %r32, %r30;
	mov.b32 	%f100, %r33;
	mul.f32 	%f101, %f99, %f100;
	abs.f32 	%f102, %f79;
	setp.gt.f32 	%p12, %f102, 0f43180000;
	setp.lt.f32 	%p13, %f79, 0f00000000;
	selp.f32 	%f103, 0f00000000, 0f7F800000, %p13;
	selp.f32 	%f5, %f103, %f101, %p12;
	setp.eq.f32 	%p14, %f31, 0f3F800000;
	setp.eq.f32 	%p15, %f32, 0f00000000;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB21_17;

	setp.gtu.f32 	%p17, %f4, 0f7F800000;
	@%p17 bra 	$L__BB21_16;

	abs.f32 	%f6, %f32;
	setp.gtu.f32 	%p18, %f6, 0f7F800000;
	@%p18 bra 	$L__BB21_16;
	bra.uni 	$L__BB21_11;

$L__BB21_16:
	add.rn.f32 	%f268, %f31, %f32;

$L__BB21_17:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f268;}

	// end inline asm
	add.s64 	%rd44, %rd1, %rd41;
	st.global.u16 	[%rd44], %rs9;
	add.s32 	%r79, %r79, %r5;
	cvt.u64.u32 	%rd61, %r79;
	setp.lt.u64 	%p30, %rd61, %rd25;
	@%p30 bra 	$L__BB21_8;
	bra.uni 	$L__BB21_49;

$L__BB21_18:
	// begin inline asm
	{  cvt.f32.f16 %f111, %rs6;}

	// end inline asm
	mul.f32 	%f113, %f111, 0f3F000000;
	cvt.rzi.f32.f32 	%f114, %f113;
	add.f32 	%f115, %f114, %f114;
	sub.f32 	%f116, %f111, %f115;
	abs.f32 	%f13, %f116;
	bra.uni 	$L__BB21_19;

$L__BB21_22:
	setp.eq.f32 	%p40, %f110, 0f00000000;
	setp.eq.f32 	%p41, %f14, 0f7F800000;
	or.pred  	%p42, %p40, %p41;
	@%p42 bra 	$L__BB21_26;
	bra.uni 	$L__BB21_23;

$L__BB21_26:
	setp.eq.f32 	%p49, %f13, 0f3F800000;
	add.f32 	%f187, %f110, %f110;
	mov.b32 	%r48, %f187;
	xor.b32  	%r49, %r48, 2139095040;
	setp.lt.f32 	%p50, %f111, 0f00000000;
	selp.b32 	%r50, %r49, %r48, %p50;
	and.b32  	%r51, %r50, 2147483647;
	selp.b32 	%r52, %r50, %r51, %p49;
	mov.b32 	%f269, %r52;
	bra.uni 	$L__BB21_28;

$L__BB21_23:
	setp.eq.f32 	%p43, %f110, 0fBF800000;
	setp.eq.f32 	%p44, %f16, 0f7F800000;
	and.pred  	%p45, %p43, %p44;
	@%p45 bra 	$L__BB21_28;

	setp.geu.f32 	%p46, %f110, 0f00000000;
	mov.f32 	%f269, %f15;
	@%p46 bra 	$L__BB21_28;

	setp.eq.f32 	%p47, %f13, 0f3F800000;
	neg.f32 	%f184, %f15;
	selp.f32 	%f185, %f184, %f15, %p47;
	cvt.rmi.f32.f32 	%f186, %f111;
	setp.neu.f32 	%p48, %f186, %f111;
	selp.f32 	%f269, 0f7FFFFFFF, %f185, %p48;
	bra.uni 	$L__BB21_28;

$L__BB21_19:
	shl.b64 	%rd45, %rd61, 1;
	add.s64 	%rd11, %rd1, %rd45;
	ld.global.u16 	%rs10, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f110, %rs10;}

	// end inline asm
	abs.f32 	%f14, %f110;
	setp.lt.f32 	%p31, %f14, 0f00800000;
	mul.f32 	%f117, %f14, 0f4B800000;
	selp.f32 	%f118, %f117, %f14, %p31;
	selp.f32 	%f119, 0fC1C00000, 0f00000000, %p31;
	mov.b32 	%r39, %f118;
	add.s32 	%r40, %r39, -1060439283;
	and.b32  	%r41, %r40, -8388608;
	sub.s32 	%r42, %r39, %r41;
	mov.b32 	%f120, %r42;
	cvt.rn.f32.s32 	%f121, %r41;
	mov.f32 	%f122, 0f34000000;
	fma.rn.f32 	%f123, %f121, %f122, %f119;
	add.f32 	%f124, %f120, 0fBF800000;
	add.f32 	%f125, %f120, 0f3F800000;
	mov.f32 	%f269, 0f3F800000;
	rcp.approx.ftz.f32 	%f126, %f125;
	add.f32 	%f127, %f124, %f124;
	mul.f32 	%f128, %f127, %f126;
	mul.f32 	%f129, %f128, %f128;
	sub.f32 	%f130, %f124, %f128;
	add.f32 	%f131, %f130, %f130;
	neg.f32 	%f132, %f128;
	fma.rn.f32 	%f133, %f132, %f124, %f131;
	mul.rn.f32 	%f134, %f126, %f133;
	mov.f32 	%f135, 0f3B52E7DB;
	mov.f32 	%f136, 0f3A2C32E4;
	fma.rn.f32 	%f137, %f136, %f129, %f135;
	mov.f32 	%f138, 0f3C93BB73;
	fma.rn.f32 	%f139, %f137, %f129, %f138;
	mov.f32 	%f140, 0f3DF6384F;
	fma.rn.f32 	%f141, %f139, %f129, %f140;
	mul.rn.f32 	%f142, %f141, %f129;
	mov.f32 	%f143, 0f3FB8AA3B;
	fma.rn.f32 	%f144, %f128, %f143, %f123;
	sub.f32 	%f145, %f123, %f144;
	fma.rn.f32 	%f146, %f128, %f143, %f145;
	fma.rn.f32 	%f147, %f134, %f143, %f146;
	mov.f32 	%f148, 0f32A55E34;
	fma.rn.f32 	%f149, %f128, %f148, %f147;
	mul.f32 	%f150, %f142, 0f40400000;
	fma.rn.f32 	%f151, %f150, %f134, %f149;
	fma.rn.f32 	%f152, %f142, %f128, %f151;
	add.rn.f32 	%f153, %f144, %f152;
	neg.f32 	%f154, %f144;
	add.rn.f32 	%f155, %f153, %f154;
	neg.f32 	%f156, %f155;
	add.rn.f32 	%f157, %f152, %f156;
	mul.rn.f32 	%f158, %f153, %f111;
	neg.f32 	%f159, %f158;
	fma.rn.f32 	%f160, %f153, %f111, %f159;
	fma.rn.f32 	%f161, %f157, %f111, %f160;
	cvt.rni.f32.f32 	%f162, %f158;
	sub.f32 	%f163, %f158, %f162;
	add.f32 	%f164, %f161, %f163;
	mov.f32 	%f165, 0f3AAF85ED;
	mov.f32 	%f166, 0f391FCB8E;
	fma.rn.f32 	%f167, %f166, %f164, %f165;
	mov.f32 	%f168, 0f3C1D9856;
	fma.rn.f32 	%f169, %f167, %f164, %f168;
	mov.f32 	%f170, 0f3D6357BB;
	fma.rn.f32 	%f171, %f169, %f164, %f170;
	mov.f32 	%f172, 0f3E75FDEC;
	fma.rn.f32 	%f173, %f171, %f164, %f172;
	mov.f32 	%f174, 0f3F317218;
	fma.rn.f32 	%f175, %f173, %f164, %f174;
	fma.rn.f32 	%f176, %f175, %f164, %f269;
	cvt.rzi.s32.f32 	%r43, %f162;
	setp.gt.f32 	%p32, %f162, 0f00000000;
	selp.b32 	%r44, 0, -2097152000, %p32;
	add.s32 	%r45, %r44, 2130706432;
	mov.b32 	%f177, %r45;
	mul.f32 	%f178, %f176, %f177;
	shl.b32 	%r46, %r43, 23;
	sub.s32 	%r47, %r46, %r44;
	mov.b32 	%f179, %r47;
	mul.f32 	%f180, %f178, %f179;
	abs.f32 	%f181, %f158;
	setp.gt.f32 	%p33, %f181, 0f43180000;
	setp.lt.f32 	%p34, %f158, 0f00000000;
	selp.f32 	%f182, 0f00000000, 0f7F800000, %p34;
	selp.f32 	%f15, %f182, %f180, %p33;
	setp.eq.f32 	%p35, %f110, 0f3F800000;
	setp.eq.f32 	%p36, %f111, 0f00000000;
	or.pred  	%p37, %p35, %p36;
	@%p37 bra 	$L__BB21_28;

	setp.gtu.f32 	%p38, %f14, 0f7F800000;
	@%p38 bra 	$L__BB21_27;

	abs.f32 	%f16, %f111;
	setp.gtu.f32 	%p39, %f16, 0f7F800000;
	@%p39 bra 	$L__BB21_27;
	bra.uni 	$L__BB21_22;

$L__BB21_27:
	add.rn.f32 	%f269, %f110, %f111;

$L__BB21_28:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f269;}

	// end inline asm
	st.global.u16 	[%rd11], %rs12;
	add.s32 	%r79, %r79, %r5;
	cvt.u64.u32 	%rd61, %r79;
	setp.lt.u64 	%p51, %rd61, %rd25;
	@%p51 bra 	$L__BB21_19;

$L__BB21_49:
	ret;

}
	// .globl	ucopy_u8
.visible .entry ucopy_u8(
	.param .u64 ucopy_u8_param_0,
	.param .u64 ucopy_u8_param_1,
	.param .u64 ucopy_u8_param_2,
	.param .u64 ucopy_u8_param_3,
	.param .u64 ucopy_u8_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [ucopy_u8_param_0];
	ld.param.u64 	%rd25, [ucopy_u8_param_1];
	ld.param.u64 	%rd27, [ucopy_u8_param_2];
	ld.param.u64 	%rd26, [ucopy_u8_param_3];
	ld.param.u64 	%rd28, [ucopy_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB22_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r36, 0;

$L__BB22_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd30, %r22;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd59, %rd35;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB22_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd59, %rd38, %rd59;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd39, %r36;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB22_2;

$L__BB22_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd60, %r4;
	@%p20 bra 	$L__BB22_8;
	bra.uni 	$L__BB22_5;

$L__BB22_8:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB22_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB22_16;

$L__BB22_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB22_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd42, %r29;
	add.s64 	%rd43, %rd42, %rd25;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd13, %rd3, %rd45;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd46, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd46, 0;
	@%p13 bra 	$L__BB22_13;

	div.u64 	%rd62, %rd12, %rd14;
	mul.lo.s64 	%rd47, %rd62, %rd14;
	sub.s64 	%rd63, %rd12, %rd47;
	bra.uni 	$L__BB22_14;

$L__BB22_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd62, %r32;
	cvt.u64.u32 	%rd63, %r34;

$L__BB22_14:
	shl.b64 	%rd48, %rd25, 3;
	add.s64 	%rd49, %rd13, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r35, %rd51;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd62;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd52, %r39;
	setp.lt.u64 	%p14, %rd52, %rd25;
	@%p14 bra 	$L__BB22_11;

	setp.eq.s64 	%p15, %rd26, 0;
	cvt.u64.u32 	%rd53, %r41;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd55, %rd1, %rd60;
	selp.b64 	%rd56, %rd55, %rd54, %p15;
	ld.global.u8 	%rs2, [%rd56];
	st.global.u8 	[%rd55], %rs2;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd60, %r4;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB22_10;
	bra.uni 	$L__BB22_19;

$L__BB22_5:
	setp.ge.u64 	%p8, %rd60, %rd24;
	@%p8 bra 	$L__BB22_19;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB22_7:
	add.s64 	%rd40, %rd8, %rd60;
	ld.global.u8 	%rs1, [%rd40];
	add.s64 	%rd41, %rd1, %rd60;
	st.global.u8 	[%rd41], %rs1;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd60, %r4;
	setp.lt.u64 	%p10, %rd60, %rd24;
	@%p10 bra 	$L__BB22_7;

$L__BB22_19:
	ret;

$L__BB22_16:
	setp.eq.s64 	%p17, %rd26, 0;
	@%p17 bra 	$L__BB22_18;

$L__BB22_17:
	ld.global.u8 	%rs3, [%rd2];
	add.s64 	%rd57, %rd1, %rd60;
	st.global.u8 	[%rd57], %rs3;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd60, %r4;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB22_17;
	bra.uni 	$L__BB22_19;

$L__BB22_18:
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd58, %r4;
	setp.lt.u64 	%p19, %rd58, %rd24;
	@%p19 bra 	$L__BB22_18;
	bra.uni 	$L__BB22_19;

}
	// .globl	ucopy_u32
.visible .entry ucopy_u32(
	.param .u64 ucopy_u32_param_0,
	.param .u64 ucopy_u32_param_1,
	.param .u64 ucopy_u32_param_2,
	.param .u64 ucopy_u32_param_3,
	.param .u64 ucopy_u32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [ucopy_u32_param_0];
	ld.param.u64 	%rd25, [ucopy_u32_param_1];
	ld.param.u64 	%rd27, [ucopy_u32_param_2];
	ld.param.u64 	%rd26, [ucopy_u32_param_3];
	ld.param.u64 	%rd28, [ucopy_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB23_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r39, 0;

$L__BB23_2:
	not.b32 	%r22, %r39;
	cvt.u64.u32 	%rd30, %r22;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd62, %rd35;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB23_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd62, %rd38, %rd62;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd39, %r39;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB23_2;

$L__BB23_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd63, %r4;
	@%p20 bra 	$L__BB23_8;
	bra.uni 	$L__BB23_5;

$L__BB23_8:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB23_19;

	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r27;
	@%p3 bra 	$L__BB23_16;

$L__BB23_10:
	mov.u32 	%r42, 0;
	mov.u32 	%r43, %r4;
	mov.u32 	%r44, %r42;

$L__BB23_11:
	not.b32 	%r30, %r42;
	cvt.u64.u32 	%rd43, %r30;
	add.s64 	%rd44, %rd43, %rd25;
	cvt.u64.u32 	%rd12, %r43;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB23_13;

	div.u64 	%rd65, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd65, %rd14;
	sub.s64 	%rd66, %rd12, %rd48;
	bra.uni 	$L__BB23_14;

$L__BB23_13:
	cvt.u32.u64 	%r31, %rd14;
	cvt.u32.u64 	%r32, %rd12;
	div.u32 	%r33, %r32, %r31;
	mul.lo.s32 	%r34, %r33, %r31;
	sub.s32 	%r35, %r32, %r34;
	cvt.u64.u32 	%rd65, %r33;
	cvt.u64.u32 	%rd66, %r35;

$L__BB23_14:
	shl.b64 	%rd49, %rd25, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r36, %rd52;
	add.s32 	%r44, %r44, %r36;
	cvt.u32.u64 	%r43, %rd65;
	add.s32 	%r42, %r42, 1;
	cvt.u64.u32 	%rd53, %r42;
	setp.lt.u64 	%p14, %rd53, %rd25;
	@%p14 bra 	$L__BB23_11;

	setp.eq.s64 	%p15, %rd26, 0;
	mul.wide.u32 	%rd54, %r44, 4;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd63, 2;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p15;
	ld.global.u32 	%r37, [%rd58];
	st.global.u32 	[%rd57], %r37;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB23_10;
	bra.uni 	$L__BB23_19;

$L__BB23_5:
	setp.ge.u64 	%p8, %rd63, %rd24;
	@%p8 bra 	$L__BB23_19;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB23_7:
	shl.b64 	%rd40, %rd63, 2;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.u32 	%r26, [%rd41];
	add.s64 	%rd42, %rd1, %rd40;
	st.global.u32 	[%rd42], %r26;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p10, %rd63, %rd24;
	@%p10 bra 	$L__BB23_7;

$L__BB23_19:
	ret;

$L__BB23_16:
	setp.eq.s64 	%p17, %rd26, 0;
	@%p17 bra 	$L__BB23_18;

$L__BB23_17:
	ld.global.u32 	%r38, [%rd2];
	shl.b64 	%rd59, %rd63, 2;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.u32 	[%rd60], %r38;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB23_17;
	bra.uni 	$L__BB23_19;

$L__BB23_18:
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd61, %r4;
	setp.lt.u64 	%p19, %rd61, %rd24;
	@%p19 bra 	$L__BB23_18;
	bra.uni 	$L__BB23_19;

}
	// .globl	ucopy_i64
.visible .entry ucopy_i64(
	.param .u64 ucopy_i64_param_0,
	.param .u64 ucopy_i64_param_1,
	.param .u64 ucopy_i64_param_2,
	.param .u64 ucopy_i64_param_3,
	.param .u64 ucopy_i64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd24, [ucopy_i64_param_0];
	ld.param.u64 	%rd25, [ucopy_i64_param_1];
	ld.param.u64 	%rd27, [ucopy_i64_param_2];
	ld.param.u64 	%rd26, [ucopy_i64_param_3];
	ld.param.u64 	%rd28, [ucopy_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB24_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB24_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd30, %r22;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd65, %rd35;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB24_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd65, %rd38, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd39, %r36;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB24_2;

$L__BB24_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd66, %r4;
	@%p20 bra 	$L__BB24_8;
	bra.uni 	$L__BB24_5;

$L__BB24_8:
	setp.ge.u64 	%p11, %rd66, %rd24;
	@%p11 bra 	$L__BB24_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB24_16;

$L__BB24_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB24_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd44, %r29;
	add.s64 	%rd45, %rd44, %rd25;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd13, %rd3, %rd47;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd48, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB24_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd49, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd49;
	bra.uni 	$L__BB24_14;

$L__BB24_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB24_14:
	shl.b64 	%rd50, %rd25, 3;
	add.s64 	%rd51, %rd13, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd69;
	cvt.u32.u64 	%r35, %rd53;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd54, %r39;
	setp.lt.u64 	%p14, %rd54, %rd25;
	@%p14 bra 	$L__BB24_11;

	setp.eq.s64 	%p15, %rd26, 0;
	mul.wide.u32 	%rd55, %r41, 8;
	add.s64 	%rd56, %rd2, %rd55;
	shl.b64 	%rd57, %rd66, 3;
	add.s64 	%rd58, %rd1, %rd57;
	selp.b64 	%rd59, %rd58, %rd56, %p15;
	ld.global.u64 	%rd60, [%rd59];
	st.global.u64 	[%rd58], %rd60;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd66, %r4;
	setp.lt.u64 	%p16, %rd66, %rd24;
	@%p16 bra 	$L__BB24_10;
	bra.uni 	$L__BB24_19;

$L__BB24_5:
	setp.ge.u64 	%p8, %rd66, %rd24;
	@%p8 bra 	$L__BB24_19;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB24_7:
	shl.b64 	%rd40, %rd66, 3;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	add.s64 	%rd43, %rd1, %rd40;
	st.global.u64 	[%rd43], %rd42;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd66, %r4;
	setp.lt.u64 	%p10, %rd66, %rd24;
	@%p10 bra 	$L__BB24_7;

$L__BB24_19:
	ret;

$L__BB24_16:
	setp.eq.s64 	%p17, %rd26, 0;
	@%p17 bra 	$L__BB24_18;

$L__BB24_17:
	ld.global.u64 	%rd61, [%rd2];
	shl.b64 	%rd62, %rd66, 3;
	add.s64 	%rd63, %rd1, %rd62;
	st.global.u64 	[%rd63], %rd61;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd66, %r4;
	setp.lt.u64 	%p18, %rd66, %rd24;
	@%p18 bra 	$L__BB24_17;
	bra.uni 	$L__BB24_19;

$L__BB24_18:
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd64, %r4;
	setp.lt.u64 	%p19, %rd64, %rd24;
	@%p19 bra 	$L__BB24_18;
	bra.uni 	$L__BB24_19;

}
	// .globl	ucopy_f32
.visible .entry ucopy_f32(
	.param .u64 ucopy_f32_param_0,
	.param .u64 ucopy_f32_param_1,
	.param .u64 ucopy_f32_param_2,
	.param .u64 ucopy_f32_param_3,
	.param .u64 ucopy_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [ucopy_f32_param_0];
	ld.param.u64 	%rd25, [ucopy_f32_param_1];
	ld.param.u64 	%rd27, [ucopy_f32_param_2];
	ld.param.u64 	%rd26, [ucopy_f32_param_3];
	ld.param.u64 	%rd28, [ucopy_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB25_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r36, 0;

$L__BB25_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd30, %r22;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd62, %rd35;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB25_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd62, %rd38, %rd62;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd39, %r36;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB25_2;

$L__BB25_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd63, %r4;
	@%p20 bra 	$L__BB25_8;
	bra.uni 	$L__BB25_5;

$L__BB25_8:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB25_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB25_16;

$L__BB25_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB25_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd43, %r29;
	add.s64 	%rd44, %rd43, %rd25;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB25_13;

	div.u64 	%rd65, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd65, %rd14;
	sub.s64 	%rd66, %rd12, %rd48;
	bra.uni 	$L__BB25_14;

$L__BB25_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd65, %r32;
	cvt.u64.u32 	%rd66, %r34;

$L__BB25_14:
	shl.b64 	%rd49, %rd25, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r35, %rd52;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd65;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd53, %r39;
	setp.lt.u64 	%p14, %rd53, %rd25;
	@%p14 bra 	$L__BB25_11;

	setp.eq.s64 	%p15, %rd26, 0;
	mul.wide.u32 	%rd54, %r41, 4;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd63, 2;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p15;
	ld.global.f32 	%f2, [%rd58];
	st.global.f32 	[%rd57], %f2;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB25_10;
	bra.uni 	$L__BB25_19;

$L__BB25_5:
	setp.ge.u64 	%p8, %rd63, %rd24;
	@%p8 bra 	$L__BB25_19;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB25_7:
	shl.b64 	%rd40, %rd63, 2;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.f32 	%f1, [%rd41];
	add.s64 	%rd42, %rd1, %rd40;
	st.global.f32 	[%rd42], %f1;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p10, %rd63, %rd24;
	@%p10 bra 	$L__BB25_7;

$L__BB25_19:
	ret;

$L__BB25_16:
	setp.eq.s64 	%p17, %rd26, 0;
	@%p17 bra 	$L__BB25_18;

$L__BB25_17:
	ld.global.f32 	%f3, [%rd2];
	shl.b64 	%rd59, %rd63, 2;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.f32 	[%rd60], %f3;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB25_17;
	bra.uni 	$L__BB25_19;

$L__BB25_18:
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd61, %r4;
	setp.lt.u64 	%p19, %rd61, %rd24;
	@%p19 bra 	$L__BB25_18;
	bra.uni 	$L__BB25_19;

}
	// .globl	ucopy_f64
.visible .entry ucopy_f64(
	.param .u64 ucopy_f64_param_0,
	.param .u64 ucopy_f64_param_1,
	.param .u64 ucopy_f64_param_2,
	.param .u64 ucopy_f64_param_3,
	.param .u64 ucopy_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [ucopy_f64_param_0];
	ld.param.u64 	%rd25, [ucopy_f64_param_1];
	ld.param.u64 	%rd27, [ucopy_f64_param_2];
	ld.param.u64 	%rd26, [ucopy_f64_param_3];
	ld.param.u64 	%rd28, [ucopy_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB26_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r36, 0;

$L__BB26_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd30, %r22;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd62, %rd35;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB26_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd62, %rd38, %rd62;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd39, %r36;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB26_2;

$L__BB26_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd63, %r4;
	@%p20 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_5;

$L__BB26_8:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB26_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB26_16;

$L__BB26_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB26_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd43, %r29;
	add.s64 	%rd44, %rd43, %rd25;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB26_13;

	div.u64 	%rd65, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd65, %rd14;
	sub.s64 	%rd66, %rd12, %rd48;
	bra.uni 	$L__BB26_14;

$L__BB26_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd65, %r32;
	cvt.u64.u32 	%rd66, %r34;

$L__BB26_14:
	shl.b64 	%rd49, %rd25, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r35, %rd52;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd65;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd53, %r39;
	setp.lt.u64 	%p14, %rd53, %rd25;
	@%p14 bra 	$L__BB26_11;

	setp.eq.s64 	%p15, %rd26, 0;
	mul.wide.u32 	%rd54, %r41, 8;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd63, 3;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p15;
	ld.global.f64 	%fd2, [%rd58];
	st.global.f64 	[%rd57], %fd2;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB26_10;
	bra.uni 	$L__BB26_19;

$L__BB26_5:
	setp.ge.u64 	%p8, %rd63, %rd24;
	@%p8 bra 	$L__BB26_19;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB26_7:
	shl.b64 	%rd40, %rd63, 3;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.f64 	%fd1, [%rd41];
	add.s64 	%rd42, %rd1, %rd40;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p10, %rd63, %rd24;
	@%p10 bra 	$L__BB26_7;

$L__BB26_19:
	ret;

$L__BB26_16:
	setp.eq.s64 	%p17, %rd26, 0;
	@%p17 bra 	$L__BB26_18;

$L__BB26_17:
	ld.global.f64 	%fd3, [%rd2];
	shl.b64 	%rd59, %rd63, 3;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.f64 	[%rd60], %fd3;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd63, %r4;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB26_17;
	bra.uni 	$L__BB26_19;

$L__BB26_18:
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd61, %r4;
	setp.lt.u64 	%p19, %rd61, %rd24;
	@%p19 bra 	$L__BB26_18;
	bra.uni 	$L__BB26_19;

}
	// .globl	uneg_f32
.visible .entry uneg_f32(
	.param .u64 uneg_f32_param_0,
	.param .u64 uneg_f32_param_1,
	.param .u64 uneg_f32_param_2,
	.param .u64 uneg_f32_param_3,
	.param .u64 uneg_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uneg_f32_param_0];
	ld.param.u64 	%rd27, [uneg_f32_param_1];
	ld.param.u64 	%rd29, [uneg_f32_param_2];
	ld.param.u64 	%rd28, [uneg_f32_param_3];
	ld.param.u64 	%rd30, [uneg_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB27_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB27_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB27_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB27_2;

$L__BB27_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB27_8;
	bra.uni 	$L__BB27_5;

$L__BB27_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB27_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB27_16;

$L__BB27_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB27_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB27_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB27_14;

$L__BB27_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB27_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB27_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	neg.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB27_10;
	bra.uni 	$L__BB27_19;

$L__BB27_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB27_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB27_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	neg.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB27_7;

$L__BB27_19:
	ret;

$L__BB27_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB27_18;

$L__BB27_17:
	ld.global.f32 	%f5, [%rd2];
	neg.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB27_17;
	bra.uni 	$L__BB27_19;

$L__BB27_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	neg.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB27_18;
	bra.uni 	$L__BB27_19;

}
	// .globl	uneg_f64
.visible .entry uneg_f64(
	.param .u64 uneg_f64_param_0,
	.param .u64 uneg_f64_param_1,
	.param .u64 uneg_f64_param_2,
	.param .u64 uneg_f64_param_3,
	.param .u64 uneg_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uneg_f64_param_0];
	ld.param.u64 	%rd27, [uneg_f64_param_1];
	ld.param.u64 	%rd29, [uneg_f64_param_2];
	ld.param.u64 	%rd28, [uneg_f64_param_3];
	ld.param.u64 	%rd30, [uneg_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB28_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB28_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB28_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB28_2;

$L__BB28_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_5;

$L__BB28_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB28_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB28_16;

$L__BB28_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB28_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB28_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB28_14;

$L__BB28_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB28_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB28_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	neg.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB28_10;
	bra.uni 	$L__BB28_19;

$L__BB28_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB28_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB28_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	neg.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB28_7;

$L__BB28_19:
	ret;

$L__BB28_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB28_18;

$L__BB28_17:
	ld.global.f64 	%fd5, [%rd2];
	neg.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB28_17;
	bra.uni 	$L__BB28_19;

$L__BB28_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	neg.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB28_18;
	bra.uni 	$L__BB28_19;

}
	// .globl	urecip_f32
.visible .entry urecip_f32(
	.param .u64 urecip_f32_param_0,
	.param .u64 urecip_f32_param_1,
	.param .u64 urecip_f32_param_2,
	.param .u64 urecip_f32_param_3,
	.param .u64 urecip_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [urecip_f32_param_0];
	ld.param.u64 	%rd27, [urecip_f32_param_1];
	ld.param.u64 	%rd29, [urecip_f32_param_2];
	ld.param.u64 	%rd28, [urecip_f32_param_3];
	ld.param.u64 	%rd30, [urecip_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB29_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB29_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB29_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB29_2;

$L__BB29_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB29_8;
	bra.uni 	$L__BB29_5;

$L__BB29_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB29_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB29_16;

$L__BB29_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB29_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB29_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB29_14;

$L__BB29_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB29_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB29_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	rcp.rn.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB29_10;
	bra.uni 	$L__BB29_19;

$L__BB29_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB29_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB29_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	rcp.rn.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB29_7;

$L__BB29_19:
	ret;

$L__BB29_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB29_18;

$L__BB29_17:
	ld.global.f32 	%f5, [%rd2];
	rcp.rn.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB29_17;
	bra.uni 	$L__BB29_19;

$L__BB29_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	rcp.rn.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB29_18;
	bra.uni 	$L__BB29_19;

}
	// .globl	urecip_f64
.visible .entry urecip_f64(
	.param .u64 urecip_f64_param_0,
	.param .u64 urecip_f64_param_1,
	.param .u64 urecip_f64_param_2,
	.param .u64 urecip_f64_param_3,
	.param .u64 urecip_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [urecip_f64_param_0];
	ld.param.u64 	%rd27, [urecip_f64_param_1];
	ld.param.u64 	%rd29, [urecip_f64_param_2];
	ld.param.u64 	%rd28, [urecip_f64_param_3];
	ld.param.u64 	%rd30, [urecip_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB30_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB30_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB30_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB30_2;

$L__BB30_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB30_8;
	bra.uni 	$L__BB30_5;

$L__BB30_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB30_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB30_16;

$L__BB30_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB30_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB30_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB30_14;

$L__BB30_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB30_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB30_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	rcp.rn.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB30_10;
	bra.uni 	$L__BB30_19;

$L__BB30_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB30_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB30_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	rcp.rn.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB30_7;

$L__BB30_19:
	ret;

$L__BB30_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB30_18;

$L__BB30_17:
	ld.global.f64 	%fd5, [%rd2];
	rcp.rn.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB30_17;
	bra.uni 	$L__BB30_19;

$L__BB30_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	rcp.rn.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB30_18;
	bra.uni 	$L__BB30_19;

}
	// .globl	uexp_f32
.visible .entry uexp_f32(
	.param .u64 uexp_f32_param_0,
	.param .u64 uexp_f32_param_1,
	.param .u64 uexp_f32_param_2,
	.param .u64 uexp_f32_param_3,
	.param .u64 uexp_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uexp_f32_param_0];
	ld.param.u64 	%rd27, [uexp_f32_param_1];
	ld.param.u64 	%rd29, [uexp_f32_param_2];
	ld.param.u64 	%rd28, [uexp_f32_param_3];
	ld.param.u64 	%rd30, [uexp_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB31_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r44, 0;

$L__BB31_2:
	not.b32 	%r22, %r44;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB31_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r44, %r44, 1;
	cvt.u64.u32 	%rd41, %r44;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB31_2;

$L__BB31_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB31_8;
	bra.uni 	$L__BB31_5;

$L__BB31_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB31_19;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r28;
	@%p3 bra 	$L__BB31_16;

$L__BB31_10:
	mov.u32 	%r47, 0;
	mov.u32 	%r48, %r4;
	mov.u32 	%r49, %r47;

$L__BB31_11:
	not.b32 	%r31, %r47;
	cvt.u64.u32 	%rd45, %r31;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r48;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB31_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB31_14;

$L__BB31_13:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd68, %r34;
	cvt.u64.u32 	%rd69, %r36;

$L__BB31_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r37, %rd54;
	add.s32 	%r49, %r49, %r37;
	cvt.u32.u64 	%r48, %rd68;
	add.s32 	%r47, %r47, 1;
	cvt.u64.u32 	%rd55, %r47;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB31_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r49, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f18, [%rd60];
	mov.f32 	%f19, 0f3F000000;
	mov.f32 	%f20, 0f3BBB989D;
	fma.rn.f32 	%f21, %f18, %f20, %f19;
	cvt.sat.f32.f32 	%f22, %f21;
	mov.f32 	%f23, 0f4B400001;
	mov.f32 	%f24, 0f437C0000;
	fma.rm.f32 	%f25, %f22, %f24, %f23;
	add.f32 	%f26, %f25, 0fCB40007F;
	neg.f32 	%f27, %f26;
	mov.f32 	%f28, 0f3FB8AA3B;
	fma.rn.f32 	%f29, %f18, %f28, %f27;
	mov.f32 	%f30, 0f32A57060;
	fma.rn.f32 	%f31, %f18, %f30, %f29;
	mov.b32 	%r38, %f25;
	shl.b32 	%r39, %r38, 23;
	mov.b32 	%f32, %r39;
	ex2.approx.ftz.f32 	%f33, %f31;
	mul.f32 	%f34, %f33, %f32;
	st.global.f32 	[%rd59], %f34;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB31_10;
	bra.uni 	$L__BB31_19;

$L__BB31_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB31_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB31_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	mov.f32 	%f2, 0f3F000000;
	mov.f32 	%f3, 0f3BBB989D;
	fma.rn.f32 	%f4, %f1, %f3, %f2;
	cvt.sat.f32.f32 	%f5, %f4;
	mov.f32 	%f6, 0f4B400001;
	mov.f32 	%f7, 0f437C0000;
	fma.rm.f32 	%f8, %f5, %f7, %f6;
	add.f32 	%f9, %f8, 0fCB40007F;
	neg.f32 	%f10, %f9;
	mov.f32 	%f11, 0f3FB8AA3B;
	fma.rn.f32 	%f12, %f1, %f11, %f10;
	mov.f32 	%f13, 0f32A57060;
	fma.rn.f32 	%f14, %f1, %f13, %f12;
	mov.b32 	%r26, %f8;
	shl.b32 	%r27, %r26, 23;
	mov.b32 	%f15, %r27;
	ex2.approx.ftz.f32 	%f16, %f14;
	mul.f32 	%f17, %f16, %f15;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f17;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB31_7;

$L__BB31_19:
	ret;

$L__BB31_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB31_18;

$L__BB31_17:
	ld.global.f32 	%f35, [%rd2];
	mov.f32 	%f36, 0f3F000000;
	mov.f32 	%f37, 0f3BBB989D;
	fma.rn.f32 	%f38, %f35, %f37, %f36;
	cvt.sat.f32.f32 	%f39, %f38;
	mov.f32 	%f40, 0f4B400001;
	mov.f32 	%f41, 0f437C0000;
	fma.rm.f32 	%f42, %f39, %f41, %f40;
	add.f32 	%f43, %f42, 0fCB40007F;
	neg.f32 	%f44, %f43;
	mov.f32 	%f45, 0f3FB8AA3B;
	fma.rn.f32 	%f46, %f35, %f45, %f44;
	mov.f32 	%f47, 0f32A57060;
	fma.rn.f32 	%f48, %f35, %f47, %f46;
	mov.b32 	%r40, %f42;
	shl.b32 	%r41, %r40, 23;
	mov.b32 	%f49, %r41;
	ex2.approx.ftz.f32 	%f50, %f48;
	mul.f32 	%f51, %f50, %f49;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f51;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB31_17;
	bra.uni 	$L__BB31_19;

$L__BB31_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f52, [%rd64];
	mov.f32 	%f53, 0f3F000000;
	mov.f32 	%f54, 0f3BBB989D;
	fma.rn.f32 	%f55, %f52, %f54, %f53;
	cvt.sat.f32.f32 	%f56, %f55;
	mov.f32 	%f57, 0f4B400001;
	mov.f32 	%f58, 0f437C0000;
	fma.rm.f32 	%f59, %f56, %f58, %f57;
	add.f32 	%f60, %f59, 0fCB40007F;
	neg.f32 	%f61, %f60;
	mov.f32 	%f62, 0f3FB8AA3B;
	fma.rn.f32 	%f63, %f52, %f62, %f61;
	mov.f32 	%f64, 0f32A57060;
	fma.rn.f32 	%f65, %f52, %f64, %f63;
	mov.b32 	%r42, %f59;
	shl.b32 	%r43, %r42, 23;
	mov.b32 	%f66, %r43;
	ex2.approx.ftz.f32 	%f67, %f65;
	mul.f32 	%f68, %f67, %f66;
	st.global.f32 	[%rd64], %f68;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB31_18;
	bra.uni 	$L__BB31_19;

}
	// .globl	uexp_f64
.visible .entry uexp_f64(
	.param .u64 uexp_f64_param_0,
	.param .u64 uexp_f64_param_1,
	.param .u64 uexp_f64_param_2,
	.param .u64 uexp_f64_param_3,
	.param .u64 uexp_f64_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<121>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd26, [uexp_f64_param_0];
	ld.param.u64 	%rd27, [uexp_f64_param_1];
	ld.param.u64 	%rd29, [uexp_f64_param_2];
	ld.param.u64 	%rd28, [uexp_f64_param_3];
	ld.param.u64 	%rd30, [uexp_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB32_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r79, 0;

$L__BB32_2:
	not.b32 	%r29, %r79;
	cvt.u64.u32 	%rd32, %r29;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd64, %rd37;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB32_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd64, %rd40, %rd64;
	add.s32 	%r79, %r79, 1;
	cvt.u64.u32 	%rd41, %r79;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB32_2;

$L__BB32_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %tid.x;
	mad.lo.s32 	%r80, %r30, %r3, %r31;
	cvt.u64.u32 	%rd65, %r80;
	@%p28 bra 	$L__BB32_11;
	bra.uni 	$L__BB32_5;

$L__BB32_11:
	setp.ge.u64 	%p14, %rd65, %rd26;
	@%p14 bra 	$L__BB32_26;

	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r45;
	@%p3 bra 	$L__BB32_22;

$L__BB32_13:
	mov.u32 	%r82, 0;
	mov.u32 	%r83, %r80;
	mov.u32 	%r84, %r82;

$L__BB32_14:
	not.b32 	%r48, %r82;
	cvt.u64.u32 	%rd46, %r48;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r83;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p16, %rd50, 0;
	@%p16 bra 	$L__BB32_16;

	div.u64 	%rd67, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd67, %rd14;
	sub.s64 	%rd68, %rd12, %rd51;
	bra.uni 	$L__BB32_17;

$L__BB32_16:
	cvt.u32.u64 	%r49, %rd14;
	cvt.u32.u64 	%r50, %rd12;
	div.u32 	%r51, %r50, %r49;
	mul.lo.s32 	%r52, %r51, %r49;
	sub.s32 	%r53, %r50, %r52;
	cvt.u64.u32 	%rd67, %r51;
	cvt.u64.u32 	%rd68, %r53;

$L__BB32_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd68;
	cvt.u32.u64 	%r54, %rd55;
	add.s32 	%r84, %r84, %r54;
	cvt.u32.u64 	%r83, %rd67;
	add.s32 	%r82, %r82, 1;
	cvt.u64.u32 	%rd56, %r82;
	setp.lt.u64 	%p17, %rd56, %rd27;
	@%p17 bra 	$L__BB32_14;

	setp.eq.s64 	%p18, %rd28, 0;
	mul.wide.u32 	%rd57, %r84, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd65, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p18;
	ld.global.f64 	%fd6, [%rd60];
	mov.f64 	%fd50, 0d4338000000000000;
	mov.f64 	%fd51, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd52, %fd6, %fd51, %fd50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd52;
	}
	mov.f64 	%fd53, 0dC338000000000000;
	add.rn.f64 	%fd54, %fd52, %fd53;
	mov.f64 	%fd55, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd56, %fd54, %fd55, %fd6;
	mov.f64 	%fd57, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd58, %fd54, %fd57, %fd56;
	mov.f64 	%fd59, 0d3E928AF3FCA213EA;
	mov.f64 	%fd60, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd61, %fd60, %fd58, %fd59;
	mov.f64 	%fd62, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	mov.f64 	%fd64, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	mov.f64 	%fd66, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	mov.f64 	%fd68, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	mov.f64 	%fd70, 0d3F81111111122322;
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	mov.f64 	%fd72, 0d3FA55555555502A1;
	fma.rn.f64 	%fd73, %fd71, %fd58, %fd72;
	mov.f64 	%fd74, 0d3FC5555555555511;
	fma.rn.f64 	%fd75, %fd73, %fd58, %fd74;
	mov.f64 	%fd76, 0d3FE000000000000B;
	fma.rn.f64 	%fd77, %fd75, %fd58, %fd76;
	mov.f64 	%fd78, 0d3FF0000000000000;
	fma.rn.f64 	%fd79, %fd77, %fd58, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd58, %fd78;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd80;
	}
	shl.b32 	%r55, %r19, 20;
	add.s32 	%r56, %r21, %r55;
	mov.b64 	%fd119, {%r20, %r56};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd6;
	}
	mov.b32 	%f5, %r57;
	abs.f32 	%f2, %f5;
	setp.lt.f32 	%p19, %f2, 0f4086232B;
	@%p19 bra 	$L__BB32_21;

	setp.lt.f64 	%p20, %fd6, 0d0000000000000000;
	add.f64 	%fd81, %fd6, 0d7FF0000000000000;
	selp.f64 	%fd119, 0d0000000000000000, %fd81, %p20;
	setp.geu.f32 	%p21, %f2, 0f40874800;
	@%p21 bra 	$L__BB32_21;

	shr.u32 	%r58, %r19, 31;
	add.s32 	%r59, %r19, %r58;
	shr.s32 	%r60, %r59, 1;
	shl.b32 	%r61, %r60, 20;
	add.s32 	%r62, %r21, %r61;
	mov.b64 	%fd82, {%r20, %r62};
	sub.s32 	%r63, %r19, %r60;
	shl.b32 	%r64, %r63, 20;
	add.s32 	%r65, %r64, 1072693248;
	mov.u32 	%r66, 0;
	mov.b64 	%fd83, {%r66, %r65};
	mul.f64 	%fd119, %fd82, %fd83;

$L__BB32_21:
	ld.param.u64 	%rd63, [uexp_f64_param_0];
	st.global.f64 	[%rd21], %fd119;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd65, %r80;
	setp.lt.u64 	%p22, %rd65, %rd63;
	@%p22 bra 	$L__BB32_13;
	bra.uni 	$L__BB32_26;

$L__BB32_22:
	shl.b64 	%rd61, %rd65, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p23, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p23;
	ld.global.f64 	%fd11, [%rd62];
	mov.f64 	%fd84, 0d4338000000000000;
	mov.f64 	%fd85, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd86, %fd11, %fd85, %fd84;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd86;
	}
	mov.f64 	%fd87, 0dC338000000000000;
	add.rn.f64 	%fd88, %fd86, %fd87;
	mov.f64 	%fd89, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd90, %fd88, %fd89, %fd11;
	mov.f64 	%fd91, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd92, %fd88, %fd91, %fd90;
	mov.f64 	%fd93, 0d3E928AF3FCA213EA;
	mov.f64 	%fd94, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F81111111122322;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	mov.f64 	%fd106, 0d3FA55555555502A1;
	fma.rn.f64 	%fd107, %fd105, %fd92, %fd106;
	mov.f64 	%fd108, 0d3FC5555555555511;
	fma.rn.f64 	%fd109, %fd107, %fd92, %fd108;
	mov.f64 	%fd110, 0d3FE000000000000B;
	fma.rn.f64 	%fd111, %fd109, %fd92, %fd110;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd92, %fd112;
	fma.rn.f64 	%fd114, %fd113, %fd92, %fd112;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd114;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd114;
	}
	shl.b32 	%r67, %r24, 20;
	add.s32 	%r68, %r26, %r67;
	mov.b64 	%fd120, {%r25, %r68};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd11;
	}
	mov.b32 	%f6, %r69;
	abs.f32 	%f3, %f6;
	setp.lt.f32 	%p24, %f3, 0f4086232B;
	@%p24 bra 	$L__BB32_25;

	setp.lt.f64 	%p25, %fd11, 0d0000000000000000;
	add.f64 	%fd115, %fd11, 0d7FF0000000000000;
	selp.f64 	%fd120, 0d0000000000000000, %fd115, %p25;
	setp.geu.f32 	%p26, %f3, 0f40874800;
	@%p26 bra 	$L__BB32_25;

	shr.u32 	%r70, %r24, 31;
	add.s32 	%r71, %r24, %r70;
	shr.s32 	%r72, %r71, 1;
	shl.b32 	%r73, %r72, 20;
	add.s32 	%r74, %r26, %r73;
	mov.b64 	%fd116, {%r25, %r74};
	sub.s32 	%r75, %r24, %r72;
	shl.b32 	%r76, %r75, 20;
	add.s32 	%r77, %r76, 1072693248;
	mov.u32 	%r78, 0;
	mov.b64 	%fd117, {%r78, %r77};
	mul.f64 	%fd120, %fd116, %fd117;

$L__BB32_25:
	st.global.f64 	[%rd24], %fd120;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd65, %r80;
	setp.lt.u64 	%p27, %rd65, %rd26;
	@%p27 bra 	$L__BB32_22;
	bra.uni 	$L__BB32_26;

$L__BB32_5:
	setp.ge.u64 	%p8, %rd65, %rd26;
	@%p8 bra 	$L__BB32_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r32;

$L__BB32_7:
	shl.b64 	%rd42, %rd65, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	mov.f64 	%fd16, 0d4338000000000000;
	mov.f64 	%fd17, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd18, %fd1, %fd17, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd18;
	}
	mov.f64 	%fd19, 0dC338000000000000;
	add.rn.f64 	%fd20, %fd18, %fd19;
	mov.f64 	%fd21, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd22, %fd20, %fd21, %fd1;
	mov.f64 	%fd23, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd24, %fd20, %fd23, %fd22;
	mov.f64 	%fd25, 0d3E928AF3FCA213EA;
	mov.f64 	%fd26, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F81111111122322;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3FA55555555502A1;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0d3FC5555555555511;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0d3FE000000000000B;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FF0000000000000;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd24, %fd44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd46;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd46;
	}
	shl.b32 	%r33, %r7, 20;
	add.s32 	%r34, %r9, %r33;
	mov.b64 	%fd118, {%r8, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	mov.b32 	%f4, %r35;
	abs.f32 	%f1, %f4;
	setp.lt.f32 	%p10, %f1, 0f4086232B;
	@%p10 bra 	$L__BB32_10;

	setp.lt.f64 	%p11, %fd1, 0d0000000000000000;
	add.f64 	%fd47, %fd1, 0d7FF0000000000000;
	selp.f64 	%fd118, 0d0000000000000000, %fd47, %p11;
	setp.geu.f32 	%p12, %f1, 0f40874800;
	@%p12 bra 	$L__BB32_10;

	shr.u32 	%r36, %r7, 31;
	add.s32 	%r37, %r7, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r9, %r39;
	mov.b64 	%fd48, {%r8, %r40};
	sub.s32 	%r41, %r7, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd49, {%r44, %r43};
	mul.f64 	%fd118, %fd48, %fd49;

$L__BB32_10:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd118;
	add.s32 	%r80, %r80, %r5;
	cvt.u64.u32 	%rd65, %r80;
	setp.lt.u64 	%p13, %rd65, %rd26;
	@%p13 bra 	$L__BB32_7;

$L__BB32_26:
	ret;

}
	// .globl	ulog_f32
.visible .entry ulog_f32(
	.param .u64 ulog_f32_param_0,
	.param .u64 ulog_f32_param_1,
	.param .u64 ulog_f32_param_2,
	.param .u64 ulog_f32_param_3,
	.param .u64 ulog_f32_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<106>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [ulog_f32_param_0];
	ld.param.u64 	%rd27, [ulog_f32_param_1];
	ld.param.u64 	%rd29, [ulog_f32_param_2];
	ld.param.u64 	%rd28, [ulog_f32_param_3];
	ld.param.u64 	%rd30, [ulog_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB33_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r46, 0;

$L__BB33_2:
	not.b32 	%r20, %r46;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB33_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd41, %r46;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB33_2;

$L__BB33_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r47, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r47;
	@%p28 bra 	$L__BB33_10;
	bra.uni 	$L__BB33_5;

$L__BB33_10:
	setp.ge.u64 	%p14, %rd64, %rd26;
	@%p14 bra 	$L__BB33_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r28;
	@%p3 bra 	$L__BB33_20;

$L__BB33_12:
	mov.u32 	%r49, 0;
	mov.u32 	%r50, %r47;
	mov.u32 	%r51, %r49;

$L__BB33_13:
	not.b32 	%r31, %r49;
	cvt.u64.u32 	%rd46, %r31;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r50;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p16, %rd50, 0;
	@%p16 bra 	$L__BB33_15;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB33_16;

$L__BB33_15:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd66, %r34;
	cvt.u64.u32 	%rd67, %r36;

$L__BB33_16:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r37, %rd55;
	add.s32 	%r51, %r51, %r37;
	cvt.u32.u64 	%r50, %rd66;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd56, %r49;
	setp.lt.u64 	%p17, %rd56, %rd27;
	@%p17 bra 	$L__BB33_13;

	setp.eq.s64 	%p18, %rd28, 0;
	mul.wide.u32 	%rd57, %r51, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p18;
	ld.global.f32 	%f43, [%rd60];
	setp.lt.f32 	%p19, %f43, 0f00800000;
	mul.f32 	%f44, %f43, 0f4B000000;
	selp.f32 	%f5, %f44, %f43, %p19;
	selp.f32 	%f45, 0fC1B80000, 0f00000000, %p19;
	mov.b32 	%r38, %f5;
	add.s32 	%r39, %r38, -1059760811;
	and.b32  	%r40, %r39, -8388608;
	sub.s32 	%r41, %r38, %r40;
	mov.b32 	%f46, %r41;
	cvt.rn.f32.s32 	%f47, %r40;
	mov.f32 	%f48, 0f34000000;
	fma.rn.f32 	%f49, %f47, %f48, %f45;
	add.f32 	%f50, %f46, 0fBF800000;
	mov.f32 	%f51, 0f3E1039F6;
	mov.f32 	%f52, 0fBE055027;
	fma.rn.f32 	%f53, %f52, %f50, %f51;
	mov.f32 	%f54, 0fBDF8CDCC;
	fma.rn.f32 	%f55, %f53, %f50, %f54;
	mov.f32 	%f56, 0f3E0F2955;
	fma.rn.f32 	%f57, %f55, %f50, %f56;
	mov.f32 	%f58, 0fBE2AD8B9;
	fma.rn.f32 	%f59, %f57, %f50, %f58;
	mov.f32 	%f60, 0f3E4CED0B;
	fma.rn.f32 	%f61, %f59, %f50, %f60;
	mov.f32 	%f62, 0fBE7FFF22;
	fma.rn.f32 	%f63, %f61, %f50, %f62;
	mov.f32 	%f64, 0f3EAAAA78;
	fma.rn.f32 	%f65, %f63, %f50, %f64;
	mov.f32 	%f66, 0fBF000000;
	fma.rn.f32 	%f67, %f65, %f50, %f66;
	mul.f32 	%f68, %f50, %f67;
	fma.rn.f32 	%f69, %f68, %f50, %f50;
	mov.f32 	%f70, 0f3F317218;
	fma.rn.f32 	%f104, %f49, %f70, %f69;
	setp.lt.u32 	%p20, %r38, 2139095040;
	@%p20 bra 	$L__BB33_19;

	mov.f32 	%f71, 0f7F800000;
	fma.rn.f32 	%f104, %f5, %f71, %f71;

$L__BB33_19:
	setp.eq.f32 	%p21, %f5, 0f00000000;
	selp.f32 	%f72, 0fFF800000, %f104, %p21;
	st.global.f32 	[%rd21], %f72;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p22, %rd64, %rd26;
	@%p22 bra 	$L__BB33_12;
	bra.uni 	$L__BB33_23;

$L__BB33_20:
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p23, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p23;
	ld.global.f32 	%f73, [%rd62];
	setp.lt.f32 	%p24, %f73, 0f00800000;
	mul.f32 	%f74, %f73, 0f4B000000;
	selp.f32 	%f9, %f74, %f73, %p24;
	selp.f32 	%f75, 0fC1B80000, 0f00000000, %p24;
	mov.b32 	%r42, %f9;
	add.s32 	%r43, %r42, -1059760811;
	and.b32  	%r44, %r43, -8388608;
	sub.s32 	%r45, %r42, %r44;
	mov.b32 	%f76, %r45;
	cvt.rn.f32.s32 	%f77, %r44;
	mov.f32 	%f78, 0f34000000;
	fma.rn.f32 	%f79, %f77, %f78, %f75;
	add.f32 	%f80, %f76, 0fBF800000;
	mov.f32 	%f81, 0f3E1039F6;
	mov.f32 	%f82, 0fBE055027;
	fma.rn.f32 	%f83, %f82, %f80, %f81;
	mov.f32 	%f84, 0fBDF8CDCC;
	fma.rn.f32 	%f85, %f83, %f80, %f84;
	mov.f32 	%f86, 0f3E0F2955;
	fma.rn.f32 	%f87, %f85, %f80, %f86;
	mov.f32 	%f88, 0fBE2AD8B9;
	fma.rn.f32 	%f89, %f87, %f80, %f88;
	mov.f32 	%f90, 0f3E4CED0B;
	fma.rn.f32 	%f91, %f89, %f80, %f90;
	mov.f32 	%f92, 0fBE7FFF22;
	fma.rn.f32 	%f93, %f91, %f80, %f92;
	mov.f32 	%f94, 0f3EAAAA78;
	fma.rn.f32 	%f95, %f93, %f80, %f94;
	mov.f32 	%f96, 0fBF000000;
	fma.rn.f32 	%f97, %f95, %f80, %f96;
	mul.f32 	%f98, %f80, %f97;
	fma.rn.f32 	%f99, %f98, %f80, %f80;
	mov.f32 	%f100, 0f3F317218;
	fma.rn.f32 	%f105, %f79, %f100, %f99;
	setp.lt.u32 	%p25, %r42, 2139095040;
	@%p25 bra 	$L__BB33_22;

	mov.f32 	%f101, 0f7F800000;
	fma.rn.f32 	%f105, %f9, %f101, %f101;

$L__BB33_22:
	setp.eq.f32 	%p26, %f9, 0f00000000;
	selp.f32 	%f102, 0fFF800000, %f105, %p26;
	st.global.f32 	[%rd24], %f102;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p27, %rd64, %rd26;
	@%p27 bra 	$L__BB33_20;
	bra.uni 	$L__BB33_23;

$L__BB33_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB33_23;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB33_7:
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f13, [%rd43];
	setp.lt.f32 	%p10, %f13, 0f00800000;
	mul.f32 	%f14, %f13, 0f4B000000;
	selp.f32 	%f1, %f14, %f13, %p10;
	selp.f32 	%f15, 0fC1B80000, 0f00000000, %p10;
	mov.b32 	%r24, %f1;
	add.s32 	%r25, %r24, -1059760811;
	and.b32  	%r26, %r25, -8388608;
	sub.s32 	%r27, %r24, %r26;
	mov.b32 	%f16, %r27;
	cvt.rn.f32.s32 	%f17, %r26;
	mov.f32 	%f18, 0f34000000;
	fma.rn.f32 	%f19, %f17, %f18, %f15;
	add.f32 	%f20, %f16, 0fBF800000;
	mov.f32 	%f21, 0f3E1039F6;
	mov.f32 	%f22, 0fBE055027;
	fma.rn.f32 	%f23, %f22, %f20, %f21;
	mov.f32 	%f24, 0fBDF8CDCC;
	fma.rn.f32 	%f25, %f23, %f20, %f24;
	mov.f32 	%f26, 0f3E0F2955;
	fma.rn.f32 	%f27, %f25, %f20, %f26;
	mov.f32 	%f28, 0fBE2AD8B9;
	fma.rn.f32 	%f29, %f27, %f20, %f28;
	mov.f32 	%f30, 0f3E4CED0B;
	fma.rn.f32 	%f31, %f29, %f20, %f30;
	mov.f32 	%f32, 0fBE7FFF22;
	fma.rn.f32 	%f33, %f31, %f20, %f32;
	mov.f32 	%f34, 0f3EAAAA78;
	fma.rn.f32 	%f35, %f33, %f20, %f34;
	mov.f32 	%f36, 0fBF000000;
	fma.rn.f32 	%f37, %f35, %f20, %f36;
	mul.f32 	%f38, %f20, %f37;
	fma.rn.f32 	%f39, %f38, %f20, %f20;
	mov.f32 	%f40, 0f3F317218;
	fma.rn.f32 	%f103, %f19, %f40, %f39;
	setp.lt.u32 	%p11, %r24, 2139095040;
	@%p11 bra 	$L__BB33_9;

	mov.f32 	%f41, 0f7F800000;
	fma.rn.f32 	%f103, %f1, %f41, %f41;

$L__BB33_9:
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f32 	%f42, 0fFF800000, %f103, %p12;
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f42;
	add.s32 	%r47, %r47, %r5;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p13, %rd64, %rd26;
	@%p13 bra 	$L__BB33_7;

$L__BB33_23:
	ret;

}
	// .globl	ulog_f64
.visible .entry ulog_f64(
	.param .u64 ulog_f64_param_0,
	.param .u64 ulog_f64_param_1,
	.param .u64 ulog_f64_param_2,
	.param .u64 ulog_f64_param_3,
	.param .u64 ulog_f64_param_4
)
{
	.reg .pred 	%p<32>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<122>;
	.reg .f64 	%fd<175>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [ulog_f64_param_0];
	ld.param.u64 	%rd27, [ulog_f64_param_1];
	ld.param.u64 	%rd29, [ulog_f64_param_2];
	ld.param.u64 	%rd28, [ulog_f64_param_3];
	ld.param.u64 	%rd30, [ulog_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p31, %p2;
	@%p3 bra 	$L__BB34_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r103, 0;

$L__BB34_2:
	not.b32 	%r50, %r103;
	cvt.u64.u32 	%rd32, %r50;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p31, -1;
	@%p5 bra 	$L__BB34_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r103, %r103, 1;
	cvt.u64.u32 	%rd41, %r103;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p31, %p2;
	@%p7 bra 	$L__BB34_2;

$L__BB34_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r51, %ctaid.x;
	mov.u32 	%r52, %tid.x;
	mad.lo.s32 	%r104, %r51, %r3, %r52;
	cvt.u64.u32 	%rd64, %r104;
	@%p31 bra 	$L__BB34_15;
	bra.uni 	$L__BB34_5;

$L__BB34_15:
	setp.ge.u64 	%p15, %rd64, %rd26;
	@%p15 bra 	$L__BB34_38;

	mov.u32 	%r67, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r67;
	@%p3 bra 	$L__BB34_30;

$L__BB34_17:
	mov.u32 	%r110, 0;
	mov.u32 	%r111, %r104;
	mov.u32 	%r112, %r110;

$L__BB34_18:
	not.b32 	%r70, %r110;
	cvt.u64.u32 	%rd46, %r70;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r111;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p17, %rd50, 0;
	@%p17 bra 	$L__BB34_20;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB34_21;

$L__BB34_20:
	cvt.u32.u64 	%r71, %rd14;
	cvt.u32.u64 	%r72, %rd12;
	div.u32 	%r73, %r72, %r71;
	mul.lo.s32 	%r74, %r73, %r71;
	sub.s32 	%r75, %r72, %r74;
	cvt.u64.u32 	%rd66, %r73;
	cvt.u64.u32 	%rd67, %r75;

$L__BB34_21:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r76, %rd55;
	add.s32 	%r112, %r112, %r76;
	cvt.u32.u64 	%r111, %rd66;
	add.s32 	%r110, %r110, 1;
	cvt.u64.u32 	%rd56, %r110;
	setp.lt.u64 	%p18, %rd56, %rd27;
	@%p18 bra 	$L__BB34_18;

	setp.eq.s64 	%p19, %rd28, 0;
	mul.wide.u32 	%rd57, %r112, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p19;
	ld.global.f64 	%fd169, [%rd60];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r113}, %fd169;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r114, %temp}, %fd169;
	}
	setp.gt.s32 	%p20, %r113, 1048575;
	mov.u32 	%r115, -1023;
	@%p20 bra 	$L__BB34_24;

	mul.f64 	%fd169, %fd169, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r113}, %fd169;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r114, %temp}, %fd169;
	}
	mov.u32 	%r115, -1077;

$L__BB34_24:
	add.s32 	%r79, %r113, -1;
	setp.lt.u32 	%p21, %r79, 2146435071;
	@%p21 bra 	$L__BB34_26;
	bra.uni 	$L__BB34_25;

$L__BB34_26:
	shr.u32 	%r81, %r113, 20;
	add.s32 	%r116, %r115, %r81;
	and.b32  	%r82, %r113, -2146435073;
	or.b32  	%r83, %r82, 1072693248;
	mov.b64 	%fd170, {%r114, %r83};
	setp.lt.s32 	%p23, %r83, 1073127583;
	@%p23 bra 	$L__BB34_28;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r84, %temp}, %fd170;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd170;
	}
	add.s32 	%r86, %r85, -1048576;
	mov.b64 	%fd170, {%r84, %r86};
	add.s32 	%r116, %r116, 1;

$L__BB34_28:
	add.f64 	%fd76, %fd170, 0d3FF0000000000000;
	mov.f64 	%fd77, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd78, %fd76;
	neg.f64 	%fd79, %fd76;
	fma.rn.f64 	%fd80, %fd79, %fd78, %fd77;
	fma.rn.f64 	%fd81, %fd80, %fd80, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd78, %fd78;
	add.f64 	%fd83, %fd170, 0dBFF0000000000000;
	mul.f64 	%fd84, %fd83, %fd82;
	fma.rn.f64 	%fd85, %fd83, %fd82, %fd84;
	mul.f64 	%fd86, %fd85, %fd85;
	mov.f64 	%fd87, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd88, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd89, %fd88, %fd86, %fd87;
	mov.f64 	%fd90, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd91, %fd89, %fd86, %fd90;
	mov.f64 	%fd92, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd93, %fd91, %fd86, %fd92;
	mov.f64 	%fd94, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd95, %fd93, %fd86, %fd94;
	mov.f64 	%fd96, 0d3F624924923BE72D;
	fma.rn.f64 	%fd97, %fd95, %fd86, %fd96;
	mov.f64 	%fd98, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd99, %fd97, %fd86, %fd98;
	mov.f64 	%fd100, 0d3FB5555555555554;
	fma.rn.f64 	%fd101, %fd99, %fd86, %fd100;
	sub.f64 	%fd102, %fd83, %fd85;
	add.f64 	%fd103, %fd102, %fd102;
	neg.f64 	%fd104, %fd85;
	fma.rn.f64 	%fd105, %fd104, %fd83, %fd103;
	mul.f64 	%fd106, %fd82, %fd105;
	mul.f64 	%fd107, %fd86, %fd101;
	fma.rn.f64 	%fd108, %fd107, %fd85, %fd106;
	xor.b32  	%r87, %r116, -2147483648;
	mov.u32 	%r88, -2147483648;
	mov.u32 	%r89, 1127219200;
	mov.b64 	%fd109, {%r87, %r89};
	mov.b64 	%fd110, {%r88, %r89};
	sub.f64 	%fd111, %fd109, %fd110;
	mov.f64 	%fd112, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd113, %fd111, %fd112, %fd85;
	neg.f64 	%fd114, %fd111;
	fma.rn.f64 	%fd115, %fd114, %fd112, %fd113;
	sub.f64 	%fd116, %fd115, %fd85;
	sub.f64 	%fd117, %fd108, %fd116;
	mov.f64 	%fd118, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd119, %fd111, %fd118, %fd117;
	add.f64 	%fd171, %fd113, %fd119;
	bra.uni 	$L__BB34_29;

$L__BB34_25:
	mov.f64 	%fd74, 0d7FF0000000000000;
	fma.rn.f64 	%fd75, %fd169, %fd74, %fd74;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd169;
	}
	mov.b32 	%f2, %r80;
	setp.eq.f32 	%p22, %f2, 0f00000000;
	selp.f64 	%fd171, 0dFFF0000000000000, %fd75, %p22;

$L__BB34_29:
	st.global.f64 	[%rd21], %fd171;
	add.s32 	%r104, %r104, %r18;
	cvt.u64.u32 	%rd64, %r104;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB34_17;
	bra.uni 	$L__BB34_38;

$L__BB34_30:
	shl.b64 	%rd61, %rd64, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p25, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p25;
	ld.global.f64 	%fd172, [%rd62];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd172;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r119, %temp}, %fd172;
	}
	setp.gt.s32 	%p26, %r118, 1048575;
	mov.u32 	%r120, -1023;
	@%p26 bra 	$L__BB34_32;

	mul.f64 	%fd172, %fd172, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd172;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r119, %temp}, %fd172;
	}
	mov.u32 	%r120, -1077;

$L__BB34_32:
	add.s32 	%r92, %r118, -1;
	setp.lt.u32 	%p27, %r92, 2146435071;
	@%p27 bra 	$L__BB34_34;
	bra.uni 	$L__BB34_33;

$L__BB34_34:
	shr.u32 	%r94, %r118, 20;
	add.s32 	%r121, %r120, %r94;
	and.b32  	%r95, %r118, -2146435073;
	or.b32  	%r96, %r95, 1072693248;
	mov.b64 	%fd173, {%r119, %r96};
	setp.lt.s32 	%p29, %r96, 1073127583;
	@%p29 bra 	$L__BB34_36;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r97, %temp}, %fd173;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r98}, %fd173;
	}
	add.s32 	%r99, %r98, -1048576;
	mov.b64 	%fd173, {%r97, %r99};
	add.s32 	%r121, %r121, 1;

$L__BB34_36:
	add.f64 	%fd122, %fd173, 0d3FF0000000000000;
	mov.f64 	%fd123, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd124, %fd122;
	neg.f64 	%fd125, %fd122;
	fma.rn.f64 	%fd126, %fd125, %fd124, %fd123;
	fma.rn.f64 	%fd127, %fd126, %fd126, %fd126;
	fma.rn.f64 	%fd128, %fd127, %fd124, %fd124;
	add.f64 	%fd129, %fd173, 0dBFF0000000000000;
	mul.f64 	%fd130, %fd129, %fd128;
	fma.rn.f64 	%fd131, %fd129, %fd128, %fd130;
	mul.f64 	%fd132, %fd131, %fd131;
	mov.f64 	%fd133, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd134, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd135, %fd134, %fd132, %fd133;
	mov.f64 	%fd136, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd137, %fd135, %fd132, %fd136;
	mov.f64 	%fd138, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd139, %fd137, %fd132, %fd138;
	mov.f64 	%fd140, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd141, %fd139, %fd132, %fd140;
	mov.f64 	%fd142, 0d3F624924923BE72D;
	fma.rn.f64 	%fd143, %fd141, %fd132, %fd142;
	mov.f64 	%fd144, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd145, %fd143, %fd132, %fd144;
	mov.f64 	%fd146, 0d3FB5555555555554;
	fma.rn.f64 	%fd147, %fd145, %fd132, %fd146;
	sub.f64 	%fd148, %fd129, %fd131;
	add.f64 	%fd149, %fd148, %fd148;
	neg.f64 	%fd150, %fd131;
	fma.rn.f64 	%fd151, %fd150, %fd129, %fd149;
	mul.f64 	%fd152, %fd128, %fd151;
	mul.f64 	%fd153, %fd132, %fd147;
	fma.rn.f64 	%fd154, %fd153, %fd131, %fd152;
	xor.b32  	%r100, %r121, -2147483648;
	mov.u32 	%r101, -2147483648;
	mov.u32 	%r102, 1127219200;
	mov.b64 	%fd155, {%r100, %r102};
	mov.b64 	%fd156, {%r101, %r102};
	sub.f64 	%fd157, %fd155, %fd156;
	mov.f64 	%fd158, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd159, %fd157, %fd158, %fd131;
	neg.f64 	%fd160, %fd157;
	fma.rn.f64 	%fd161, %fd160, %fd158, %fd159;
	sub.f64 	%fd162, %fd161, %fd131;
	sub.f64 	%fd163, %fd154, %fd162;
	mov.f64 	%fd164, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd165, %fd157, %fd164, %fd163;
	add.f64 	%fd174, %fd159, %fd165;
	bra.uni 	$L__BB34_37;

$L__BB34_33:
	mov.f64 	%fd120, 0d7FF0000000000000;
	fma.rn.f64 	%fd121, %fd172, %fd120, %fd120;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd172;
	}
	mov.b32 	%f3, %r93;
	setp.eq.f32 	%p28, %f3, 0f00000000;
	selp.f64 	%fd174, 0dFFF0000000000000, %fd121, %p28;

$L__BB34_37:
	st.global.f64 	[%rd24], %fd174;
	add.s32 	%r104, %r104, %r18;
	cvt.u64.u32 	%rd64, %r104;
	setp.lt.u64 	%p30, %rd64, %rd26;
	@%p30 bra 	$L__BB34_30;
	bra.uni 	$L__BB34_38;

$L__BB34_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB34_38;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r53, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r53;

$L__BB34_7:
	shl.b64 	%rd42, %rd64, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd166, [%rd43];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r105}, %fd166;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r106, %temp}, %fd166;
	}
	setp.gt.s32 	%p10, %r105, 1048575;
	mov.u32 	%r107, -1023;
	@%p10 bra 	$L__BB34_9;

	mul.f64 	%fd166, %fd166, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r105}, %fd166;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r106, %temp}, %fd166;
	}
	mov.u32 	%r107, -1077;

$L__BB34_9:
	add.s32 	%r56, %r105, -1;
	setp.lt.u32 	%p11, %r56, 2146435071;
	@%p11 bra 	$L__BB34_11;
	bra.uni 	$L__BB34_10;

$L__BB34_11:
	shr.u32 	%r58, %r105, 20;
	add.s32 	%r108, %r107, %r58;
	and.b32  	%r59, %r105, -2146435073;
	or.b32  	%r60, %r59, 1072693248;
	mov.b64 	%fd167, {%r106, %r60};
	setp.lt.s32 	%p13, %r60, 1073127583;
	@%p13 bra 	$L__BB34_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd167;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd167;
	}
	add.s32 	%r63, %r62, -1048576;
	mov.b64 	%fd167, {%r61, %r63};
	add.s32 	%r108, %r108, 1;

$L__BB34_13:
	add.f64 	%fd30, %fd167, 0d3FF0000000000000;
	mov.f64 	%fd31, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd32, %fd30;
	neg.f64 	%fd33, %fd30;
	fma.rn.f64 	%fd34, %fd33, %fd32, %fd31;
	fma.rn.f64 	%fd35, %fd34, %fd34, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd32, %fd32;
	add.f64 	%fd37, %fd167, 0dBFF0000000000000;
	mul.f64 	%fd38, %fd37, %fd36;
	fma.rn.f64 	%fd39, %fd37, %fd36, %fd38;
	mul.f64 	%fd40, %fd39, %fd39;
	mov.f64 	%fd41, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd42, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd43, %fd42, %fd40, %fd41;
	mov.f64 	%fd44, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd45, %fd43, %fd40, %fd44;
	mov.f64 	%fd46, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd47, %fd45, %fd40, %fd46;
	mov.f64 	%fd48, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd49, %fd47, %fd40, %fd48;
	mov.f64 	%fd50, 0d3F624924923BE72D;
	fma.rn.f64 	%fd51, %fd49, %fd40, %fd50;
	mov.f64 	%fd52, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd53, %fd51, %fd40, %fd52;
	mov.f64 	%fd54, 0d3FB5555555555554;
	fma.rn.f64 	%fd55, %fd53, %fd40, %fd54;
	sub.f64 	%fd56, %fd37, %fd39;
	add.f64 	%fd57, %fd56, %fd56;
	neg.f64 	%fd58, %fd39;
	fma.rn.f64 	%fd59, %fd58, %fd37, %fd57;
	mul.f64 	%fd60, %fd36, %fd59;
	mul.f64 	%fd61, %fd40, %fd55;
	fma.rn.f64 	%fd62, %fd61, %fd39, %fd60;
	xor.b32  	%r64, %r108, -2147483648;
	mov.u32 	%r65, -2147483648;
	mov.u32 	%r66, 1127219200;
	mov.b64 	%fd63, {%r64, %r66};
	mov.b64 	%fd64, {%r65, %r66};
	sub.f64 	%fd65, %fd63, %fd64;
	mov.f64 	%fd66, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd67, %fd65, %fd66, %fd39;
	neg.f64 	%fd68, %fd65;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	sub.f64 	%fd70, %fd69, %fd39;
	sub.f64 	%fd71, %fd62, %fd70;
	mov.f64 	%fd72, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd73, %fd65, %fd72, %fd71;
	add.f64 	%fd168, %fd67, %fd73;
	bra.uni 	$L__BB34_14;

$L__BB34_10:
	mov.f64 	%fd28, 0d7FF0000000000000;
	fma.rn.f64 	%fd29, %fd166, %fd28, %fd28;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd166;
	}
	mov.b32 	%f1, %r57;
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f64 	%fd168, 0dFFF0000000000000, %fd29, %p12;

$L__BB34_14:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd168;
	add.s32 	%r104, %r104, %r5;
	cvt.u64.u32 	%rd64, %r104;
	setp.lt.u64 	%p14, %rd64, %rd26;
	@%p14 bra 	$L__BB34_7;

$L__BB34_38:
	ret;

}
	// .globl	usin_f32
.visible .entry usin_f32(
	.param .u64 usin_f32_param_0,
	.param .u64 usin_f32_param_1,
	.param .u64 usin_f32_param_2,
	.param .u64 usin_f32_param_3,
	.param .u64 usin_f32_param_4
)
{
	.local .align 4 .b8 	__local_depot35[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<50>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<196>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<132>;


	mov.u64 	%SPL, __local_depot35;
	ld.param.u64 	%rd42, [usin_f32_param_0];
	ld.param.u64 	%rd43, [usin_f32_param_1];
	ld.param.u64 	%rd46, [usin_f32_param_2];
	ld.param.u64 	%rd44, [usin_f32_param_3];
	ld.param.u64 	%rd45, [usin_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd45;
	cvta.to.global.u64 	%rd2, %rd44;
	cvta.to.global.u64 	%rd3, %rd46;
	add.u64 	%rd4, %SPL, 0;
	setp.eq.s64 	%p3, %rd43, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p49, %p2;
	@%p3 bra 	$L__BB35_4;

	mov.u64 	%rd119, 1;
	mov.u32 	%r179, 0;

$L__BB35_2:
	not.b32 	%r70, %r179;
	cvt.u64.u32 	%rd49, %r70;
	add.s64 	%rd50, %rd49, %rd43;
	and.b64  	%rd6, %rd50, 4294967295;
	add.s64 	%rd51, %rd6, %rd43;
	shl.b64 	%rd52, %rd51, 3;
	add.s64 	%rd53, %rd3, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	setp.ne.s64 	%p5, %rd119, %rd54;
	mov.pred 	%p49, -1;
	@%p5 bra 	$L__BB35_4;

	shl.b64 	%rd55, %rd6, 3;
	add.s64 	%rd56, %rd3, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd119, %rd57, %rd119;
	add.s32 	%r179, %r179, 1;
	cvt.u64.u32 	%rd58, %r179;
	setp.lt.u64 	%p7, %rd58, %rd43;
	mov.pred 	%p49, %p2;
	@%p7 bra 	$L__BB35_2;

$L__BB35_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r71, %ctaid.x;
	mov.u32 	%r72, %tid.x;
	mad.lo.s32 	%r180, %r71, %r3, %r72;
	cvt.u64.u32 	%rd120, %r180;
	add.s64 	%rd9, %rd4, 24;
	@%p49 bra 	$L__BB35_20;
	bra.uni 	$L__BB35_5;

$L__BB35_20:
	setp.ge.u64 	%p21, %rd120, %rd42;
	@%p21 bra 	$L__BB35_54;

	mov.u32 	%r106, %nctaid.x;
	mul.lo.s32 	%r24, %r3, %r106;
	@%p3 bra 	$L__BB35_40;

$L__BB35_22:
	mov.u32 	%r185, 0;
	mov.u32 	%r186, %r180;
	mov.u32 	%r187, %r185;

$L__BB35_23:
	not.b32 	%r109, %r185;
	cvt.u64.u32 	%rd77, %r109;
	add.s64 	%rd78, %rd77, %rd43;
	cvt.u64.u32 	%rd18, %r186;
	shl.b64 	%rd79, %rd78, 3;
	and.b64  	%rd80, %rd79, 34359738360;
	add.s64 	%rd19, %rd3, %rd80;
	ld.global.u64 	%rd20, [%rd19];
	and.b64  	%rd81, %rd20, -4294967296;
	setp.eq.s64 	%p23, %rd81, 0;
	@%p23 bra 	$L__BB35_25;

	div.u64 	%rd124, %rd18, %rd20;
	mul.lo.s64 	%rd82, %rd124, %rd20;
	sub.s64 	%rd125, %rd18, %rd82;
	bra.uni 	$L__BB35_26;

$L__BB35_25:
	cvt.u32.u64 	%r110, %rd20;
	cvt.u32.u64 	%r111, %rd18;
	div.u32 	%r112, %r111, %r110;
	mul.lo.s32 	%r113, %r112, %r110;
	sub.s32 	%r114, %r111, %r113;
	cvt.u64.u32 	%rd124, %r112;
	cvt.u64.u32 	%rd125, %r114;

$L__BB35_26:
	shl.b64 	%rd83, %rd43, 3;
	add.s64 	%rd84, %rd19, %rd83;
	ld.global.u64 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd125;
	cvt.u32.u64 	%r115, %rd86;
	add.s32 	%r187, %r187, %r115;
	cvt.u32.u64 	%r186, %rd124;
	add.s32 	%r185, %r185, 1;
	cvt.u64.u32 	%rd87, %r185;
	setp.lt.u64 	%p24, %rd87, %rd43;
	@%p24 bra 	$L__BB35_23;

	mul.wide.u32 	%rd88, %r187, 4;
	add.s64 	%rd89, %rd2, %rd88;
	shl.b64 	%rd91, %rd120, 2;
	add.s64 	%rd27, %rd1, %rd91;
	setp.eq.s64 	%p25, %rd44, 0;
	selp.b64 	%rd92, %rd27, %rd89, %p25;
	ld.global.f32 	%f14, [%rd92];
	mul.f32 	%f61, %f14, 0f3F22F983;
	cvt.rni.s32.f32 	%r190, %f61;
	cvt.rn.f32.s32 	%f62, %r190;
	mov.f32 	%f63, 0fBFC90FDA;
	fma.rn.f32 	%f64, %f62, %f63, %f14;
	mov.f32 	%f65, 0fB3A22168;
	fma.rn.f32 	%f66, %f62, %f65, %f64;
	mov.f32 	%f67, 0fA7C234C5;
	fma.rn.f32 	%f106, %f62, %f67, %f66;
	abs.f32 	%f16, %f14;
	setp.ltu.f32 	%p26, %f16, 0f47CE4780;
	@%p26 bra 	$L__BB35_35;

	setp.eq.f32 	%p27, %f16, 0f7F800000;
	@%p27 bra 	$L__BB35_34;
	bra.uni 	$L__BB35_29;

$L__BB35_34:
	mov.f32 	%f70, 0f00000000;
	mul.rn.f32 	%f106, %f14, %f70;
	mov.u32 	%r190, 0;
	bra.uni 	$L__BB35_35;

$L__BB35_29:
	mov.b32 	%r33, %f14;
	shr.u32 	%r116, %r33, 23;
	and.b32  	%r117, %r116, 255;
	add.s32 	%r34, %r117, -128;
	shl.b32 	%r118, %r33, 8;
	or.b32  	%r35, %r118, -2147483648;
	shr.u32 	%r36, %r34, 5;
	mov.u64 	%rd126, 0;
	mov.u64 	%rd127, %rd126;

$L__BB35_30:
	.pragma "nounroll";
	shl.b64 	%rd95, %rd126, 2;
	mov.u64 	%rd96, __cudart_i2opi_f;
	add.s64 	%rd97, %rd96, %rd95;
	ld.global.nc.u32 	%r119, [%rd97];
	mad.wide.u32 	%rd98, %r119, %r35, %rd127;
	shr.u64 	%rd127, %rd98, 32;
	add.s64 	%rd99, %rd4, %rd95;
	st.local.u32 	[%rd99], %rd98;
	cvt.u32.u64 	%r120, %rd126;
	add.s32 	%r121, %r120, 1;
	cvt.s64.s32 	%rd126, %r121;
	setp.ne.s32 	%p28, %r121, 6;
	@%p28 bra 	$L__BB35_30;

	st.local.u32 	[%rd9], %rd127;
	mov.u32 	%r122, 4;
	sub.s32 	%r37, %r122, %r36;
	mov.u32 	%r123, 6;
	sub.s32 	%r124, %r123, %r36;
	mul.wide.s32 	%rd100, %r124, 4;
	add.s64 	%rd101, %rd4, %rd100;
	ld.local.u32 	%r188, [%rd101];
	ld.local.u32 	%r189, [%rd101+-4];
	and.b32  	%r40, %r34, 31;
	setp.eq.s32 	%p29, %r40, 0;
	@%p29 bra 	$L__BB35_33;

	mov.u32 	%r125, 32;
	sub.s32 	%r126, %r125, %r40;
	shr.u32 	%r127, %r189, %r126;
	shl.b32 	%r128, %r188, %r40;
	add.s32 	%r188, %r127, %r128;
	mul.wide.s32 	%rd102, %r37, 4;
	add.s64 	%rd103, %rd4, %rd102;
	ld.local.u32 	%r129, [%rd103];
	shr.u32 	%r130, %r129, %r126;
	shl.b32 	%r131, %r189, %r40;
	add.s32 	%r189, %r130, %r131;

$L__BB35_33:
	and.b32  	%r132, %r33, -2147483648;
	shr.u32 	%r133, %r189, 30;
	shl.b32 	%r134, %r188, 2;
	or.b32  	%r135, %r133, %r134;
	shr.u32 	%r136, %r135, 31;
	shr.u32 	%r137, %r188, 30;
	add.s32 	%r138, %r136, %r137;
	neg.s32 	%r139, %r138;
	setp.eq.s32 	%p30, %r132, 0;
	selp.b32 	%r190, %r138, %r139, %p30;
	setp.ne.s32 	%p31, %r136, 0;
	xor.b32  	%r140, %r132, -2147483648;
	selp.b32 	%r141, %r140, %r132, %p31;
	selp.b32 	%r142, -1, 0, %p31;
	xor.b32  	%r143, %r135, %r142;
	shl.b32 	%r144, %r189, 2;
	xor.b32  	%r145, %r144, %r142;
	cvt.u64.u32 	%rd104, %r143;
	cvt.u64.u32 	%rd105, %r145;
	bfi.b64 	%rd106, %rd104, %rd105, 32, 32;
	cvt.rn.f64.s64 	%fd3, %rd106;
	mul.f64 	%fd4, %fd3, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f68, %fd4;
	setp.eq.s32 	%p32, %r141, 0;
	neg.f32 	%f69, %f68;
	selp.f32 	%f106, %f68, %f69, %p32;

$L__BB35_35:
	and.b32  	%r47, %r190, 1;
	setp.eq.s32 	%p33, %r47, 0;
	selp.f32 	%f20, %f106, 0f3F800000, %p33;
	mul.rn.f32 	%f21, %f106, %f106;
	mov.f32 	%f107, 0fB94D4153;
	@%p33 bra 	$L__BB35_37;

	mov.f32 	%f72, 0fBAB607ED;
	mov.f32 	%f73, 0f37CBAC00;
	fma.rn.f32 	%f107, %f73, %f21, %f72;

$L__BB35_37:
	selp.f32 	%f74, 0f3C0885E4, 0f3D2AAABB, %p33;
	fma.rn.f32 	%f75, %f107, %f21, %f74;
	selp.f32 	%f76, 0fBE2AAAA8, 0fBEFFFFFF, %p33;
	fma.rn.f32 	%f77, %f75, %f21, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f79, %f21, %f20, %f78;
	fma.rn.f32 	%f108, %f77, %f79, %f20;
	and.b32  	%r147, %r190, 2;
	setp.eq.s32 	%p35, %r147, 0;
	@%p35 bra 	$L__BB35_39;

	mov.f32 	%f81, 0fBF800000;
	fma.rn.f32 	%f108, %f108, %f81, %f78;

$L__BB35_39:
	st.global.f32 	[%rd27], %f108;
	add.s32 	%r180, %r180, %r24;
	cvt.u64.u32 	%rd120, %r180;
	setp.lt.u64 	%p36, %rd120, %rd42;
	@%p36 bra 	$L__BB35_22;
	bra.uni 	$L__BB35_54;

$L__BB35_5:
	setp.ge.u64 	%p8, %rd120, %rd42;
	@%p8 bra 	$L__BB35_54;

	setp.eq.s64 	%p9, %rd44, 0;
	selp.b64 	%rd10, %rd1, %rd2, %p9;
	mov.u32 	%r73, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r73;

$L__BB35_7:
	shl.b64 	%rd59, %rd120, 2;
	add.s64 	%rd60, %rd10, %rd59;
	ld.global.f32 	%f1, [%rd60];
	mul.f32 	%f40, %f1, 0f3F22F983;
	cvt.rni.s32.f32 	%r183, %f40;
	cvt.rn.f32.s32 	%f41, %r183;
	mov.f32 	%f42, 0fBFC90FDA;
	fma.rn.f32 	%f43, %f41, %f42, %f1;
	mov.f32 	%f44, 0fB3A22168;
	fma.rn.f32 	%f45, %f41, %f44, %f43;
	mov.f32 	%f46, 0fA7C234C5;
	fma.rn.f32 	%f103, %f41, %f46, %f45;
	abs.f32 	%f3, %f1;
	setp.ltu.f32 	%p10, %f3, 0f47CE4780;
	@%p10 bra 	$L__BB35_15;

	setp.eq.f32 	%p11, %f3, 0f7F800000;
	@%p11 bra 	$L__BB35_14;
	bra.uni 	$L__BB35_9;

$L__BB35_14:
	mov.f32 	%f49, 0f00000000;
	mul.rn.f32 	%f103, %f1, %f49;
	mov.u32 	%r183, 0;
	bra.uni 	$L__BB35_15;

$L__BB35_9:
	mov.b32 	%r8, %f1;
	shr.u32 	%r74, %r8, 23;
	and.b32  	%r75, %r74, 255;
	add.s32 	%r9, %r75, -128;
	shl.b32 	%r76, %r8, 8;
	or.b32  	%r10, %r76, -2147483648;
	shr.u32 	%r11, %r9, 5;
	mov.u64 	%rd121, 0;
	mov.u64 	%rd122, %rd121;

$L__BB35_10:
	.pragma "nounroll";
	shl.b64 	%rd63, %rd121, 2;
	mov.u64 	%rd64, __cudart_i2opi_f;
	add.s64 	%rd65, %rd64, %rd63;
	ld.global.nc.u32 	%r77, [%rd65];
	mad.wide.u32 	%rd66, %r77, %r10, %rd122;
	shr.u64 	%rd122, %rd66, 32;
	add.s64 	%rd67, %rd4, %rd63;
	st.local.u32 	[%rd67], %rd66;
	cvt.u32.u64 	%r78, %rd121;
	add.s32 	%r79, %r78, 1;
	cvt.s64.s32 	%rd121, %r79;
	setp.ne.s32 	%p12, %r79, 6;
	@%p12 bra 	$L__BB35_10;

	st.local.u32 	[%rd9], %rd122;
	mov.u32 	%r80, 4;
	sub.s32 	%r12, %r80, %r11;
	mov.u32 	%r81, 6;
	sub.s32 	%r82, %r81, %r11;
	mul.wide.s32 	%rd68, %r82, 4;
	add.s64 	%rd69, %rd4, %rd68;
	ld.local.u32 	%r181, [%rd69];
	ld.local.u32 	%r182, [%rd69+-4];
	and.b32  	%r15, %r9, 31;
	setp.eq.s32 	%p13, %r15, 0;
	@%p13 bra 	$L__BB35_13;

	mov.u32 	%r83, 32;
	sub.s32 	%r84, %r83, %r15;
	shr.u32 	%r85, %r182, %r84;
	shl.b32 	%r86, %r181, %r15;
	add.s32 	%r181, %r85, %r86;
	mul.wide.s32 	%rd70, %r12, 4;
	add.s64 	%rd71, %rd4, %rd70;
	ld.local.u32 	%r87, [%rd71];
	shr.u32 	%r88, %r87, %r84;
	shl.b32 	%r89, %r182, %r15;
	add.s32 	%r182, %r88, %r89;

$L__BB35_13:
	and.b32  	%r90, %r8, -2147483648;
	shr.u32 	%r91, %r182, 30;
	shl.b32 	%r92, %r181, 2;
	or.b32  	%r93, %r91, %r92;
	shr.u32 	%r94, %r93, 31;
	shr.u32 	%r95, %r181, 30;
	add.s32 	%r96, %r94, %r95;
	neg.s32 	%r97, %r96;
	setp.eq.s32 	%p14, %r90, 0;
	selp.b32 	%r183, %r96, %r97, %p14;
	setp.ne.s32 	%p15, %r94, 0;
	xor.b32  	%r98, %r90, -2147483648;
	selp.b32 	%r99, %r98, %r90, %p15;
	selp.b32 	%r100, -1, 0, %p15;
	xor.b32  	%r101, %r93, %r100;
	shl.b32 	%r102, %r182, 2;
	xor.b32  	%r103, %r102, %r100;
	cvt.u64.u32 	%rd72, %r101;
	cvt.u64.u32 	%rd73, %r103;
	bfi.b64 	%rd74, %rd72, %rd73, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd74;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f47, %fd2;
	setp.eq.s32 	%p16, %r99, 0;
	neg.f32 	%f48, %f47;
	selp.f32 	%f103, %f47, %f48, %p16;

$L__BB35_15:
	and.b32  	%r22, %r183, 1;
	setp.eq.s32 	%p17, %r22, 0;
	selp.f32 	%f7, %f103, 0f3F800000, %p17;
	mul.rn.f32 	%f8, %f103, %f103;
	mov.f32 	%f104, 0fB94D4153;
	@%p17 bra 	$L__BB35_17;

	mov.f32 	%f51, 0fBAB607ED;
	mov.f32 	%f52, 0f37CBAC00;
	fma.rn.f32 	%f104, %f52, %f8, %f51;

$L__BB35_17:
	selp.f32 	%f53, 0f3C0885E4, 0f3D2AAABB, %p17;
	fma.rn.f32 	%f54, %f104, %f8, %f53;
	selp.f32 	%f55, 0fBE2AAAA8, 0fBEFFFFFF, %p17;
	fma.rn.f32 	%f56, %f54, %f8, %f55;
	mov.f32 	%f57, 0f00000000;
	fma.rn.f32 	%f58, %f8, %f7, %f57;
	fma.rn.f32 	%f105, %f56, %f58, %f7;
	and.b32  	%r105, %r183, 2;
	setp.eq.s32 	%p19, %r105, 0;
	@%p19 bra 	$L__BB35_19;

	mov.f32 	%f60, 0fBF800000;
	fma.rn.f32 	%f105, %f105, %f60, %f57;

$L__BB35_19:
	add.s64 	%rd76, %rd1, %rd59;
	st.global.f32 	[%rd76], %f105;
	add.s32 	%r180, %r180, %r5;
	cvt.u64.u32 	%rd120, %r180;
	setp.lt.u64 	%p20, %rd120, %rd42;
	@%p20 bra 	$L__BB35_7;
	bra.uni 	$L__BB35_54;

$L__BB35_40:
	mov.u64 	%rd109, __cudart_i2opi_f;

$L__BB35_41:
	shl.b64 	%rd107, %rd120, 2;
	add.s64 	%rd34, %rd1, %rd107;
	setp.eq.s64 	%p37, %rd44, 0;
	selp.b64 	%rd108, %rd34, %rd2, %p37;
	ld.global.f32 	%f27, [%rd108];
	mul.f32 	%f82, %f27, 0f3F22F983;
	cvt.rni.s32.f32 	%r195, %f82;
	cvt.rn.f32.s32 	%f83, %r195;
	mov.f32 	%f84, 0fBFC90FDA;
	fma.rn.f32 	%f85, %f83, %f84, %f27;
	mov.f32 	%f86, 0fB3A22168;
	fma.rn.f32 	%f87, %f83, %f86, %f85;
	mov.f32 	%f88, 0fA7C234C5;
	fma.rn.f32 	%f109, %f83, %f88, %f87;
	abs.f32 	%f29, %f27;
	setp.ltu.f32 	%p38, %f29, 0f47CE4780;
	@%p38 bra 	$L__BB35_49;

	setp.eq.f32 	%p39, %f29, 0f7F800000;
	@%p39 bra 	$L__BB35_48;
	bra.uni 	$L__BB35_43;

$L__BB35_48:
	mov.f32 	%f91, 0f00000000;
	mul.rn.f32 	%f109, %f27, %f91;
	mov.u32 	%r195, 0;
	bra.uni 	$L__BB35_49;

$L__BB35_43:
	mov.b32 	%r51, %f27;
	shr.u32 	%r149, %r51, 23;
	and.b32  	%r150, %r149, 255;
	add.s32 	%r52, %r150, -128;
	shl.b32 	%r151, %r51, 8;
	or.b32  	%r53, %r151, -2147483648;
	shr.u32 	%r54, %r52, 5;
	mov.u64 	%rd131, 0;
	mov.u32 	%r192, 0;
	mov.u64 	%rd129, %rd109;
	mov.u64 	%rd130, %rd4;

$L__BB35_44:
	.pragma "nounroll";
	ld.global.nc.u32 	%r152, [%rd129];
	mad.wide.u32 	%rd111, %r152, %r53, %rd131;
	shr.u64 	%rd131, %rd111, 32;
	st.local.u32 	[%rd130], %rd111;
	add.s64 	%rd130, %rd130, 4;
	add.s64 	%rd129, %rd129, 4;
	add.s32 	%r192, %r192, 1;
	setp.ne.s32 	%p40, %r192, 6;
	@%p40 bra 	$L__BB35_44;

	st.local.u32 	[%rd9], %rd131;
	mov.u32 	%r153, 4;
	sub.s32 	%r57, %r153, %r54;
	mov.u32 	%r154, 6;
	sub.s32 	%r155, %r154, %r54;
	mul.wide.s32 	%rd112, %r155, 4;
	add.s64 	%rd113, %rd4, %rd112;
	ld.local.u32 	%r193, [%rd113];
	ld.local.u32 	%r194, [%rd113+-4];
	and.b32  	%r60, %r52, 31;
	setp.eq.s32 	%p41, %r60, 0;
	@%p41 bra 	$L__BB35_47;

	mov.u32 	%r156, 32;
	sub.s32 	%r157, %r156, %r60;
	shr.u32 	%r158, %r194, %r157;
	shl.b32 	%r159, %r193, %r60;
	add.s32 	%r193, %r158, %r159;
	mul.wide.s32 	%rd114, %r57, 4;
	add.s64 	%rd115, %rd4, %rd114;
	ld.local.u32 	%r160, [%rd115];
	shr.u32 	%r161, %r160, %r157;
	shl.b32 	%r162, %r194, %r60;
	add.s32 	%r194, %r161, %r162;

$L__BB35_47:
	and.b32  	%r163, %r51, -2147483648;
	shr.u32 	%r164, %r194, 30;
	shl.b32 	%r165, %r193, 2;
	or.b32  	%r166, %r164, %r165;
	shr.u32 	%r167, %r166, 31;
	shr.u32 	%r168, %r193, 30;
	add.s32 	%r169, %r167, %r168;
	neg.s32 	%r170, %r169;
	setp.eq.s32 	%p42, %r163, 0;
	selp.b32 	%r195, %r169, %r170, %p42;
	setp.ne.s32 	%p43, %r167, 0;
	xor.b32  	%r171, %r163, -2147483648;
	selp.b32 	%r172, %r171, %r163, %p43;
	selp.b32 	%r173, -1, 0, %p43;
	xor.b32  	%r174, %r166, %r173;
	shl.b32 	%r175, %r194, 2;
	xor.b32  	%r176, %r175, %r173;
	cvt.u64.u32 	%rd116, %r174;
	cvt.u64.u32 	%rd117, %r176;
	bfi.b64 	%rd118, %rd116, %rd117, 32, 32;
	cvt.rn.f64.s64 	%fd5, %rd118;
	mul.f64 	%fd6, %fd5, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f89, %fd6;
	setp.eq.s32 	%p44, %r172, 0;
	neg.f32 	%f90, %f89;
	selp.f32 	%f109, %f89, %f90, %p44;

$L__BB35_49:
	and.b32  	%r67, %r195, 1;
	setp.eq.s32 	%p45, %r67, 0;
	selp.f32 	%f33, %f109, 0f3F800000, %p45;
	mul.rn.f32 	%f34, %f109, %f109;
	mov.f32 	%f110, 0fB94D4153;
	@%p45 bra 	$L__BB35_51;

	mov.f32 	%f93, 0fBAB607ED;
	mov.f32 	%f94, 0f37CBAC00;
	fma.rn.f32 	%f110, %f94, %f34, %f93;

$L__BB35_51:
	selp.f32 	%f95, 0f3C0885E4, 0f3D2AAABB, %p45;
	fma.rn.f32 	%f96, %f110, %f34, %f95;
	selp.f32 	%f97, 0fBE2AAAA8, 0fBEFFFFFF, %p45;
	fma.rn.f32 	%f98, %f96, %f34, %f97;
	mov.f32 	%f99, 0f00000000;
	fma.rn.f32 	%f100, %f34, %f33, %f99;
	fma.rn.f32 	%f111, %f98, %f100, %f33;
	and.b32  	%r178, %r195, 2;
	setp.eq.s32 	%p47, %r178, 0;
	@%p47 bra 	$L__BB35_53;

	mov.f32 	%f102, 0fBF800000;
	fma.rn.f32 	%f111, %f111, %f102, %f99;

$L__BB35_53:
	st.global.f32 	[%rd34], %f111;
	add.s32 	%r180, %r180, %r24;
	cvt.u64.u32 	%rd120, %r180;
	setp.lt.u64 	%p48, %rd120, %rd42;
	@%p48 bra 	$L__BB35_41;

$L__BB35_54:
	ret;

}
	// .globl	usin_f64
.visible .entry usin_f64(
	.param .u64 usin_f64_param_0,
	.param .u64 usin_f64_param_1,
	.param .u64 usin_f64_param_2,
	.param .u64 usin_f64_param_3,
	.param .u64 usin_f64_param_4
)
{
	.local .align 4 .b8 	__local_depot36[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<38>;
	.reg .b32 	%r<77>;
	.reg .f64 	%fd<121>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot36;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd28, [usin_f64_param_0];
	ld.param.u64 	%rd29, [usin_f64_param_1];
	ld.param.u64 	%rd31, [usin_f64_param_2];
	ld.param.u64 	%rd30, [usin_f64_param_3];
	ld.param.u64 	%rd32, [usin_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	add.u64 	%rd34, %SP, 4;
	add.u64 	%rd5, %SPL, 4;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p37, %p2;
	@%p3 bra 	$L__BB36_4;

	mov.u64 	%rd79, 1;
	mov.u32 	%r67, 0;

$L__BB36_2:
	not.b32 	%r29, %r67;
	cvt.u64.u32 	%rd36, %r29;
	add.s64 	%rd37, %rd36, %rd29;
	and.b64  	%rd7, %rd37, 4294967295;
	add.s64 	%rd38, %rd7, %rd29;
	shl.b64 	%rd39, %rd38, 3;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	setp.ne.s64 	%p5, %rd79, %rd41;
	mov.pred 	%p37, -1;
	@%p5 bra 	$L__BB36_4;

	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd3, %rd42;
	ld.global.u64 	%rd44, [%rd43];
	mul.lo.s64 	%rd79, %rd44, %rd79;
	add.s32 	%r67, %r67, 1;
	cvt.u64.u32 	%rd45, %r67;
	setp.lt.u64 	%p7, %rd45, %rd29;
	mov.pred 	%p37, %p2;
	@%p7 bra 	$L__BB36_2;

$L__BB36_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %tid.x;
	mad.lo.s32 	%r68, %r30, %r3, %r31;
	cvt.u64.u32 	%rd80, %r68;
	@%p37 bra 	$L__BB36_16;
	bra.uni 	$L__BB36_5;

$L__BB36_16:
	setp.ge.u64 	%p17, %rd80, %rd28;
	@%p17 bra 	$L__BB36_42;

	mov.u32 	%r41, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r41;
	@%p3 bra 	$L__BB36_32;

$L__BB36_18:
	mov.u32 	%r71, 0;
	mov.u32 	%r72, %r68;
	mov.u32 	%r73, %r71;

$L__BB36_19:
	not.b32 	%r44, %r71;
	cvt.u64.u32 	%rd54, %r44;
	add.s64 	%rd55, %rd54, %rd29;
	cvt.u64.u32 	%rd14, %r72;
	shl.b64 	%rd56, %rd55, 3;
	and.b64  	%rd57, %rd56, 34359738360;
	add.s64 	%rd15, %rd3, %rd57;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd58, %rd16, -4294967296;
	setp.eq.s64 	%p19, %rd58, 0;
	@%p19 bra 	$L__BB36_21;

	div.u64 	%rd82, %rd14, %rd16;
	mul.lo.s64 	%rd59, %rd82, %rd16;
	sub.s64 	%rd83, %rd14, %rd59;
	bra.uni 	$L__BB36_22;

$L__BB36_21:
	cvt.u32.u64 	%r45, %rd16;
	cvt.u32.u64 	%r46, %rd14;
	div.u32 	%r47, %r46, %r45;
	mul.lo.s32 	%r48, %r47, %r45;
	sub.s32 	%r49, %r46, %r48;
	cvt.u64.u32 	%rd82, %r47;
	cvt.u64.u32 	%rd83, %r49;

$L__BB36_22:
	shl.b64 	%rd60, %rd29, 3;
	add.s64 	%rd61, %rd15, %rd60;
	ld.global.u64 	%rd62, [%rd61];
	mul.lo.s64 	%rd63, %rd62, %rd83;
	cvt.u32.u64 	%r50, %rd63;
	add.s32 	%r73, %r73, %r50;
	cvt.u32.u64 	%r72, %rd82;
	add.s32 	%r71, %r71, 1;
	cvt.u64.u32 	%rd64, %r71;
	setp.lt.u64 	%p20, %rd64, %rd29;
	@%p20 bra 	$L__BB36_19;

	mul.wide.u32 	%rd65, %r73, 8;
	add.s64 	%rd66, %rd2, %rd65;
	shl.b64 	%rd67, %rd80, 3;
	add.s64 	%rd23, %rd1, %rd67;
	setp.eq.s64 	%p21, %rd30, 0;
	selp.b64 	%rd68, %rd23, %rd66, %p21;
	ld.global.f64 	%fd13, [%rd68];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd13;
	}
	and.b32  	%r53, %r52, 2147483647;
	setp.eq.s32 	%p22, %r53, 2146435072;
	setp.eq.s32 	%p23, %r51, 0;
	and.pred  	%p24, %p23, %p22;
	@%p24 bra 	$L__BB36_26;
	bra.uni 	$L__BB36_24;

$L__BB36_26:
	mov.f64 	%fd71, 0d0000000000000000;
	mul.rn.f64 	%fd115, %fd13, %fd71;
	mov.u32 	%r74, 0;
	bra.uni 	$L__BB36_27;

$L__BB36_24:
	mul.f64 	%fd62, %fd13, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r74, %fd62;
	st.local.u32 	[%rd5], %r74;
	cvt.rn.f64.s32 	%fd63, %r74;
	neg.f64 	%fd64, %fd63;
	mov.f64 	%fd65, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd66, %fd64, %fd65, %fd13;
	mov.f64 	%fd67, 0d3C91A62633145C00;
	fma.rn.f64 	%fd68, %fd64, %fd67, %fd66;
	mov.f64 	%fd69, 0d397B839A252049C0;
	fma.rn.f64 	%fd115, %fd64, %fd69, %fd68;
	abs.f64 	%fd70, %fd13;
	setp.ltu.f64 	%p25, %fd70, 0d41E0000000000000;
	@%p25 bra 	$L__BB36_27;

	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd13;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd34;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd115, [retval0+0];
	} // callseq 1
	ld.local.u32 	%r74, [%rd5];

$L__BB36_27:
	and.b32  	%r55, %r74, 1;
	shl.b32 	%r56, %r74, 3;
	and.b32  	%r57, %r56, 8;
	setp.eq.s32 	%p26, %r55, 0;
	selp.f64 	%fd72, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p26;
	mul.wide.s32 	%rd70, %r57, 8;
	mov.u64 	%rd71, __cudart_sin_cos_coeffs;
	add.s64 	%rd72, %rd71, %rd70;
	ld.global.nc.f64 	%fd73, [%rd72+8];
	mul.rn.f64 	%fd18, %fd115, %fd115;
	fma.rn.f64 	%fd74, %fd72, %fd18, %fd73;
	ld.global.nc.f64 	%fd75, [%rd72+16];
	fma.rn.f64 	%fd76, %fd74, %fd18, %fd75;
	ld.global.nc.f64 	%fd77, [%rd72+24];
	fma.rn.f64 	%fd78, %fd76, %fd18, %fd77;
	ld.global.nc.f64 	%fd79, [%rd72+32];
	fma.rn.f64 	%fd80, %fd78, %fd18, %fd79;
	ld.global.nc.f64 	%fd81, [%rd72+40];
	fma.rn.f64 	%fd82, %fd80, %fd18, %fd81;
	ld.global.nc.f64 	%fd83, [%rd72+48];
	fma.rn.f64 	%fd19, %fd82, %fd18, %fd83;
	fma.rn.f64 	%fd117, %fd19, %fd115, %fd115;
	@%p26 bra 	$L__BB36_29;

	mov.f64 	%fd84, 0d3FF0000000000000;
	fma.rn.f64 	%fd117, %fd19, %fd18, %fd84;

$L__BB36_29:
	and.b32  	%r58, %r74, 2;
	setp.eq.s32 	%p27, %r58, 0;
	@%p27 bra 	$L__BB36_31;

	mov.f64 	%fd85, 0d0000000000000000;
	mov.f64 	%fd86, 0dBFF0000000000000;
	fma.rn.f64 	%fd117, %fd117, %fd86, %fd85;

$L__BB36_31:
	st.global.f64 	[%rd23], %fd117;
	add.s32 	%r68, %r68, %r11;
	cvt.u64.u32 	%rd80, %r68;
	setp.lt.u64 	%p28, %rd80, %rd28;
	@%p28 bra 	$L__BB36_18;
	bra.uni 	$L__BB36_42;

$L__BB36_5:
	setp.ge.u64 	%p8, %rd80, %rd28;
	@%p8 bra 	$L__BB36_42;

	setp.eq.s64 	%p9, %rd30, 0;
	selp.b64 	%rd10, %rd1, %rd2, %p9;
	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r32;

$L__BB36_7:
	shl.b64 	%rd46, %rd80, 3;
	add.s64 	%rd47, %rd10, %rd46;
	ld.global.f64 	%fd1, [%rd47];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd1;
	}
	and.b32  	%r35, %r34, 2147483647;
	setp.eq.s32 	%p10, %r35, 2146435072;
	setp.eq.s32 	%p11, %r33, 0;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB36_10;
	bra.uni 	$L__BB36_8;

$L__BB36_10:
	mov.f64 	%fd46, 0d0000000000000000;
	mul.rn.f64 	%fd112, %fd1, %fd46;
	mov.u32 	%r69, 0;
	bra.uni 	$L__BB36_11;

$L__BB36_8:
	mul.f64 	%fd37, %fd1, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r69, %fd37;
	st.local.u32 	[%rd4], %r69;
	cvt.rn.f64.s32 	%fd38, %r69;
	neg.f64 	%fd39, %fd38;
	mov.f64 	%fd40, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd41, %fd39, %fd40, %fd1;
	mov.f64 	%fd42, 0d3C91A62633145C00;
	fma.rn.f64 	%fd43, %fd39, %fd42, %fd41;
	mov.f64 	%fd44, 0d397B839A252049C0;
	fma.rn.f64 	%fd112, %fd39, %fd44, %fd43;
	abs.f64 	%fd45, %fd1;
	setp.ltu.f64 	%p13, %fd45, 0d41E0000000000000;
	@%p13 bra 	$L__BB36_11;

	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd112, [retval0+0];
	} // callseq 0
	ld.local.u32 	%r69, [%rd4];

$L__BB36_11:
	and.b32  	%r37, %r69, 1;
	shl.b32 	%r38, %r69, 3;
	and.b32  	%r39, %r38, 8;
	setp.eq.s32 	%p14, %r37, 0;
	selp.f64 	%fd47, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p14;
	mul.wide.s32 	%rd49, %r39, 8;
	mov.u64 	%rd50, __cudart_sin_cos_coeffs;
	add.s64 	%rd51, %rd50, %rd49;
	ld.global.nc.f64 	%fd48, [%rd51+8];
	mul.rn.f64 	%fd6, %fd112, %fd112;
	fma.rn.f64 	%fd49, %fd47, %fd6, %fd48;
	ld.global.nc.f64 	%fd50, [%rd51+16];
	fma.rn.f64 	%fd51, %fd49, %fd6, %fd50;
	ld.global.nc.f64 	%fd52, [%rd51+24];
	fma.rn.f64 	%fd53, %fd51, %fd6, %fd52;
	ld.global.nc.f64 	%fd54, [%rd51+32];
	fma.rn.f64 	%fd55, %fd53, %fd6, %fd54;
	ld.global.nc.f64 	%fd56, [%rd51+40];
	fma.rn.f64 	%fd57, %fd55, %fd6, %fd56;
	ld.global.nc.f64 	%fd58, [%rd51+48];
	fma.rn.f64 	%fd7, %fd57, %fd6, %fd58;
	fma.rn.f64 	%fd114, %fd7, %fd112, %fd112;
	@%p14 bra 	$L__BB36_13;

	mov.f64 	%fd59, 0d3FF0000000000000;
	fma.rn.f64 	%fd114, %fd7, %fd6, %fd59;

$L__BB36_13:
	and.b32  	%r40, %r69, 2;
	setp.eq.s32 	%p15, %r40, 0;
	@%p15 bra 	$L__BB36_15;

	mov.f64 	%fd60, 0d0000000000000000;
	mov.f64 	%fd61, 0dBFF0000000000000;
	fma.rn.f64 	%fd114, %fd114, %fd61, %fd60;

$L__BB36_15:
	add.s64 	%rd53, %rd1, %rd46;
	st.global.f64 	[%rd53], %fd114;
	add.s32 	%r68, %r68, %r5;
	cvt.u64.u32 	%rd80, %r68;
	setp.lt.u64 	%p16, %rd80, %rd28;
	@%p16 bra 	$L__BB36_7;
	bra.uni 	$L__BB36_42;

$L__BB36_32:
	mov.u64 	%rd77, __cudart_sin_cos_coeffs;

$L__BB36_33:
	shl.b64 	%rd73, %rd80, 3;
	add.s64 	%rd26, %rd1, %rd73;
	setp.eq.s64 	%p29, %rd30, 0;
	selp.b64 	%rd74, %rd26, %rd2, %p29;
	ld.global.f64 	%fd25, [%rd74];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd25;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd25;
	}
	and.b32  	%r61, %r60, 2147483647;
	setp.eq.s32 	%p30, %r61, 2146435072;
	setp.eq.s32 	%p31, %r59, 0;
	and.pred  	%p32, %p31, %p30;
	@%p32 bra 	$L__BB36_36;
	bra.uni 	$L__BB36_34;

$L__BB36_36:
	mov.f64 	%fd96, 0d0000000000000000;
	mul.rn.f64 	%fd118, %fd25, %fd96;
	mov.u32 	%r76, 0;
	bra.uni 	$L__BB36_37;

$L__BB36_34:
	mul.f64 	%fd87, %fd25, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r76, %fd87;
	st.local.u32 	[%rd5], %r76;
	cvt.rn.f64.s32 	%fd88, %r76;
	neg.f64 	%fd89, %fd88;
	mov.f64 	%fd90, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd91, %fd89, %fd90, %fd25;
	mov.f64 	%fd92, 0d3C91A62633145C00;
	fma.rn.f64 	%fd93, %fd89, %fd92, %fd91;
	mov.f64 	%fd94, 0d397B839A252049C0;
	fma.rn.f64 	%fd118, %fd89, %fd94, %fd93;
	abs.f64 	%fd95, %fd25;
	setp.ltu.f64 	%p33, %fd95, 0d41E0000000000000;
	@%p33 bra 	$L__BB36_37;

	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd34;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd118, [retval0+0];
	} // callseq 2
	ld.local.u32 	%r76, [%rd5];

$L__BB36_37:
	and.b32  	%r63, %r76, 1;
	shl.b32 	%r64, %r76, 3;
	and.b32  	%r65, %r64, 8;
	setp.eq.s32 	%p34, %r63, 0;
	selp.f64 	%fd97, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p34;
	mul.wide.s32 	%rd76, %r65, 8;
	add.s64 	%rd78, %rd77, %rd76;
	ld.global.nc.f64 	%fd98, [%rd78+8];
	mul.rn.f64 	%fd30, %fd118, %fd118;
	fma.rn.f64 	%fd99, %fd97, %fd30, %fd98;
	ld.global.nc.f64 	%fd100, [%rd78+16];
	fma.rn.f64 	%fd101, %fd99, %fd30, %fd100;
	ld.global.nc.f64 	%fd102, [%rd78+24];
	fma.rn.f64 	%fd103, %fd101, %fd30, %fd102;
	ld.global.nc.f64 	%fd104, [%rd78+32];
	fma.rn.f64 	%fd105, %fd103, %fd30, %fd104;
	ld.global.nc.f64 	%fd106, [%rd78+40];
	fma.rn.f64 	%fd107, %fd105, %fd30, %fd106;
	ld.global.nc.f64 	%fd108, [%rd78+48];
	fma.rn.f64 	%fd31, %fd107, %fd30, %fd108;
	fma.rn.f64 	%fd120, %fd31, %fd118, %fd118;
	@%p34 bra 	$L__BB36_39;

	mov.f64 	%fd109, 0d3FF0000000000000;
	fma.rn.f64 	%fd120, %fd31, %fd30, %fd109;

$L__BB36_39:
	and.b32  	%r66, %r76, 2;
	setp.eq.s32 	%p35, %r66, 0;
	@%p35 bra 	$L__BB36_41;

	mov.f64 	%fd110, 0d0000000000000000;
	mov.f64 	%fd111, 0dBFF0000000000000;
	fma.rn.f64 	%fd120, %fd120, %fd111, %fd110;

$L__BB36_41:
	st.global.f64 	[%rd26], %fd120;
	add.s32 	%r68, %r68, %r11;
	cvt.u64.u32 	%rd80, %r68;
	setp.lt.u64 	%p36, %rd80, %rd28;
	@%p36 bra 	$L__BB36_33;

$L__BB36_42:
	ret;

}
	// .globl	ucos_f32
.visible .entry ucos_f32(
	.param .u64 ucos_f32_param_0,
	.param .u64 ucos_f32_param_1,
	.param .u64 ucos_f32_param_2,
	.param .u64 ucos_f32_param_3,
	.param .u64 ucos_f32_param_4
)
{
	.local .align 4 .b8 	__local_depot37[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<50>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<199>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<132>;


	mov.u64 	%SPL, __local_depot37;
	ld.param.u64 	%rd42, [ucos_f32_param_0];
	ld.param.u64 	%rd43, [ucos_f32_param_1];
	ld.param.u64 	%rd46, [ucos_f32_param_2];
	ld.param.u64 	%rd44, [ucos_f32_param_3];
	ld.param.u64 	%rd45, [ucos_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd45;
	cvta.to.global.u64 	%rd2, %rd44;
	cvta.to.global.u64 	%rd3, %rd46;
	add.u64 	%rd4, %SPL, 0;
	setp.eq.s64 	%p3, %rd43, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p49, %p2;
	@%p3 bra 	$L__BB37_4;

	mov.u64 	%rd119, 1;
	mov.u32 	%r182, 0;

$L__BB37_2:
	not.b32 	%r73, %r182;
	cvt.u64.u32 	%rd49, %r73;
	add.s64 	%rd50, %rd49, %rd43;
	and.b64  	%rd6, %rd50, 4294967295;
	add.s64 	%rd51, %rd6, %rd43;
	shl.b64 	%rd52, %rd51, 3;
	add.s64 	%rd53, %rd3, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	setp.ne.s64 	%p5, %rd119, %rd54;
	mov.pred 	%p49, -1;
	@%p5 bra 	$L__BB37_4;

	shl.b64 	%rd55, %rd6, 3;
	add.s64 	%rd56, %rd3, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	mul.lo.s64 	%rd119, %rd57, %rd119;
	add.s32 	%r182, %r182, 1;
	cvt.u64.u32 	%rd58, %r182;
	setp.lt.u64 	%p7, %rd58, %rd43;
	mov.pred 	%p49, %p2;
	@%p7 bra 	$L__BB37_2;

$L__BB37_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r74, %ctaid.x;
	mov.u32 	%r75, %tid.x;
	mad.lo.s32 	%r183, %r74, %r3, %r75;
	cvt.u64.u32 	%rd120, %r183;
	add.s64 	%rd9, %rd4, 24;
	@%p49 bra 	$L__BB37_20;
	bra.uni 	$L__BB37_5;

$L__BB37_20:
	setp.ge.u64 	%p21, %rd120, %rd42;
	@%p21 bra 	$L__BB37_54;

	mov.u32 	%r109, %nctaid.x;
	mul.lo.s32 	%r25, %r3, %r109;
	@%p3 bra 	$L__BB37_40;

$L__BB37_22:
	mov.u32 	%r188, 0;
	mov.u32 	%r189, %r183;
	mov.u32 	%r190, %r188;

$L__BB37_23:
	not.b32 	%r112, %r188;
	cvt.u64.u32 	%rd77, %r112;
	add.s64 	%rd78, %rd77, %rd43;
	cvt.u64.u32 	%rd18, %r189;
	shl.b64 	%rd79, %rd78, 3;
	and.b64  	%rd80, %rd79, 34359738360;
	add.s64 	%rd19, %rd3, %rd80;
	ld.global.u64 	%rd20, [%rd19];
	and.b64  	%rd81, %rd20, -4294967296;
	setp.eq.s64 	%p23, %rd81, 0;
	@%p23 bra 	$L__BB37_25;

	div.u64 	%rd124, %rd18, %rd20;
	mul.lo.s64 	%rd82, %rd124, %rd20;
	sub.s64 	%rd125, %rd18, %rd82;
	bra.uni 	$L__BB37_26;

$L__BB37_25:
	cvt.u32.u64 	%r113, %rd20;
	cvt.u32.u64 	%r114, %rd18;
	div.u32 	%r115, %r114, %r113;
	mul.lo.s32 	%r116, %r115, %r113;
	sub.s32 	%r117, %r114, %r116;
	cvt.u64.u32 	%rd124, %r115;
	cvt.u64.u32 	%rd125, %r117;

$L__BB37_26:
	shl.b64 	%rd83, %rd43, 3;
	add.s64 	%rd84, %rd19, %rd83;
	ld.global.u64 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd125;
	cvt.u32.u64 	%r118, %rd86;
	add.s32 	%r190, %r190, %r118;
	cvt.u32.u64 	%r189, %rd124;
	add.s32 	%r188, %r188, 1;
	cvt.u64.u32 	%rd87, %r188;
	setp.lt.u64 	%p24, %rd87, %rd43;
	@%p24 bra 	$L__BB37_23;

	mul.wide.u32 	%rd88, %r190, 4;
	add.s64 	%rd89, %rd2, %rd88;
	shl.b64 	%rd91, %rd120, 2;
	add.s64 	%rd27, %rd1, %rd91;
	setp.eq.s64 	%p25, %rd44, 0;
	selp.b64 	%rd92, %rd27, %rd89, %p25;
	ld.global.f32 	%f14, [%rd92];
	mul.f32 	%f61, %f14, 0f3F22F983;
	cvt.rni.s32.f32 	%r193, %f61;
	cvt.rn.f32.s32 	%f62, %r193;
	mov.f32 	%f63, 0fBFC90FDA;
	fma.rn.f32 	%f64, %f62, %f63, %f14;
	mov.f32 	%f65, 0fB3A22168;
	fma.rn.f32 	%f66, %f62, %f65, %f64;
	mov.f32 	%f67, 0fA7C234C5;
	fma.rn.f32 	%f106, %f62, %f67, %f66;
	abs.f32 	%f16, %f14;
	setp.ltu.f32 	%p26, %f16, 0f47CE4780;
	@%p26 bra 	$L__BB37_35;

	setp.eq.f32 	%p27, %f16, 0f7F800000;
	@%p27 bra 	$L__BB37_34;
	bra.uni 	$L__BB37_29;

$L__BB37_34:
	mov.f32 	%f70, 0f00000000;
	mul.rn.f32 	%f106, %f14, %f70;
	mov.u32 	%r193, 0;
	bra.uni 	$L__BB37_35;

$L__BB37_29:
	mov.b32 	%r34, %f14;
	shr.u32 	%r119, %r34, 23;
	and.b32  	%r120, %r119, 255;
	add.s32 	%r35, %r120, -128;
	shl.b32 	%r121, %r34, 8;
	or.b32  	%r36, %r121, -2147483648;
	shr.u32 	%r37, %r35, 5;
	mov.u64 	%rd126, 0;
	mov.u64 	%rd127, %rd126;

$L__BB37_30:
	.pragma "nounroll";
	shl.b64 	%rd95, %rd126, 2;
	mov.u64 	%rd96, __cudart_i2opi_f;
	add.s64 	%rd97, %rd96, %rd95;
	ld.global.nc.u32 	%r122, [%rd97];
	mad.wide.u32 	%rd98, %r122, %r36, %rd127;
	shr.u64 	%rd127, %rd98, 32;
	add.s64 	%rd99, %rd4, %rd95;
	st.local.u32 	[%rd99], %rd98;
	cvt.u32.u64 	%r123, %rd126;
	add.s32 	%r124, %r123, 1;
	cvt.s64.s32 	%rd126, %r124;
	setp.ne.s32 	%p28, %r124, 6;
	@%p28 bra 	$L__BB37_30;

	st.local.u32 	[%rd9], %rd127;
	mov.u32 	%r125, 4;
	sub.s32 	%r38, %r125, %r37;
	mov.u32 	%r126, 6;
	sub.s32 	%r127, %r126, %r37;
	mul.wide.s32 	%rd100, %r127, 4;
	add.s64 	%rd101, %rd4, %rd100;
	ld.local.u32 	%r191, [%rd101];
	ld.local.u32 	%r192, [%rd101+-4];
	and.b32  	%r41, %r35, 31;
	setp.eq.s32 	%p29, %r41, 0;
	@%p29 bra 	$L__BB37_33;

	mov.u32 	%r128, 32;
	sub.s32 	%r129, %r128, %r41;
	shr.u32 	%r130, %r192, %r129;
	shl.b32 	%r131, %r191, %r41;
	add.s32 	%r191, %r130, %r131;
	mul.wide.s32 	%rd102, %r38, 4;
	add.s64 	%rd103, %rd4, %rd102;
	ld.local.u32 	%r132, [%rd103];
	shr.u32 	%r133, %r132, %r129;
	shl.b32 	%r134, %r192, %r41;
	add.s32 	%r192, %r133, %r134;

$L__BB37_33:
	and.b32  	%r135, %r34, -2147483648;
	shr.u32 	%r136, %r192, 30;
	shl.b32 	%r137, %r191, 2;
	or.b32  	%r138, %r136, %r137;
	shr.u32 	%r139, %r138, 31;
	shr.u32 	%r140, %r191, 30;
	add.s32 	%r141, %r139, %r140;
	neg.s32 	%r142, %r141;
	setp.eq.s32 	%p30, %r135, 0;
	selp.b32 	%r193, %r141, %r142, %p30;
	setp.ne.s32 	%p31, %r139, 0;
	xor.b32  	%r143, %r135, -2147483648;
	selp.b32 	%r144, %r143, %r135, %p31;
	selp.b32 	%r145, -1, 0, %p31;
	xor.b32  	%r146, %r138, %r145;
	shl.b32 	%r147, %r192, 2;
	xor.b32  	%r148, %r147, %r145;
	cvt.u64.u32 	%rd104, %r146;
	cvt.u64.u32 	%rd105, %r148;
	bfi.b64 	%rd106, %rd104, %rd105, 32, 32;
	cvt.rn.f64.s64 	%fd3, %rd106;
	mul.f64 	%fd4, %fd3, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f68, %fd4;
	setp.eq.s32 	%p32, %r144, 0;
	neg.f32 	%f69, %f68;
	selp.f32 	%f106, %f68, %f69, %p32;

$L__BB37_35:
	add.s32 	%r48, %r193, 1;
	and.b32  	%r49, %r48, 1;
	setp.eq.s32 	%p33, %r49, 0;
	selp.f32 	%f20, %f106, 0f3F800000, %p33;
	mul.rn.f32 	%f21, %f106, %f106;
	mov.f32 	%f107, 0fB94D4153;
	@%p33 bra 	$L__BB37_37;

	mov.f32 	%f72, 0fBAB607ED;
	mov.f32 	%f73, 0f37CBAC00;
	fma.rn.f32 	%f107, %f73, %f21, %f72;

$L__BB37_37:
	selp.f32 	%f74, 0f3C0885E4, 0f3D2AAABB, %p33;
	fma.rn.f32 	%f75, %f107, %f21, %f74;
	selp.f32 	%f76, 0fBE2AAAA8, 0fBEFFFFFF, %p33;
	fma.rn.f32 	%f77, %f75, %f21, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f79, %f21, %f20, %f78;
	fma.rn.f32 	%f108, %f77, %f79, %f20;
	and.b32  	%r150, %r48, 2;
	setp.eq.s32 	%p35, %r150, 0;
	@%p35 bra 	$L__BB37_39;

	mov.f32 	%f81, 0fBF800000;
	fma.rn.f32 	%f108, %f108, %f81, %f78;

$L__BB37_39:
	st.global.f32 	[%rd27], %f108;
	add.s32 	%r183, %r183, %r25;
	cvt.u64.u32 	%rd120, %r183;
	setp.lt.u64 	%p36, %rd120, %rd42;
	@%p36 bra 	$L__BB37_22;
	bra.uni 	$L__BB37_54;

$L__BB37_5:
	setp.ge.u64 	%p8, %rd120, %rd42;
	@%p8 bra 	$L__BB37_54;

	setp.eq.s64 	%p9, %rd44, 0;
	selp.b64 	%rd10, %rd1, %rd2, %p9;
	mov.u32 	%r76, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r76;

$L__BB37_7:
	shl.b64 	%rd59, %rd120, 2;
	add.s64 	%rd60, %rd10, %rd59;
	ld.global.f32 	%f1, [%rd60];
	mul.f32 	%f40, %f1, 0f3F22F983;
	cvt.rni.s32.f32 	%r186, %f40;
	cvt.rn.f32.s32 	%f41, %r186;
	mov.f32 	%f42, 0fBFC90FDA;
	fma.rn.f32 	%f43, %f41, %f42, %f1;
	mov.f32 	%f44, 0fB3A22168;
	fma.rn.f32 	%f45, %f41, %f44, %f43;
	mov.f32 	%f46, 0fA7C234C5;
	fma.rn.f32 	%f103, %f41, %f46, %f45;
	abs.f32 	%f3, %f1;
	setp.ltu.f32 	%p10, %f3, 0f47CE4780;
	@%p10 bra 	$L__BB37_15;

	setp.eq.f32 	%p11, %f3, 0f7F800000;
	@%p11 bra 	$L__BB37_14;
	bra.uni 	$L__BB37_9;

$L__BB37_14:
	mov.f32 	%f49, 0f00000000;
	mul.rn.f32 	%f103, %f1, %f49;
	mov.u32 	%r186, 0;
	bra.uni 	$L__BB37_15;

$L__BB37_9:
	mov.b32 	%r8, %f1;
	shr.u32 	%r77, %r8, 23;
	and.b32  	%r78, %r77, 255;
	add.s32 	%r9, %r78, -128;
	shl.b32 	%r79, %r8, 8;
	or.b32  	%r10, %r79, -2147483648;
	shr.u32 	%r11, %r9, 5;
	mov.u64 	%rd121, 0;
	mov.u64 	%rd122, %rd121;

$L__BB37_10:
	.pragma "nounroll";
	shl.b64 	%rd63, %rd121, 2;
	mov.u64 	%rd64, __cudart_i2opi_f;
	add.s64 	%rd65, %rd64, %rd63;
	ld.global.nc.u32 	%r80, [%rd65];
	mad.wide.u32 	%rd66, %r80, %r10, %rd122;
	shr.u64 	%rd122, %rd66, 32;
	add.s64 	%rd67, %rd4, %rd63;
	st.local.u32 	[%rd67], %rd66;
	cvt.u32.u64 	%r81, %rd121;
	add.s32 	%r82, %r81, 1;
	cvt.s64.s32 	%rd121, %r82;
	setp.ne.s32 	%p12, %r82, 6;
	@%p12 bra 	$L__BB37_10;

	st.local.u32 	[%rd9], %rd122;
	mov.u32 	%r83, 4;
	sub.s32 	%r12, %r83, %r11;
	mov.u32 	%r84, 6;
	sub.s32 	%r85, %r84, %r11;
	mul.wide.s32 	%rd68, %r85, 4;
	add.s64 	%rd69, %rd4, %rd68;
	ld.local.u32 	%r184, [%rd69];
	ld.local.u32 	%r185, [%rd69+-4];
	and.b32  	%r15, %r9, 31;
	setp.eq.s32 	%p13, %r15, 0;
	@%p13 bra 	$L__BB37_13;

	mov.u32 	%r86, 32;
	sub.s32 	%r87, %r86, %r15;
	shr.u32 	%r88, %r185, %r87;
	shl.b32 	%r89, %r184, %r15;
	add.s32 	%r184, %r88, %r89;
	mul.wide.s32 	%rd70, %r12, 4;
	add.s64 	%rd71, %rd4, %rd70;
	ld.local.u32 	%r90, [%rd71];
	shr.u32 	%r91, %r90, %r87;
	shl.b32 	%r92, %r185, %r15;
	add.s32 	%r185, %r91, %r92;

$L__BB37_13:
	and.b32  	%r93, %r8, -2147483648;
	shr.u32 	%r94, %r185, 30;
	shl.b32 	%r95, %r184, 2;
	or.b32  	%r96, %r94, %r95;
	shr.u32 	%r97, %r96, 31;
	shr.u32 	%r98, %r184, 30;
	add.s32 	%r99, %r97, %r98;
	neg.s32 	%r100, %r99;
	setp.eq.s32 	%p14, %r93, 0;
	selp.b32 	%r186, %r99, %r100, %p14;
	setp.ne.s32 	%p15, %r97, 0;
	xor.b32  	%r101, %r93, -2147483648;
	selp.b32 	%r102, %r101, %r93, %p15;
	selp.b32 	%r103, -1, 0, %p15;
	xor.b32  	%r104, %r96, %r103;
	shl.b32 	%r105, %r185, 2;
	xor.b32  	%r106, %r105, %r103;
	cvt.u64.u32 	%rd72, %r104;
	cvt.u64.u32 	%rd73, %r106;
	bfi.b64 	%rd74, %rd72, %rd73, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd74;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f47, %fd2;
	setp.eq.s32 	%p16, %r102, 0;
	neg.f32 	%f48, %f47;
	selp.f32 	%f103, %f47, %f48, %p16;

$L__BB37_15:
	add.s32 	%r22, %r186, 1;
	and.b32  	%r23, %r22, 1;
	setp.eq.s32 	%p17, %r23, 0;
	selp.f32 	%f7, %f103, 0f3F800000, %p17;
	mul.rn.f32 	%f8, %f103, %f103;
	mov.f32 	%f104, 0fB94D4153;
	@%p17 bra 	$L__BB37_17;

	mov.f32 	%f51, 0fBAB607ED;
	mov.f32 	%f52, 0f37CBAC00;
	fma.rn.f32 	%f104, %f52, %f8, %f51;

$L__BB37_17:
	selp.f32 	%f53, 0f3C0885E4, 0f3D2AAABB, %p17;
	fma.rn.f32 	%f54, %f104, %f8, %f53;
	selp.f32 	%f55, 0fBE2AAAA8, 0fBEFFFFFF, %p17;
	fma.rn.f32 	%f56, %f54, %f8, %f55;
	mov.f32 	%f57, 0f00000000;
	fma.rn.f32 	%f58, %f8, %f7, %f57;
	fma.rn.f32 	%f105, %f56, %f58, %f7;
	and.b32  	%r108, %r22, 2;
	setp.eq.s32 	%p19, %r108, 0;
	@%p19 bra 	$L__BB37_19;

	mov.f32 	%f60, 0fBF800000;
	fma.rn.f32 	%f105, %f105, %f60, %f57;

$L__BB37_19:
	add.s64 	%rd76, %rd1, %rd59;
	st.global.f32 	[%rd76], %f105;
	add.s32 	%r183, %r183, %r5;
	cvt.u64.u32 	%rd120, %r183;
	setp.lt.u64 	%p20, %rd120, %rd42;
	@%p20 bra 	$L__BB37_7;
	bra.uni 	$L__BB37_54;

$L__BB37_40:
	mov.u64 	%rd109, __cudart_i2opi_f;

$L__BB37_41:
	shl.b64 	%rd107, %rd120, 2;
	add.s64 	%rd34, %rd1, %rd107;
	setp.eq.s64 	%p37, %rd44, 0;
	selp.b64 	%rd108, %rd34, %rd2, %p37;
	ld.global.f32 	%f27, [%rd108];
	mul.f32 	%f82, %f27, 0f3F22F983;
	cvt.rni.s32.f32 	%r198, %f82;
	cvt.rn.f32.s32 	%f83, %r198;
	mov.f32 	%f84, 0fBFC90FDA;
	fma.rn.f32 	%f85, %f83, %f84, %f27;
	mov.f32 	%f86, 0fB3A22168;
	fma.rn.f32 	%f87, %f83, %f86, %f85;
	mov.f32 	%f88, 0fA7C234C5;
	fma.rn.f32 	%f109, %f83, %f88, %f87;
	abs.f32 	%f29, %f27;
	setp.ltu.f32 	%p38, %f29, 0f47CE4780;
	@%p38 bra 	$L__BB37_49;

	setp.eq.f32 	%p39, %f29, 0f7F800000;
	@%p39 bra 	$L__BB37_48;
	bra.uni 	$L__BB37_43;

$L__BB37_48:
	mov.f32 	%f91, 0f00000000;
	mul.rn.f32 	%f109, %f27, %f91;
	mov.u32 	%r198, 0;
	bra.uni 	$L__BB37_49;

$L__BB37_43:
	mov.b32 	%r53, %f27;
	shr.u32 	%r152, %r53, 23;
	and.b32  	%r153, %r152, 255;
	add.s32 	%r54, %r153, -128;
	shl.b32 	%r154, %r53, 8;
	or.b32  	%r55, %r154, -2147483648;
	shr.u32 	%r56, %r54, 5;
	mov.u64 	%rd131, 0;
	mov.u32 	%r195, 0;
	mov.u64 	%rd129, %rd109;
	mov.u64 	%rd130, %rd4;

$L__BB37_44:
	.pragma "nounroll";
	ld.global.nc.u32 	%r155, [%rd129];
	mad.wide.u32 	%rd111, %r155, %r55, %rd131;
	shr.u64 	%rd131, %rd111, 32;
	st.local.u32 	[%rd130], %rd111;
	add.s64 	%rd130, %rd130, 4;
	add.s64 	%rd129, %rd129, 4;
	add.s32 	%r195, %r195, 1;
	setp.ne.s32 	%p40, %r195, 6;
	@%p40 bra 	$L__BB37_44;

	st.local.u32 	[%rd9], %rd131;
	mov.u32 	%r156, 4;
	sub.s32 	%r59, %r156, %r56;
	mov.u32 	%r157, 6;
	sub.s32 	%r158, %r157, %r56;
	mul.wide.s32 	%rd112, %r158, 4;
	add.s64 	%rd113, %rd4, %rd112;
	ld.local.u32 	%r196, [%rd113];
	ld.local.u32 	%r197, [%rd113+-4];
	and.b32  	%r62, %r54, 31;
	setp.eq.s32 	%p41, %r62, 0;
	@%p41 bra 	$L__BB37_47;

	mov.u32 	%r159, 32;
	sub.s32 	%r160, %r159, %r62;
	shr.u32 	%r161, %r197, %r160;
	shl.b32 	%r162, %r196, %r62;
	add.s32 	%r196, %r161, %r162;
	mul.wide.s32 	%rd114, %r59, 4;
	add.s64 	%rd115, %rd4, %rd114;
	ld.local.u32 	%r163, [%rd115];
	shr.u32 	%r164, %r163, %r160;
	shl.b32 	%r165, %r197, %r62;
	add.s32 	%r197, %r164, %r165;

$L__BB37_47:
	and.b32  	%r166, %r53, -2147483648;
	shr.u32 	%r167, %r197, 30;
	shl.b32 	%r168, %r196, 2;
	or.b32  	%r169, %r167, %r168;
	shr.u32 	%r170, %r169, 31;
	shr.u32 	%r171, %r196, 30;
	add.s32 	%r172, %r170, %r171;
	neg.s32 	%r173, %r172;
	setp.eq.s32 	%p42, %r166, 0;
	selp.b32 	%r198, %r172, %r173, %p42;
	setp.ne.s32 	%p43, %r170, 0;
	xor.b32  	%r174, %r166, -2147483648;
	selp.b32 	%r175, %r174, %r166, %p43;
	selp.b32 	%r176, -1, 0, %p43;
	xor.b32  	%r177, %r169, %r176;
	shl.b32 	%r178, %r197, 2;
	xor.b32  	%r179, %r178, %r176;
	cvt.u64.u32 	%rd116, %r177;
	cvt.u64.u32 	%rd117, %r179;
	bfi.b64 	%rd118, %rd116, %rd117, 32, 32;
	cvt.rn.f64.s64 	%fd5, %rd118;
	mul.f64 	%fd6, %fd5, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f89, %fd6;
	setp.eq.s32 	%p44, %r175, 0;
	neg.f32 	%f90, %f89;
	selp.f32 	%f109, %f89, %f90, %p44;

$L__BB37_49:
	add.s32 	%r69, %r198, 1;
	and.b32  	%r70, %r69, 1;
	setp.eq.s32 	%p45, %r70, 0;
	selp.f32 	%f33, %f109, 0f3F800000, %p45;
	mul.rn.f32 	%f34, %f109, %f109;
	mov.f32 	%f110, 0fB94D4153;
	@%p45 bra 	$L__BB37_51;

	mov.f32 	%f93, 0fBAB607ED;
	mov.f32 	%f94, 0f37CBAC00;
	fma.rn.f32 	%f110, %f94, %f34, %f93;

$L__BB37_51:
	selp.f32 	%f95, 0f3C0885E4, 0f3D2AAABB, %p45;
	fma.rn.f32 	%f96, %f110, %f34, %f95;
	selp.f32 	%f97, 0fBE2AAAA8, 0fBEFFFFFF, %p45;
	fma.rn.f32 	%f98, %f96, %f34, %f97;
	mov.f32 	%f99, 0f00000000;
	fma.rn.f32 	%f100, %f34, %f33, %f99;
	fma.rn.f32 	%f111, %f98, %f100, %f33;
	and.b32  	%r181, %r69, 2;
	setp.eq.s32 	%p47, %r181, 0;
	@%p47 bra 	$L__BB37_53;

	mov.f32 	%f102, 0fBF800000;
	fma.rn.f32 	%f111, %f111, %f102, %f99;

$L__BB37_53:
	st.global.f32 	[%rd34], %f111;
	add.s32 	%r183, %r183, %r25;
	cvt.u64.u32 	%rd120, %r183;
	setp.lt.u64 	%p48, %rd120, %rd42;
	@%p48 bra 	$L__BB37_41;

$L__BB37_54:
	ret;

}
	// .globl	ucos_f64
.visible .entry ucos_f64(
	.param .u64 ucos_f64_param_0,
	.param .u64 ucos_f64_param_1,
	.param .u64 ucos_f64_param_2,
	.param .u64 ucos_f64_param_3,
	.param .u64 ucos_f64_param_4
)
{
	.local .align 4 .b8 	__local_depot38[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<38>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<127>;
	.reg .b64 	%rd<86>;


	mov.u64 	%SPL, __local_depot38;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd27, [ucos_f64_param_0];
	ld.param.u64 	%rd28, [ucos_f64_param_1];
	ld.param.u64 	%rd30, [ucos_f64_param_2];
	ld.param.u64 	%rd29, [ucos_f64_param_3];
	ld.param.u64 	%rd31, [ucos_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	add.u64 	%rd32, %SP, 4;
	add.u64 	%rd4, %SPL, 4;
	setp.eq.s64 	%p3, %rd28, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p37, %p2;
	@%p3 bra 	$L__BB38_4;

	mov.u64 	%rd80, 1;
	mov.u32 	%r73, 0;

$L__BB38_2:
	not.b32 	%r35, %r73;
	cvt.u64.u32 	%rd34, %r35;
	add.s64 	%rd35, %rd34, %rd28;
	and.b64  	%rd6, %rd35, 4294967295;
	add.s64 	%rd36, %rd6, %rd28;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd80, %rd39;
	mov.pred 	%p37, -1;
	@%p5 bra 	$L__BB38_4;

	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd80, %rd42, %rd80;
	add.s32 	%r73, %r73, 1;
	cvt.u64.u32 	%rd43, %r73;
	setp.lt.u64 	%p7, %rd43, %rd28;
	mov.pred 	%p37, %p2;
	@%p7 bra 	$L__BB38_2;

$L__BB38_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r36, %ctaid.x;
	mov.u32 	%r37, %tid.x;
	mad.lo.s32 	%r74, %r36, %r3, %r37;
	cvt.u64.u32 	%rd81, %r74;
	@%p37 bra 	$L__BB38_17;
	bra.uni 	$L__BB38_5;

$L__BB38_17:
	setp.ge.u64 	%p17, %rd81, %rd27;
	@%p17 bra 	$L__BB38_45;

	mov.u32 	%r47, %nctaid.x;
	mul.lo.s32 	%r13, %r3, %r47;
	@%p3 bra 	$L__BB38_34;

$L__BB38_19:
	mov.u32 	%r78, 0;
	mov.u32 	%r79, %r74;
	mov.u32 	%r80, %r78;

$L__BB38_20:
	not.b32 	%r50, %r78;
	cvt.u64.u32 	%rd55, %r50;
	add.s64 	%rd56, %rd55, %rd28;
	cvt.u64.u32 	%rd13, %r79;
	shl.b64 	%rd57, %rd56, 3;
	and.b64  	%rd58, %rd57, 34359738360;
	add.s64 	%rd14, %rd3, %rd58;
	ld.global.u64 	%rd15, [%rd14];
	and.b64  	%rd59, %rd15, -4294967296;
	setp.eq.s64 	%p19, %rd59, 0;
	@%p19 bra 	$L__BB38_22;

	div.u64 	%rd83, %rd13, %rd15;
	mul.lo.s64 	%rd60, %rd83, %rd15;
	sub.s64 	%rd84, %rd13, %rd60;
	bra.uni 	$L__BB38_23;

$L__BB38_22:
	cvt.u32.u64 	%r51, %rd15;
	cvt.u32.u64 	%r52, %rd13;
	div.u32 	%r53, %r52, %r51;
	mul.lo.s32 	%r54, %r53, %r51;
	sub.s32 	%r55, %r52, %r54;
	cvt.u64.u32 	%rd83, %r53;
	cvt.u64.u32 	%rd84, %r55;

$L__BB38_23:
	shl.b64 	%rd61, %rd28, 3;
	add.s64 	%rd62, %rd14, %rd61;
	ld.global.u64 	%rd63, [%rd62];
	mul.lo.s64 	%rd64, %rd63, %rd84;
	cvt.u32.u64 	%r56, %rd64;
	add.s32 	%r80, %r80, %r56;
	cvt.u32.u64 	%r79, %rd83;
	add.s32 	%r78, %r78, 1;
	cvt.u64.u32 	%rd65, %r78;
	setp.lt.u64 	%p20, %rd65, %rd28;
	@%p20 bra 	$L__BB38_20;

	mul.wide.u32 	%rd66, %r80, 8;
	add.s64 	%rd67, %rd2, %rd66;
	shl.b64 	%rd68, %rd81, 3;
	add.s64 	%rd22, %rd1, %rd68;
	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd69, %rd22, %rd67, %p21;
	ld.global.f64 	%fd14, [%rd69];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r57, %temp}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd14;
	}
	and.b32  	%r59, %r58, 2147483647;
	setp.eq.s32 	%p22, %r59, 2146435072;
	setp.eq.s32 	%p23, %r57, 0;
	and.pred  	%p24, %p23, %p22;
	@%p24 bra 	$L__BB38_28;
	bra.uni 	$L__BB38_25;

$L__BB38_28:
	mov.f64 	%fd74, 0d0000000000000000;
	mul.rn.f64 	%fd120, %fd14, %fd74;
	mov.u32 	%r82, 1;
	bra.uni 	$L__BB38_29;

$L__BB38_25:
	mul.f64 	%fd65, %fd14, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r81, %fd65;
	st.local.u32 	[%rd4], %r81;
	cvt.rn.f64.s32 	%fd66, %r81;
	neg.f64 	%fd67, %fd66;
	mov.f64 	%fd68, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd69, %fd67, %fd68, %fd14;
	mov.f64 	%fd70, 0d3C91A62633145C00;
	fma.rn.f64 	%fd71, %fd67, %fd70, %fd69;
	mov.f64 	%fd72, 0d397B839A252049C0;
	fma.rn.f64 	%fd120, %fd67, %fd72, %fd71;
	abs.f64 	%fd73, %fd14;
	setp.ltu.f64 	%p25, %fd73, 0d41E0000000000000;
	@%p25 bra 	$L__BB38_27;

	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd120, [retval0+0];
	} // callseq 4
	ld.local.u32 	%r81, [%rd4];

$L__BB38_27:
	add.s32 	%r82, %r81, 1;

$L__BB38_29:
	and.b32  	%r61, %r82, 1;
	shl.b32 	%r62, %r82, 3;
	and.b32  	%r63, %r62, 8;
	setp.eq.s32 	%p26, %r61, 0;
	selp.f64 	%fd75, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p26;
	mul.wide.s32 	%rd71, %r63, 8;
	mov.u64 	%rd72, __cudart_sin_cos_coeffs;
	add.s64 	%rd73, %rd72, %rd71;
	ld.global.nc.f64 	%fd76, [%rd73+8];
	mul.rn.f64 	%fd20, %fd120, %fd120;
	fma.rn.f64 	%fd77, %fd75, %fd20, %fd76;
	ld.global.nc.f64 	%fd78, [%rd73+16];
	fma.rn.f64 	%fd79, %fd77, %fd20, %fd78;
	ld.global.nc.f64 	%fd80, [%rd73+24];
	fma.rn.f64 	%fd81, %fd79, %fd20, %fd80;
	ld.global.nc.f64 	%fd82, [%rd73+32];
	fma.rn.f64 	%fd83, %fd81, %fd20, %fd82;
	ld.global.nc.f64 	%fd84, [%rd73+40];
	fma.rn.f64 	%fd85, %fd83, %fd20, %fd84;
	ld.global.nc.f64 	%fd86, [%rd73+48];
	fma.rn.f64 	%fd21, %fd85, %fd20, %fd86;
	fma.rn.f64 	%fd122, %fd21, %fd120, %fd120;
	@%p26 bra 	$L__BB38_31;

	mov.f64 	%fd87, 0d3FF0000000000000;
	fma.rn.f64 	%fd122, %fd21, %fd20, %fd87;

$L__BB38_31:
	and.b32  	%r64, %r82, 2;
	setp.eq.s32 	%p27, %r64, 0;
	@%p27 bra 	$L__BB38_33;

	mov.f64 	%fd88, 0d0000000000000000;
	mov.f64 	%fd89, 0dBFF0000000000000;
	fma.rn.f64 	%fd122, %fd122, %fd89, %fd88;

$L__BB38_33:
	st.global.f64 	[%rd22], %fd122;
	add.s32 	%r74, %r74, %r13;
	cvt.u64.u32 	%rd81, %r74;
	setp.lt.u64 	%p28, %rd81, %rd27;
	@%p28 bra 	$L__BB38_19;
	bra.uni 	$L__BB38_45;

$L__BB38_5:
	setp.ge.u64 	%p8, %rd81, %rd27;
	@%p8 bra 	$L__BB38_45;

	setp.eq.s64 	%p9, %rd29, 0;
	selp.b64 	%rd9, %rd1, %rd2, %p9;
	mov.u32 	%r38, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r38;

$L__BB38_7:
	shl.b64 	%rd44, %rd81, 3;
	add.s64 	%rd45, %rd9, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd1;
	}
	and.b32  	%r41, %r40, 2147483647;
	setp.eq.s32 	%p10, %r41, 2146435072;
	setp.eq.s32 	%p11, %r39, 0;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB38_11;
	bra.uni 	$L__BB38_8;

$L__BB38_11:
	mov.f64 	%fd49, 0d0000000000000000;
	mul.rn.f64 	%fd116, %fd1, %fd49;
	mov.u32 	%r76, 1;
	bra.uni 	$L__BB38_12;

$L__BB38_8:
	mul.f64 	%fd40, %fd1, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r75, %fd40;
	add.u64 	%rd46, %SP, 0;
	add.u64 	%rd47, %SPL, 0;
	st.local.u32 	[%rd47], %r75;
	cvt.rn.f64.s32 	%fd41, %r75;
	neg.f64 	%fd42, %fd41;
	mov.f64 	%fd43, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd44, %fd42, %fd43, %fd1;
	mov.f64 	%fd45, 0d3C91A62633145C00;
	fma.rn.f64 	%fd46, %fd42, %fd45, %fd44;
	mov.f64 	%fd47, 0d397B839A252049C0;
	fma.rn.f64 	%fd116, %fd42, %fd47, %fd46;
	abs.f64 	%fd48, %fd1;
	setp.ltu.f64 	%p13, %fd48, 0d41E0000000000000;
	@%p13 bra 	$L__BB38_10;

	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd46;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd116, [retval0+0];
	} // callseq 3
	ld.local.u32 	%r75, [%rd47];

$L__BB38_10:
	add.s32 	%r76, %r75, 1;

$L__BB38_12:
	and.b32  	%r43, %r76, 1;
	shl.b32 	%r44, %r76, 3;
	and.b32  	%r45, %r44, 8;
	setp.eq.s32 	%p14, %r43, 0;
	selp.f64 	%fd50, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p14;
	mul.wide.s32 	%rd50, %r45, 8;
	mov.u64 	%rd51, __cudart_sin_cos_coeffs;
	add.s64 	%rd52, %rd51, %rd50;
	ld.global.nc.f64 	%fd51, [%rd52+8];
	mul.rn.f64 	%fd7, %fd116, %fd116;
	fma.rn.f64 	%fd52, %fd50, %fd7, %fd51;
	ld.global.nc.f64 	%fd53, [%rd52+16];
	fma.rn.f64 	%fd54, %fd52, %fd7, %fd53;
	ld.global.nc.f64 	%fd55, [%rd52+24];
	fma.rn.f64 	%fd56, %fd54, %fd7, %fd55;
	ld.global.nc.f64 	%fd57, [%rd52+32];
	fma.rn.f64 	%fd58, %fd56, %fd7, %fd57;
	ld.global.nc.f64 	%fd59, [%rd52+40];
	fma.rn.f64 	%fd60, %fd58, %fd7, %fd59;
	ld.global.nc.f64 	%fd61, [%rd52+48];
	fma.rn.f64 	%fd8, %fd60, %fd7, %fd61;
	fma.rn.f64 	%fd118, %fd8, %fd116, %fd116;
	@%p14 bra 	$L__BB38_14;

	mov.f64 	%fd62, 0d3FF0000000000000;
	fma.rn.f64 	%fd118, %fd8, %fd7, %fd62;

$L__BB38_14:
	and.b32  	%r46, %r76, 2;
	setp.eq.s32 	%p15, %r46, 0;
	@%p15 bra 	$L__BB38_16;

	mov.f64 	%fd63, 0d0000000000000000;
	mov.f64 	%fd64, 0dBFF0000000000000;
	fma.rn.f64 	%fd118, %fd118, %fd64, %fd63;

$L__BB38_16:
	add.s64 	%rd54, %rd1, %rd44;
	st.global.f64 	[%rd54], %fd118;
	add.s32 	%r74, %r74, %r5;
	cvt.u64.u32 	%rd81, %r74;
	setp.lt.u64 	%p16, %rd81, %rd27;
	@%p16 bra 	$L__BB38_7;
	bra.uni 	$L__BB38_45;

$L__BB38_34:
	mov.u64 	%rd78, __cudart_sin_cos_coeffs;

$L__BB38_35:
	shl.b64 	%rd74, %rd81, 3;
	add.s64 	%rd25, %rd1, %rd74;
	setp.eq.s64 	%p29, %rd29, 0;
	selp.b64 	%rd75, %rd25, %rd2, %p29;
	ld.global.f64 	%fd27, [%rd75];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd27;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd27;
	}
	and.b32  	%r67, %r66, 2147483647;
	setp.eq.s32 	%p30, %r67, 2146435072;
	setp.eq.s32 	%p31, %r65, 0;
	and.pred  	%p32, %p31, %p30;
	@%p32 bra 	$L__BB38_39;
	bra.uni 	$L__BB38_36;

$L__BB38_39:
	mov.f64 	%fd99, 0d0000000000000000;
	mul.rn.f64 	%fd124, %fd27, %fd99;
	mov.u32 	%r85, 1;
	bra.uni 	$L__BB38_40;

$L__BB38_36:
	mul.f64 	%fd90, %fd27, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r84, %fd90;
	st.local.u32 	[%rd4], %r84;
	cvt.rn.f64.s32 	%fd91, %r84;
	neg.f64 	%fd92, %fd91;
	mov.f64 	%fd93, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd94, %fd92, %fd93, %fd27;
	mov.f64 	%fd95, 0d3C91A62633145C00;
	fma.rn.f64 	%fd96, %fd92, %fd95, %fd94;
	mov.f64 	%fd97, 0d397B839A252049C0;
	fma.rn.f64 	%fd124, %fd92, %fd97, %fd96;
	abs.f64 	%fd98, %fd27;
	setp.ltu.f64 	%p33, %fd98, 0d41E0000000000000;
	@%p33 bra 	$L__BB38_38;

	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd27;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd124, [retval0+0];
	} // callseq 5
	ld.local.u32 	%r84, [%rd4];

$L__BB38_38:
	add.s32 	%r85, %r84, 1;

$L__BB38_40:
	and.b32  	%r69, %r85, 1;
	shl.b32 	%r70, %r85, 3;
	and.b32  	%r71, %r70, 8;
	setp.eq.s32 	%p34, %r69, 0;
	selp.f64 	%fd100, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p34;
	mul.wide.s32 	%rd77, %r71, 8;
	add.s64 	%rd79, %rd78, %rd77;
	ld.global.nc.f64 	%fd101, [%rd79+8];
	mul.rn.f64 	%fd33, %fd124, %fd124;
	fma.rn.f64 	%fd102, %fd100, %fd33, %fd101;
	ld.global.nc.f64 	%fd103, [%rd79+16];
	fma.rn.f64 	%fd104, %fd102, %fd33, %fd103;
	ld.global.nc.f64 	%fd105, [%rd79+24];
	fma.rn.f64 	%fd106, %fd104, %fd33, %fd105;
	ld.global.nc.f64 	%fd107, [%rd79+32];
	fma.rn.f64 	%fd108, %fd106, %fd33, %fd107;
	ld.global.nc.f64 	%fd109, [%rd79+40];
	fma.rn.f64 	%fd110, %fd108, %fd33, %fd109;
	ld.global.nc.f64 	%fd111, [%rd79+48];
	fma.rn.f64 	%fd34, %fd110, %fd33, %fd111;
	fma.rn.f64 	%fd126, %fd34, %fd124, %fd124;
	@%p34 bra 	$L__BB38_42;

	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd126, %fd34, %fd33, %fd112;

$L__BB38_42:
	and.b32  	%r72, %r85, 2;
	setp.eq.s32 	%p35, %r72, 0;
	@%p35 bra 	$L__BB38_44;

	mov.f64 	%fd113, 0d0000000000000000;
	mov.f64 	%fd114, 0dBFF0000000000000;
	fma.rn.f64 	%fd126, %fd126, %fd114, %fd113;

$L__BB38_44:
	st.global.f64 	[%rd25], %fd126;
	add.s32 	%r74, %r74, %r13;
	cvt.u64.u32 	%rd81, %r74;
	setp.lt.u64 	%p36, %rd81, %rd27;
	@%p36 bra 	$L__BB38_35;

$L__BB38_45:
	ret;

}
	// .globl	utanh_f32
.visible .entry utanh_f32(
	.param .u64 utanh_f32_param_0,
	.param .u64 utanh_f32_param_1,
	.param .u64 utanh_f32_param_2,
	.param .u64 utanh_f32_param_3,
	.param .u64 utanh_f32_param_4
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [utanh_f32_param_0];
	ld.param.u64 	%rd27, [utanh_f32_param_1];
	ld.param.u64 	%rd29, [utanh_f32_param_2];
	ld.param.u64 	%rd28, [utanh_f32_param_3];
	ld.param.u64 	%rd30, [utanh_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p25, %p2;
	@%p3 bra 	$L__BB39_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r46, 0;

$L__BB39_2:
	not.b32 	%r20, %r46;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p25, -1;
	@%p5 bra 	$L__BB39_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd41, %r46;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p25, %p2;
	@%p7 bra 	$L__BB39_2;

$L__BB39_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r47, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r47;
	@%p25 bra 	$L__BB39_11;
	bra.uni 	$L__BB39_5;

$L__BB39_11:
	setp.ge.u64 	%p13, %rd64, %rd26;
	@%p13 bra 	$L__BB39_26;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r28;
	@%p3 bra 	$L__BB39_22;

$L__BB39_13:
	mov.u32 	%r49, 0;
	mov.u32 	%r50, %r47;
	mov.u32 	%r51, %r49;

$L__BB39_14:
	not.b32 	%r31, %r49;
	cvt.u64.u32 	%rd46, %r31;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r50;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB39_16;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB39_17;

$L__BB39_16:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd66, %r34;
	cvt.u64.u32 	%rd67, %r36;

$L__BB39_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r37, %rd55;
	add.s32 	%r51, %r51, %r37;
	cvt.u32.u64 	%r50, %rd66;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd56, %r49;
	setp.lt.u64 	%p16, %rd56, %rd27;
	@%p16 bra 	$L__BB39_14;

	setp.eq.s64 	%p17, %rd28, 0;
	mul.wide.u32 	%rd57, %r51, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p17;
	ld.global.f32 	%f6, [%rd60];
	abs.f32 	%f7, %f6;
	setp.ltu.f32 	%p18, %f7, 0f3F19999A;
	@%p18 bra 	$L__BB39_20;
	bra.uni 	$L__BB39_19;

$L__BB39_20:
	mul.f32 	%f42, %f6, %f6;
	mov.f32 	%f43, 0fBD563CAE;
	mov.f32 	%f44, 0f3C80F082;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3E085941;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0fBEAAA9ED;
	fma.rn.f32 	%f49, %f47, %f42, %f48;
	mov.f32 	%f50, 0f00000000;
	fma.rn.f32 	%f51, %f49, %f42, %f50;
	fma.rn.f32 	%f71, %f51, %f6, %f6;
	bra.uni 	$L__BB39_21;

$L__BB39_19:
	mul.f32 	%f34, %f7, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f35, %f34;
	add.f32 	%f36, %f35, 0f3F800000;
	mov.f32 	%f37, 0f3F800000;
	rcp.approx.ftz.f32 	%f38, %f36;
	mov.f32 	%f39, 0fC0000000;
	fma.rn.f32 	%f40, %f38, %f39, %f37;
	setp.ge.f32 	%p19, %f7, 0f41102CB4;
	selp.f32 	%f41, 0f3F800000, %f40, %p19;
	mov.b32 	%r38, %f41;
	mov.b32 	%r39, %f6;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, %r38;
	mov.b32 	%f71, %r41;

$L__BB39_21:
	st.global.f32 	[%rd21], %f71;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p20, %rd64, %rd26;
	@%p20 bra 	$L__BB39_13;
	bra.uni 	$L__BB39_26;

$L__BB39_22:
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p21, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p21;
	ld.global.f32 	%f11, [%rd62];
	abs.f32 	%f12, %f11;
	setp.ltu.f32 	%p22, %f12, 0f3F19999A;
	@%p22 bra 	$L__BB39_24;
	bra.uni 	$L__BB39_23;

$L__BB39_24:
	mul.f32 	%f60, %f11, %f11;
	mov.f32 	%f61, 0fBD563CAE;
	mov.f32 	%f62, 0f3C80F082;
	fma.rn.f32 	%f63, %f62, %f60, %f61;
	mov.f32 	%f64, 0f3E085941;
	fma.rn.f32 	%f65, %f63, %f60, %f64;
	mov.f32 	%f66, 0fBEAAA9ED;
	fma.rn.f32 	%f67, %f65, %f60, %f66;
	mov.f32 	%f68, 0f00000000;
	fma.rn.f32 	%f69, %f67, %f60, %f68;
	fma.rn.f32 	%f72, %f69, %f11, %f11;
	bra.uni 	$L__BB39_25;

$L__BB39_23:
	mul.f32 	%f52, %f12, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	add.f32 	%f54, %f53, 0f3F800000;
	mov.f32 	%f55, 0f3F800000;
	rcp.approx.ftz.f32 	%f56, %f54;
	mov.f32 	%f57, 0fC0000000;
	fma.rn.f32 	%f58, %f56, %f57, %f55;
	setp.ge.f32 	%p23, %f12, 0f41102CB4;
	selp.f32 	%f59, 0f3F800000, %f58, %p23;
	mov.b32 	%r42, %f59;
	mov.b32 	%r43, %f11;
	and.b32  	%r44, %r43, -2147483648;
	or.b32  	%r45, %r44, %r42;
	mov.b32 	%f72, %r45;

$L__BB39_25:
	st.global.f32 	[%rd24], %f72;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB39_22;
	bra.uni 	$L__BB39_26;

$L__BB39_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB39_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB39_7:
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	abs.f32 	%f2, %f1;
	setp.ltu.f32 	%p10, %f2, 0f3F19999A;
	@%p10 bra 	$L__BB39_9;
	bra.uni 	$L__BB39_8;

$L__BB39_9:
	mul.f32 	%f24, %f1, %f1;
	mov.f32 	%f25, 0fBD563CAE;
	mov.f32 	%f26, 0f3C80F082;
	fma.rn.f32 	%f27, %f26, %f24, %f25;
	mov.f32 	%f28, 0f3E085941;
	fma.rn.f32 	%f29, %f27, %f24, %f28;
	mov.f32 	%f30, 0fBEAAA9ED;
	fma.rn.f32 	%f31, %f29, %f24, %f30;
	mov.f32 	%f32, 0f00000000;
	fma.rn.f32 	%f33, %f31, %f24, %f32;
	fma.rn.f32 	%f70, %f33, %f1, %f1;
	bra.uni 	$L__BB39_10;

$L__BB39_8:
	mul.f32 	%f16, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f17, %f16;
	add.f32 	%f18, %f17, 0f3F800000;
	mov.f32 	%f19, 0f3F800000;
	rcp.approx.ftz.f32 	%f20, %f18;
	mov.f32 	%f21, 0fC0000000;
	fma.rn.f32 	%f22, %f20, %f21, %f19;
	setp.ge.f32 	%p11, %f2, 0f41102CB4;
	selp.f32 	%f23, 0f3F800000, %f22, %p11;
	mov.b32 	%r24, %f23;
	mov.b32 	%r25, %f1;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, %r24;
	mov.b32 	%f70, %r27;

$L__BB39_10:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f70;
	add.s32 	%r47, %r47, %r5;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p12, %rd64, %rd26;
	@%p12 bra 	$L__BB39_7;

$L__BB39_26:
	ret;

}
	// .globl	utanh_f64
.visible .entry utanh_f64(
	.param .u64 utanh_f64_param_0,
	.param .u64 utanh_f64_param_1,
	.param .u64 utanh_f64_param_2,
	.param .u64 utanh_f64_param_3,
	.param .u64 utanh_f64_param_4
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<214>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [utanh_f64_param_0];
	ld.param.u64 	%rd27, [utanh_f64_param_1];
	ld.param.u64 	%rd29, [utanh_f64_param_2];
	ld.param.u64 	%rd28, [utanh_f64_param_3];
	ld.param.u64 	%rd30, [utanh_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p25, %p2;
	@%p3 bra 	$L__BB40_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r55, 0;

$L__BB40_2:
	not.b32 	%r26, %r55;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p25, -1;
	@%p5 bra 	$L__BB40_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r55, %r55, 1;
	cvt.u64.u32 	%rd41, %r55;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p25, %p2;
	@%p7 bra 	$L__BB40_2;

$L__BB40_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %tid.x;
	mad.lo.s32 	%r56, %r27, %r3, %r28;
	cvt.u64.u32 	%rd64, %r56;
	@%p25 bra 	$L__BB40_11;
	bra.uni 	$L__BB40_5;

$L__BB40_11:
	setp.ge.u64 	%p13, %rd64, %rd26;
	@%p13 bra 	$L__BB40_26;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r35;
	@%p3 bra 	$L__BB40_22;

$L__BB40_13:
	mov.u32 	%r58, 0;
	mov.u32 	%r59, %r56;
	mov.u32 	%r60, %r58;

$L__BB40_14:
	not.b32 	%r38, %r58;
	cvt.u64.u32 	%rd46, %r38;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r59;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB40_16;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB40_17;

$L__BB40_16:
	cvt.u32.u64 	%r39, %rd14;
	cvt.u32.u64 	%r40, %rd12;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd66, %r41;
	cvt.u64.u32 	%rd67, %r43;

$L__BB40_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r44, %rd55;
	add.s32 	%r60, %r60, %r44;
	cvt.u32.u64 	%r59, %rd66;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd56, %r58;
	setp.lt.u64 	%p16, %rd56, %rd27;
	@%p16 bra 	$L__BB40_14;

	setp.eq.s64 	%p17, %rd28, 0;
	mul.wide.u32 	%rd57, %r60, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p17;
	ld.global.f64 	%fd6, [%rd60];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd6;
	}
	and.b32  	%r19, %r18, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd6;
	}
	mov.b64 	%fd7, {%r45, %r19};
	setp.ltu.f64 	%p18, %fd7, 0d3FE4F92224DD2F1A;
	@%p18 bra 	$L__BB40_20;
	bra.uni 	$L__BB40_19;

$L__BB40_20:
	mul.f64 	%fd122, %fd6, %fd6;
	mov.f64 	%fd123, 0d3F14359F420AFC3D;
	mov.f64 	%fd124, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd125, %fd124, %fd122, %fd123;
	mov.f64 	%fd126, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd127, %fd125, %fd122, %fd126;
	mov.f64 	%fd128, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd129, %fd127, %fd122, %fd128;
	mov.f64 	%fd130, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd131, %fd129, %fd122, %fd130;
	mov.f64 	%fd132, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd133, %fd131, %fd122, %fd132;
	mov.f64 	%fd134, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd135, %fd133, %fd122, %fd134;
	mov.f64 	%fd136, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd137, %fd135, %fd122, %fd136;
	mov.f64 	%fd138, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd139, %fd137, %fd122, %fd138;
	mov.f64 	%fd140, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd141, %fd139, %fd122, %fd140;
	mov.f64 	%fd142, 0dBFD5555555555550;
	fma.rn.f64 	%fd143, %fd141, %fd122, %fd142;
	mov.f64 	%fd144, 0d0000000000000000;
	fma.rn.f64 	%fd145, %fd143, %fd122, %fd144;
	fma.rn.f64 	%fd212, %fd145, %fd6, %fd6;
	bra.uni 	$L__BB40_21;

$L__BB40_19:
	add.f64 	%fd81, %fd7, %fd7;
	mov.f64 	%fd82, 0d4000000000000000;
	cvt.rn.f32.f64 	%f5, %fd81;
	mul.f32 	%f6, %f5, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f7, %f6;
	cvt.f64.f32 	%fd83, %f7;
	neg.f64 	%fd84, %fd83;
	mov.f64 	%fd85, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd86, %fd84, %fd85, %fd81;
	mov.f64 	%fd87, 0d3E928A27F89B6999;
	mov.f64 	%fd88, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd89, %fd88, %fd86, %fd87;
	mov.f64 	%fd90, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd91, %fd89, %fd86, %fd90;
	mov.f64 	%fd92, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd93, %fd91, %fd86, %fd92;
	mov.f64 	%fd94, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd95, %fd93, %fd86, %fd94;
	mov.f64 	%fd96, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd97, %fd95, %fd86, %fd96;
	mov.f64 	%fd98, 0d3F811111111173C4;
	fma.rn.f64 	%fd99, %fd97, %fd86, %fd98;
	mov.f64 	%fd100, 0d3FA555555555211A;
	fma.rn.f64 	%fd101, %fd99, %fd86, %fd100;
	mov.f64 	%fd102, 0d3FC5555555555540;
	fma.rn.f64 	%fd103, %fd101, %fd86, %fd102;
	mov.f64 	%fd104, 0d3FE0000000000005;
	fma.rn.f64 	%fd105, %fd103, %fd86, %fd104;
	mul.f64 	%fd106, %fd86, %fd105;
	fma.rn.f64 	%fd107, %fd106, %fd86, %fd86;
	ex2.approx.ftz.f32 	%f8, %f7;
	cvt.f64.f32 	%fd108, %f8;
	mov.f64 	%fd109, 0d3FF0000000000000;
	sub.f64 	%fd110, %fd109, %fd108;
	neg.f64 	%fd111, %fd107;
	fma.rn.f64 	%fd112, %fd111, %fd108, %fd110;
	sub.f64 	%fd113, %fd82, %fd112;
	rcp.approx.ftz.f64 	%fd114, %fd113;
	neg.f64 	%fd115, %fd113;
	fma.rn.f64 	%fd116, %fd115, %fd114, %fd109;
	fma.rn.f64 	%fd117, %fd116, %fd116, %fd116;
	fma.rn.f64 	%fd118, %fd117, %fd114, %fd114;
	neg.f64 	%fd119, %fd118;
	fma.rn.f64 	%fd120, %fd82, %fd119, %fd109;
	setp.gt.u32 	%p19, %r19, 1077088193;
	selp.f64 	%fd121, 0d3FF0000000000000, %fd120, %p19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd121;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd121;
	}
	and.b32  	%r48, %r18, -2147483648;
	or.b32  	%r49, %r47, %r48;
	mov.b64 	%fd212, {%r46, %r49};

$L__BB40_21:
	st.global.f64 	[%rd21], %fd212;
	add.s32 	%r56, %r56, %r10;
	cvt.u64.u32 	%rd64, %r56;
	setp.lt.u64 	%p20, %rd64, %rd26;
	@%p20 bra 	$L__BB40_13;
	bra.uni 	$L__BB40_26;

$L__BB40_22:
	shl.b64 	%rd61, %rd64, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p21, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p21;
	ld.global.f64 	%fd11, [%rd62];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd11;
	}
	and.b32  	%r23, %r22, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd11;
	}
	mov.b64 	%fd12, {%r50, %r23};
	setp.ltu.f64 	%p22, %fd12, 0d3FE4F92224DD2F1A;
	@%p22 bra 	$L__BB40_24;
	bra.uni 	$L__BB40_23;

$L__BB40_24:
	mul.f64 	%fd187, %fd11, %fd11;
	mov.f64 	%fd188, 0d3F14359F420AFC3D;
	mov.f64 	%fd189, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd190, %fd189, %fd187, %fd188;
	mov.f64 	%fd191, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd192, %fd190, %fd187, %fd191;
	mov.f64 	%fd193, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd194, %fd192, %fd187, %fd193;
	mov.f64 	%fd195, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd196, %fd194, %fd187, %fd195;
	mov.f64 	%fd197, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd198, %fd196, %fd187, %fd197;
	mov.f64 	%fd199, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd200, %fd198, %fd187, %fd199;
	mov.f64 	%fd201, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd202, %fd200, %fd187, %fd201;
	mov.f64 	%fd203, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd204, %fd202, %fd187, %fd203;
	mov.f64 	%fd205, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd206, %fd204, %fd187, %fd205;
	mov.f64 	%fd207, 0dBFD5555555555550;
	fma.rn.f64 	%fd208, %fd206, %fd187, %fd207;
	mov.f64 	%fd209, 0d0000000000000000;
	fma.rn.f64 	%fd210, %fd208, %fd187, %fd209;
	fma.rn.f64 	%fd213, %fd210, %fd11, %fd11;
	bra.uni 	$L__BB40_25;

$L__BB40_23:
	add.f64 	%fd146, %fd12, %fd12;
	mov.f64 	%fd147, 0d4000000000000000;
	cvt.rn.f32.f64 	%f9, %fd146;
	mul.f32 	%f10, %f9, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f11, %f10;
	cvt.f64.f32 	%fd148, %f11;
	neg.f64 	%fd149, %fd148;
	mov.f64 	%fd150, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd151, %fd149, %fd150, %fd146;
	mov.f64 	%fd152, 0d3E928A27F89B6999;
	mov.f64 	%fd153, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd154, %fd153, %fd151, %fd152;
	mov.f64 	%fd155, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd156, %fd154, %fd151, %fd155;
	mov.f64 	%fd157, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd158, %fd156, %fd151, %fd157;
	mov.f64 	%fd159, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd160, %fd158, %fd151, %fd159;
	mov.f64 	%fd161, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd162, %fd160, %fd151, %fd161;
	mov.f64 	%fd163, 0d3F811111111173C4;
	fma.rn.f64 	%fd164, %fd162, %fd151, %fd163;
	mov.f64 	%fd165, 0d3FA555555555211A;
	fma.rn.f64 	%fd166, %fd164, %fd151, %fd165;
	mov.f64 	%fd167, 0d3FC5555555555540;
	fma.rn.f64 	%fd168, %fd166, %fd151, %fd167;
	mov.f64 	%fd169, 0d3FE0000000000005;
	fma.rn.f64 	%fd170, %fd168, %fd151, %fd169;
	mul.f64 	%fd171, %fd151, %fd170;
	fma.rn.f64 	%fd172, %fd171, %fd151, %fd151;
	ex2.approx.ftz.f32 	%f12, %f11;
	cvt.f64.f32 	%fd173, %f12;
	mov.f64 	%fd174, 0d3FF0000000000000;
	sub.f64 	%fd175, %fd174, %fd173;
	neg.f64 	%fd176, %fd172;
	fma.rn.f64 	%fd177, %fd176, %fd173, %fd175;
	sub.f64 	%fd178, %fd147, %fd177;
	rcp.approx.ftz.f64 	%fd179, %fd178;
	neg.f64 	%fd180, %fd178;
	fma.rn.f64 	%fd181, %fd180, %fd179, %fd174;
	fma.rn.f64 	%fd182, %fd181, %fd181, %fd181;
	fma.rn.f64 	%fd183, %fd182, %fd179, %fd179;
	neg.f64 	%fd184, %fd183;
	fma.rn.f64 	%fd185, %fd147, %fd184, %fd174;
	setp.gt.u32 	%p23, %r23, 1077088193;
	selp.f64 	%fd186, 0d3FF0000000000000, %fd185, %p23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd186;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd186;
	}
	and.b32  	%r53, %r22, -2147483648;
	or.b32  	%r54, %r52, %r53;
	mov.b64 	%fd213, {%r51, %r54};

$L__BB40_25:
	st.global.f64 	[%rd24], %fd213;
	add.s32 	%r56, %r56, %r10;
	cvt.u64.u32 	%rd64, %r56;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB40_22;
	bra.uni 	$L__BB40_26;

$L__BB40_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB40_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r29;

$L__BB40_7:
	shl.b64 	%rd42, %rd64, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd1;
	}
	and.b32  	%r8, %r7, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r30, %r8};
	setp.ltu.f64 	%p10, %fd2, 0d3FE4F92224DD2F1A;
	@%p10 bra 	$L__BB40_9;
	bra.uni 	$L__BB40_8;

$L__BB40_9:
	mul.f64 	%fd57, %fd1, %fd1;
	mov.f64 	%fd58, 0d3F14359F420AFC3D;
	mov.f64 	%fd59, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd60, %fd59, %fd57, %fd58;
	mov.f64 	%fd61, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd62, %fd60, %fd57, %fd61;
	mov.f64 	%fd63, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd64, %fd62, %fd57, %fd63;
	mov.f64 	%fd65, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd66, %fd64, %fd57, %fd65;
	mov.f64 	%fd67, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd68, %fd66, %fd57, %fd67;
	mov.f64 	%fd69, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd70, %fd68, %fd57, %fd69;
	mov.f64 	%fd71, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd72, %fd70, %fd57, %fd71;
	mov.f64 	%fd73, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd74, %fd72, %fd57, %fd73;
	mov.f64 	%fd75, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd76, %fd74, %fd57, %fd75;
	mov.f64 	%fd77, 0dBFD5555555555550;
	fma.rn.f64 	%fd78, %fd76, %fd57, %fd77;
	mov.f64 	%fd79, 0d0000000000000000;
	fma.rn.f64 	%fd80, %fd78, %fd57, %fd79;
	fma.rn.f64 	%fd211, %fd80, %fd1, %fd1;
	bra.uni 	$L__BB40_10;

$L__BB40_8:
	add.f64 	%fd16, %fd2, %fd2;
	mov.f64 	%fd17, 0d4000000000000000;
	cvt.rn.f32.f64 	%f1, %fd16;
	mul.f32 	%f2, %f1, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f3, %f2;
	cvt.f64.f32 	%fd18, %f3;
	neg.f64 	%fd19, %fd18;
	mov.f64 	%fd20, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd21, %fd19, %fd20, %fd16;
	mov.f64 	%fd22, 0d3E928A27F89B6999;
	mov.f64 	%fd23, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd24, %fd23, %fd21, %fd22;
	mov.f64 	%fd25, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd26, %fd24, %fd21, %fd25;
	mov.f64 	%fd27, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd28, %fd26, %fd21, %fd27;
	mov.f64 	%fd29, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd30, %fd28, %fd21, %fd29;
	mov.f64 	%fd31, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd32, %fd30, %fd21, %fd31;
	mov.f64 	%fd33, 0d3F811111111173C4;
	fma.rn.f64 	%fd34, %fd32, %fd21, %fd33;
	mov.f64 	%fd35, 0d3FA555555555211A;
	fma.rn.f64 	%fd36, %fd34, %fd21, %fd35;
	mov.f64 	%fd37, 0d3FC5555555555540;
	fma.rn.f64 	%fd38, %fd36, %fd21, %fd37;
	mov.f64 	%fd39, 0d3FE0000000000005;
	fma.rn.f64 	%fd40, %fd38, %fd21, %fd39;
	mul.f64 	%fd41, %fd21, %fd40;
	fma.rn.f64 	%fd42, %fd41, %fd21, %fd21;
	ex2.approx.ftz.f32 	%f4, %f3;
	cvt.f64.f32 	%fd43, %f4;
	mov.f64 	%fd44, 0d3FF0000000000000;
	sub.f64 	%fd45, %fd44, %fd43;
	neg.f64 	%fd46, %fd42;
	fma.rn.f64 	%fd47, %fd46, %fd43, %fd45;
	sub.f64 	%fd48, %fd17, %fd47;
	rcp.approx.ftz.f64 	%fd49, %fd48;
	neg.f64 	%fd50, %fd48;
	fma.rn.f64 	%fd51, %fd50, %fd49, %fd44;
	fma.rn.f64 	%fd52, %fd51, %fd51, %fd51;
	fma.rn.f64 	%fd53, %fd52, %fd49, %fd49;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd17, %fd54, %fd44;
	setp.gt.u32 	%p11, %r8, 1077088193;
	selp.f64 	%fd56, 0d3FF0000000000000, %fd55, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd56;
	}
	and.b32  	%r33, %r7, -2147483648;
	or.b32  	%r34, %r32, %r33;
	mov.b64 	%fd211, {%r31, %r34};

$L__BB40_10:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd211;
	add.s32 	%r56, %r56, %r5;
	cvt.u64.u32 	%rd64, %r56;
	setp.lt.u64 	%p12, %rd64, %rd26;
	@%p12 bra 	$L__BB40_7;

$L__BB40_26:
	ret;

}
	// .globl	uerf_f32
.visible .entry uerf_f32(
	.param .u64 uerf_f32_param_0,
	.param .u64 uerf_f32_param_1,
	.param .u64 uerf_f32_param_2,
	.param .u64 uerf_f32_param_3,
	.param .u64 uerf_f32_param_4
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<79>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [uerf_f32_param_0];
	ld.param.u64 	%rd27, [uerf_f32_param_1];
	ld.param.u64 	%rd29, [uerf_f32_param_2];
	ld.param.u64 	%rd28, [uerf_f32_param_3];
	ld.param.u64 	%rd30, [uerf_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p25, %p2;
	@%p3 bra 	$L__BB41_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r46, 0;

$L__BB41_2:
	not.b32 	%r20, %r46;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p25, -1;
	@%p5 bra 	$L__BB41_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd41, %r46;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p25, %p2;
	@%p7 bra 	$L__BB41_2;

$L__BB41_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r47, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r47;
	@%p25 bra 	$L__BB41_10;
	bra.uni 	$L__BB41_5;

$L__BB41_10:
	setp.ge.u64 	%p13, %rd64, %rd26;
	@%p13 bra 	$L__BB41_23;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r28;
	@%p3 bra 	$L__BB41_20;

$L__BB41_12:
	mov.u32 	%r49, 0;
	mov.u32 	%r50, %r47;
	mov.u32 	%r51, %r49;

$L__BB41_13:
	not.b32 	%r31, %r49;
	cvt.u64.u32 	%rd46, %r31;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r50;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB41_15;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB41_16;

$L__BB41_15:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd66, %r34;
	cvt.u64.u32 	%rd67, %r36;

$L__BB41_16:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r37, %rd55;
	add.s32 	%r51, %r51, %r37;
	cvt.u32.u64 	%r50, %rd66;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd56, %r49;
	setp.lt.u64 	%p16, %rd56, %rd27;
	@%p16 bra 	$L__BB41_13;

	setp.eq.s64 	%p17, %rd28, 0;
	mul.wide.u32 	%rd57, %r51, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p17;
	ld.global.f32 	%f5, [%rd60];
	abs.f32 	%f34, %f5;
	setp.ltu.f32 	%p18, %f34, 0f3F8060FE;
	setp.ge.f32 	%p19, %f34, 0f3F8060FE;
	mul.f32 	%f35, %f5, %f5;
	selp.f32 	%f36, %f34, %f35, %p19;
	selp.f32 	%f37, 0f38EB4C3A, 0f38B1E96A, %p19;
	selp.f32 	%f38, 0fBAAE005B, 0fBA574D20, %p19;
	fma.rn.f32 	%f39, %f37, %f36, %f38;
	selp.f32 	%f40, 0f3C09919F, 0f3BAAD5EA, %p19;
	fma.rn.f32 	%f41, %f39, %f36, %f40;
	selp.f32 	%f42, 0fBD24D99A, 0fBCDC1BE7, %p19;
	fma.rn.f32 	%f43, %f41, %f36, %f42;
	selp.f32 	%f44, 0f3E235519, 0f3DE718AF, %p19;
	fma.rn.f32 	%f45, %f43, %f36, %f44;
	selp.f32 	%f46, 0f3F69B4F9, 0fBEC093AC, %p19;
	fma.rn.f32 	%f47, %f45, %f36, %f46;
	selp.f32 	%f48, 0f3F210A14, 0f3E0375D3, %p19;
	fma.rn.f32 	%f49, %f47, %f36, %f48;
	neg.f32 	%f50, %f34;
	selp.f32 	%f51, %f50, %f5, %p19;
	fma.rn.f32 	%f77, %f49, %f51, %f51;
	@%p18 bra 	$L__BB41_19;

	ex2.approx.ftz.f32 	%f52, %f77;
	mov.f32 	%f53, 0f3F800000;
	sub.f32 	%f54, %f53, %f52;
	mov.b32 	%r38, %f54;
	mov.b32 	%r39, %f5;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, %r38;
	mov.b32 	%f77, %r41;

$L__BB41_19:
	st.global.f32 	[%rd21], %f77;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p20, %rd64, %rd26;
	@%p20 bra 	$L__BB41_12;
	bra.uni 	$L__BB41_23;

$L__BB41_20:
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p21, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p21;
	ld.global.f32 	%f9, [%rd62];
	abs.f32 	%f55, %f9;
	setp.ltu.f32 	%p22, %f55, 0f3F8060FE;
	setp.ge.f32 	%p23, %f55, 0f3F8060FE;
	mul.f32 	%f56, %f9, %f9;
	selp.f32 	%f57, %f55, %f56, %p23;
	selp.f32 	%f58, 0f38EB4C3A, 0f38B1E96A, %p23;
	selp.f32 	%f59, 0fBAAE005B, 0fBA574D20, %p23;
	fma.rn.f32 	%f60, %f58, %f57, %f59;
	selp.f32 	%f61, 0f3C09919F, 0f3BAAD5EA, %p23;
	fma.rn.f32 	%f62, %f60, %f57, %f61;
	selp.f32 	%f63, 0fBD24D99A, 0fBCDC1BE7, %p23;
	fma.rn.f32 	%f64, %f62, %f57, %f63;
	selp.f32 	%f65, 0f3E235519, 0f3DE718AF, %p23;
	fma.rn.f32 	%f66, %f64, %f57, %f65;
	selp.f32 	%f67, 0f3F69B4F9, 0fBEC093AC, %p23;
	fma.rn.f32 	%f68, %f66, %f57, %f67;
	selp.f32 	%f69, 0f3F210A14, 0f3E0375D3, %p23;
	fma.rn.f32 	%f70, %f68, %f57, %f69;
	neg.f32 	%f71, %f55;
	selp.f32 	%f72, %f71, %f9, %p23;
	fma.rn.f32 	%f78, %f70, %f72, %f72;
	@%p22 bra 	$L__BB41_22;

	ex2.approx.ftz.f32 	%f73, %f78;
	mov.f32 	%f74, 0f3F800000;
	sub.f32 	%f75, %f74, %f73;
	mov.b32 	%r42, %f75;
	mov.b32 	%r43, %f9;
	and.b32  	%r44, %r43, -2147483648;
	or.b32  	%r45, %r44, %r42;
	mov.b32 	%f78, %r45;

$L__BB41_22:
	st.global.f32 	[%rd24], %f78;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB41_20;
	bra.uni 	$L__BB41_23;

$L__BB41_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB41_23;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB41_7:
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	abs.f32 	%f13, %f1;
	setp.ltu.f32 	%p10, %f13, 0f3F8060FE;
	setp.ge.f32 	%p11, %f13, 0f3F8060FE;
	mul.f32 	%f14, %f1, %f1;
	selp.f32 	%f15, %f13, %f14, %p11;
	selp.f32 	%f16, 0f38EB4C3A, 0f38B1E96A, %p11;
	selp.f32 	%f17, 0fBAAE005B, 0fBA574D20, %p11;
	fma.rn.f32 	%f18, %f16, %f15, %f17;
	selp.f32 	%f19, 0f3C09919F, 0f3BAAD5EA, %p11;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	selp.f32 	%f21, 0fBD24D99A, 0fBCDC1BE7, %p11;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	selp.f32 	%f23, 0f3E235519, 0f3DE718AF, %p11;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	selp.f32 	%f25, 0f3F69B4F9, 0fBEC093AC, %p11;
	fma.rn.f32 	%f26, %f24, %f15, %f25;
	selp.f32 	%f27, 0f3F210A14, 0f3E0375D3, %p11;
	fma.rn.f32 	%f28, %f26, %f15, %f27;
	neg.f32 	%f29, %f13;
	selp.f32 	%f30, %f29, %f1, %p11;
	fma.rn.f32 	%f76, %f28, %f30, %f30;
	@%p10 bra 	$L__BB41_9;

	ex2.approx.ftz.f32 	%f31, %f76;
	mov.f32 	%f32, 0f3F800000;
	sub.f32 	%f33, %f32, %f31;
	mov.b32 	%r24, %f33;
	mov.b32 	%r25, %f1;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, %r24;
	mov.b32 	%f76, %r27;

$L__BB41_9:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f76;
	add.s32 	%r47, %r47, %r5;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p12, %rd64, %rd26;
	@%p12 bra 	$L__BB41_7;

$L__BB41_23:
	ret;

}
	// .globl	uerf_f64
.visible .entry uerf_f64(
	.param .u64 uerf_f64_param_0,
	.param .u64 uerf_f64_param_1,
	.param .u64 uerf_f64_param_2,
	.param .u64 uerf_f64_param_3,
	.param .u64 uerf_f64_param_4
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<56>;
	.reg .f64 	%fd<262>;
	.reg .b64 	%rd<73>;


	ld.param.u64 	%rd24, [uerf_f64_param_0];
	ld.param.u64 	%rd25, [uerf_f64_param_1];
	ld.param.u64 	%rd27, [uerf_f64_param_2];
	ld.param.u64 	%rd26, [uerf_f64_param_3];
	ld.param.u64 	%rd28, [uerf_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p22, %p2;
	@%p3 bra 	$L__BB42_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r49, 0;

$L__BB42_2:
	not.b32 	%r20, %r49;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd67, %rd35;
	mov.pred 	%p22, -1;
	@%p5 bra 	$L__BB42_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd67, %rd38, %rd67;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd39, %r49;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p22, %p2;
	@%p7 bra 	$L__BB42_2;

$L__BB42_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r50, %r21, %r3, %r22;
	cvt.u64.u32 	%rd68, %r50;
	@%p22 bra 	$L__BB42_8;
	bra.uni 	$L__BB42_5;

$L__BB42_8:
	setp.ge.u64 	%p12, %rd68, %rd24;
	@%p12 bra 	$L__BB42_17;

	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r29;
	@%p3 bra 	$L__BB42_16;

$L__BB42_10:
	mov.u32 	%r52, 0;
	mov.u32 	%r53, %r50;
	mov.u32 	%r54, %r52;

$L__BB42_11:
	not.b32 	%r32, %r52;
	cvt.u64.u32 	%rd43, %r32;
	add.s64 	%rd44, %rd43, %rd25;
	cvt.u64.u32 	%rd12, %r53;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p14, %rd47, 0;
	@%p14 bra 	$L__BB42_13;

	div.u64 	%rd70, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd70, %rd14;
	sub.s64 	%rd71, %rd12, %rd48;
	bra.uni 	$L__BB42_14;

$L__BB42_13:
	cvt.u32.u64 	%r33, %rd14;
	cvt.u32.u64 	%r34, %rd12;
	div.u32 	%r35, %r34, %r33;
	mul.lo.s32 	%r36, %r35, %r33;
	sub.s32 	%r37, %r34, %r36;
	cvt.u64.u32 	%rd70, %r35;
	cvt.u64.u32 	%rd71, %r37;

$L__BB42_14:
	shl.b64 	%rd49, %rd25, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd71;
	cvt.u32.u64 	%r38, %rd52;
	add.s32 	%r54, %r54, %r38;
	cvt.u32.u64 	%r53, %rd70;
	add.s32 	%r52, %r52, 1;
	cvt.u64.u32 	%rd53, %r52;
	setp.lt.u64 	%p15, %rd53, %rd25;
	@%p15 bra 	$L__BB42_11;

	ld.param.u64 	%rd63, [uerf_f64_param_3];
	ld.param.u64 	%rd62, [uerf_f64_param_0];
	setp.eq.s64 	%p16, %rd63, 0;
	mul.wide.u32 	%rd54, %r54, 8;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd68, 3;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p16;
	ld.global.f64 	%fd88, [%rd58];
	abs.f64 	%fd89, %fd88;
	mov.f64 	%fd90, 0d3D47088FDB46FA5F;
	mov.f64 	%fd91, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd92, %fd91, %fd89, %fd90;
	mov.f64 	%fd93, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd94, %fd92, %fd89, %fd93;
	mov.f64 	%fd95, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd96, %fd94, %fd89, %fd95;
	mov.f64 	%fd97, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd98, %fd96, %fd89, %fd97;
	mov.f64 	%fd99, 0d3E2485EE67835925;
	fma.rn.f64 	%fd100, %fd98, %fd89, %fd99;
	mov.f64 	%fd101, 0dBE476DB45919F583;
	fma.rn.f64 	%fd102, %fd100, %fd89, %fd101;
	mov.f64 	%fd103, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd104, %fd102, %fd89, %fd103;
	mov.f64 	%fd105, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd106, %fd104, %fd89, %fd105;
	mov.f64 	%fd107, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd108, %fd106, %fd89, %fd107;
	mov.f64 	%fd109, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd110, %fd108, %fd89, %fd109;
	mov.f64 	%fd111, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd112, %fd110, %fd89, %fd111;
	mov.f64 	%fd113, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd114, %fd112, %fd89, %fd113;
	mov.f64 	%fd115, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd116, %fd114, %fd89, %fd115;
	mov.f64 	%fd117, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd118, %fd116, %fd89, %fd117;
	mov.f64 	%fd119, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd120, %fd118, %fd89, %fd119;
	mov.f64 	%fd121, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd122, %fd120, %fd89, %fd121;
	mov.f64 	%fd123, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd124, %fd122, %fd89, %fd123;
	mov.f64 	%fd125, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd126, %fd124, %fd89, %fd125;
	mov.f64 	%fd127, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd128, %fd126, %fd89, %fd127;
	mov.f64 	%fd129, 0dBF9396685912A453;
	fma.rn.f64 	%fd130, %fd128, %fd89, %fd129;
	mov.f64 	%fd131, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd132, %fd130, %fd89, %fd131;
	mov.f64 	%fd133, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd134, %fd132, %fd89, %fd133;
	mov.f64 	%fd135, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd136, %fd134, %fd89, %fd135;
	fma.rn.f64 	%fd137, %fd136, %fd89, %fd89;
	sub.f64 	%fd138, %fd89, %fd137;
	fma.rn.f64 	%fd139, %fd136, %fd89, %fd138;
	neg.f64 	%fd140, %fd137;
	neg.f64 	%fd141, %fd139;
	cvt.rn.f32.f64 	%f5, %fd137;
	mul.f32 	%f6, %f5, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f7, %f6;
	cvt.f64.f32 	%fd142, %f7;
	neg.f64 	%fd143, %fd142;
	mov.f64 	%fd144, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd145, %fd143, %fd144, %fd140;
	mov.f64 	%fd146, 0d3E928A27F89B6999;
	mov.f64 	%fd147, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	mov.f64 	%fd149, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd150, %fd148, %fd145, %fd149;
	mov.f64 	%fd151, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd152, %fd150, %fd145, %fd151;
	mov.f64 	%fd153, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd154, %fd152, %fd145, %fd153;
	mov.f64 	%fd155, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd156, %fd154, %fd145, %fd155;
	mov.f64 	%fd157, 0d3F811111111173C4;
	fma.rn.f64 	%fd158, %fd156, %fd145, %fd157;
	mov.f64 	%fd159, 0d3FA555555555211A;
	fma.rn.f64 	%fd160, %fd158, %fd145, %fd159;
	mov.f64 	%fd161, 0d3FC5555555555540;
	fma.rn.f64 	%fd162, %fd160, %fd145, %fd161;
	mov.f64 	%fd163, 0d3FE0000000000005;
	fma.rn.f64 	%fd164, %fd162, %fd145, %fd163;
	mul.f64 	%fd165, %fd145, %fd164;
	fma.rn.f64 	%fd166, %fd165, %fd145, %fd141;
	add.f64 	%fd167, %fd145, %fd166;
	ex2.approx.ftz.f32 	%f8, %f7;
	cvt.f64.f32 	%fd168, %f8;
	mov.f64 	%fd169, 0d3FF0000000000000;
	sub.f64 	%fd170, %fd169, %fd168;
	neg.f64 	%fd171, %fd167;
	fma.rn.f64 	%fd172, %fd171, %fd168, %fd170;
	setp.ge.f64 	%p17, %fd89, 0d4017AFB48DC96626;
	selp.f64 	%fd173, 0d3FF0000000000000, %fd172, %p17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd173;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd173;
	}
	and.b32  	%r42, %r39, -2147483648;
	or.b32  	%r43, %r41, %r42;
	mov.b64 	%fd174, {%r40, %r43};
	st.global.f64 	[%rd57], %fd174;
	add.s32 	%r50, %r50, %r8;
	cvt.u64.u32 	%rd68, %r50;
	setp.lt.u64 	%p18, %rd68, %rd62;
	@%p18 bra 	$L__BB42_10;
	bra.uni 	$L__BB42_17;

$L__BB42_16:
	ld.param.u64 	%rd66, [uerf_f64_param_3];
	ld.param.u64 	%rd65, [uerf_f64_param_0];
	shl.b64 	%rd59, %rd68, 3;
	add.s64 	%rd60, %rd1, %rd59;
	setp.eq.s64 	%p19, %rd66, 0;
	selp.b64 	%rd61, %rd60, %rd2, %p19;
	ld.global.f64 	%fd175, [%rd61];
	abs.f64 	%fd176, %fd175;
	mov.f64 	%fd177, 0d3D47088FDB46FA5F;
	mov.f64 	%fd178, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd179, %fd178, %fd176, %fd177;
	mov.f64 	%fd180, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd181, %fd179, %fd176, %fd180;
	mov.f64 	%fd182, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd183, %fd181, %fd176, %fd182;
	mov.f64 	%fd184, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd185, %fd183, %fd176, %fd184;
	mov.f64 	%fd186, 0d3E2485EE67835925;
	fma.rn.f64 	%fd187, %fd185, %fd176, %fd186;
	mov.f64 	%fd188, 0dBE476DB45919F583;
	fma.rn.f64 	%fd189, %fd187, %fd176, %fd188;
	mov.f64 	%fd190, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd191, %fd189, %fd176, %fd190;
	mov.f64 	%fd192, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd193, %fd191, %fd176, %fd192;
	mov.f64 	%fd194, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd195, %fd193, %fd176, %fd194;
	mov.f64 	%fd196, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd197, %fd195, %fd176, %fd196;
	mov.f64 	%fd198, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd199, %fd197, %fd176, %fd198;
	mov.f64 	%fd200, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd201, %fd199, %fd176, %fd200;
	mov.f64 	%fd202, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd203, %fd201, %fd176, %fd202;
	mov.f64 	%fd204, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd205, %fd203, %fd176, %fd204;
	mov.f64 	%fd206, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd207, %fd205, %fd176, %fd206;
	mov.f64 	%fd208, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd209, %fd207, %fd176, %fd208;
	mov.f64 	%fd210, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd211, %fd209, %fd176, %fd210;
	mov.f64 	%fd212, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd213, %fd211, %fd176, %fd212;
	mov.f64 	%fd214, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd215, %fd213, %fd176, %fd214;
	mov.f64 	%fd216, 0dBF9396685912A453;
	fma.rn.f64 	%fd217, %fd215, %fd176, %fd216;
	mov.f64 	%fd218, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd219, %fd217, %fd176, %fd218;
	mov.f64 	%fd220, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd221, %fd219, %fd176, %fd220;
	mov.f64 	%fd222, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd223, %fd221, %fd176, %fd222;
	fma.rn.f64 	%fd224, %fd223, %fd176, %fd176;
	sub.f64 	%fd225, %fd176, %fd224;
	fma.rn.f64 	%fd226, %fd223, %fd176, %fd225;
	neg.f64 	%fd227, %fd224;
	neg.f64 	%fd228, %fd226;
	cvt.rn.f32.f64 	%f9, %fd224;
	mul.f32 	%f10, %f9, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f11, %f10;
	cvt.f64.f32 	%fd229, %f11;
	neg.f64 	%fd230, %fd229;
	mov.f64 	%fd231, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd232, %fd230, %fd231, %fd227;
	mov.f64 	%fd233, 0d3E928A27F89B6999;
	mov.f64 	%fd234, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd235, %fd234, %fd232, %fd233;
	mov.f64 	%fd236, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd237, %fd235, %fd232, %fd236;
	mov.f64 	%fd238, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd239, %fd237, %fd232, %fd238;
	mov.f64 	%fd240, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd241, %fd239, %fd232, %fd240;
	mov.f64 	%fd242, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd243, %fd241, %fd232, %fd242;
	mov.f64 	%fd244, 0d3F811111111173C4;
	fma.rn.f64 	%fd245, %fd243, %fd232, %fd244;
	mov.f64 	%fd246, 0d3FA555555555211A;
	fma.rn.f64 	%fd247, %fd245, %fd232, %fd246;
	mov.f64 	%fd248, 0d3FC5555555555540;
	fma.rn.f64 	%fd249, %fd247, %fd232, %fd248;
	mov.f64 	%fd250, 0d3FE0000000000005;
	fma.rn.f64 	%fd251, %fd249, %fd232, %fd250;
	mul.f64 	%fd252, %fd232, %fd251;
	fma.rn.f64 	%fd253, %fd252, %fd232, %fd228;
	add.f64 	%fd254, %fd232, %fd253;
	ex2.approx.ftz.f32 	%f12, %f11;
	cvt.f64.f32 	%fd255, %f12;
	mov.f64 	%fd256, 0d3FF0000000000000;
	sub.f64 	%fd257, %fd256, %fd255;
	neg.f64 	%fd258, %fd254;
	fma.rn.f64 	%fd259, %fd258, %fd255, %fd257;
	setp.ge.f64 	%p20, %fd176, 0d4017AFB48DC96626;
	selp.f64 	%fd260, 0d3FF0000000000000, %fd259, %p20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd175;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd260;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd260;
	}
	and.b32  	%r47, %r44, -2147483648;
	or.b32  	%r48, %r46, %r47;
	mov.b64 	%fd261, {%r45, %r48};
	st.global.f64 	[%rd60], %fd261;
	add.s32 	%r50, %r50, %r8;
	cvt.u64.u32 	%rd68, %r50;
	setp.lt.u64 	%p21, %rd68, %rd65;
	@%p21 bra 	$L__BB42_16;
	bra.uni 	$L__BB42_17;

$L__BB42_5:
	setp.ge.u64 	%p8, %rd68, %rd24;
	@%p8 bra 	$L__BB42_17;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB42_7:
	ld.param.u64 	%rd64, [uerf_f64_param_0];
	shl.b64 	%rd40, %rd68, 3;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.f64 	%fd1, [%rd41];
	abs.f64 	%fd2, %fd1;
	mov.f64 	%fd3, 0d3D47088FDB46FA5F;
	mov.f64 	%fd4, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd5, %fd4, %fd2, %fd3;
	mov.f64 	%fd6, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd7, %fd5, %fd2, %fd6;
	mov.f64 	%fd8, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd9, %fd7, %fd2, %fd8;
	mov.f64 	%fd10, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd11, %fd9, %fd2, %fd10;
	mov.f64 	%fd12, 0d3E2485EE67835925;
	fma.rn.f64 	%fd13, %fd11, %fd2, %fd12;
	mov.f64 	%fd14, 0dBE476DB45919F583;
	fma.rn.f64 	%fd15, %fd13, %fd2, %fd14;
	mov.f64 	%fd16, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd17, %fd15, %fd2, %fd16;
	mov.f64 	%fd18, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd19, %fd17, %fd2, %fd18;
	mov.f64 	%fd20, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd21, %fd19, %fd2, %fd20;
	mov.f64 	%fd22, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd23, %fd21, %fd2, %fd22;
	mov.f64 	%fd24, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd25, %fd23, %fd2, %fd24;
	mov.f64 	%fd26, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd27, %fd25, %fd2, %fd26;
	mov.f64 	%fd28, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd29, %fd27, %fd2, %fd28;
	mov.f64 	%fd30, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd31, %fd29, %fd2, %fd30;
	mov.f64 	%fd32, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd33, %fd31, %fd2, %fd32;
	mov.f64 	%fd34, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd35, %fd33, %fd2, %fd34;
	mov.f64 	%fd36, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd37, %fd35, %fd2, %fd36;
	mov.f64 	%fd38, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd39, %fd37, %fd2, %fd38;
	mov.f64 	%fd40, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd41, %fd39, %fd2, %fd40;
	mov.f64 	%fd42, 0dBF9396685912A453;
	fma.rn.f64 	%fd43, %fd41, %fd2, %fd42;
	mov.f64 	%fd44, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd45, %fd43, %fd2, %fd44;
	mov.f64 	%fd46, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd47, %fd45, %fd2, %fd46;
	mov.f64 	%fd48, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd49, %fd47, %fd2, %fd48;
	fma.rn.f64 	%fd50, %fd49, %fd2, %fd2;
	sub.f64 	%fd51, %fd2, %fd50;
	fma.rn.f64 	%fd52, %fd49, %fd2, %fd51;
	neg.f64 	%fd53, %fd50;
	neg.f64 	%fd54, %fd52;
	cvt.rn.f32.f64 	%f1, %fd50;
	mul.f32 	%f2, %f1, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f3, %f2;
	cvt.f64.f32 	%fd55, %f3;
	neg.f64 	%fd56, %fd55;
	mov.f64 	%fd57, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd58, %fd56, %fd57, %fd53;
	mov.f64 	%fd59, 0d3E928A27F89B6999;
	mov.f64 	%fd60, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd61, %fd60, %fd58, %fd59;
	mov.f64 	%fd62, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	mov.f64 	%fd64, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	mov.f64 	%fd66, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	mov.f64 	%fd68, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	mov.f64 	%fd70, 0d3F811111111173C4;
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	mov.f64 	%fd72, 0d3FA555555555211A;
	fma.rn.f64 	%fd73, %fd71, %fd58, %fd72;
	mov.f64 	%fd74, 0d3FC5555555555540;
	fma.rn.f64 	%fd75, %fd73, %fd58, %fd74;
	mov.f64 	%fd76, 0d3FE0000000000005;
	fma.rn.f64 	%fd77, %fd75, %fd58, %fd76;
	mul.f64 	%fd78, %fd58, %fd77;
	fma.rn.f64 	%fd79, %fd78, %fd58, %fd54;
	add.f64 	%fd80, %fd58, %fd79;
	ex2.approx.ftz.f32 	%f4, %f3;
	cvt.f64.f32 	%fd81, %f4;
	mov.f64 	%fd82, 0d3FF0000000000000;
	sub.f64 	%fd83, %fd82, %fd81;
	neg.f64 	%fd84, %fd80;
	fma.rn.f64 	%fd85, %fd84, %fd81, %fd83;
	setp.ge.f64 	%p10, %fd2, 0d4017AFB48DC96626;
	selp.f64 	%fd86, 0d3FF0000000000000, %fd85, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd86;
	}
	and.b32  	%r27, %r24, -2147483648;
	or.b32  	%r28, %r26, %r27;
	mov.b64 	%fd87, {%r25, %r28};
	add.s64 	%rd42, %rd1, %rd40;
	st.global.f64 	[%rd42], %fd87;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd68, %r50;
	setp.lt.u64 	%p11, %rd68, %rd64;
	@%p11 bra 	$L__BB42_7;

$L__BB42_17:
	ret;

}
	// .globl	uceil_f32
.visible .entry uceil_f32(
	.param .u64 uceil_f32_param_0,
	.param .u64 uceil_f32_param_1,
	.param .u64 uceil_f32_param_2,
	.param .u64 uceil_f32_param_3,
	.param .u64 uceil_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uceil_f32_param_0];
	ld.param.u64 	%rd27, [uceil_f32_param_1];
	ld.param.u64 	%rd29, [uceil_f32_param_2];
	ld.param.u64 	%rd28, [uceil_f32_param_3];
	ld.param.u64 	%rd30, [uceil_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB43_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB43_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB43_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB43_2;

$L__BB43_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB43_8;
	bra.uni 	$L__BB43_5;

$L__BB43_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB43_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB43_16;

$L__BB43_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB43_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB43_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB43_14;

$L__BB43_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB43_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB43_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	cvt.rpi.f32.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB43_10;
	bra.uni 	$L__BB43_19;

$L__BB43_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB43_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB43_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	cvt.rpi.f32.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB43_7;

$L__BB43_19:
	ret;

$L__BB43_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB43_18;

$L__BB43_17:
	ld.global.f32 	%f5, [%rd2];
	cvt.rpi.f32.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB43_17;
	bra.uni 	$L__BB43_19;

$L__BB43_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	cvt.rpi.f32.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB43_18;
	bra.uni 	$L__BB43_19;

}
	// .globl	uceil_f64
.visible .entry uceil_f64(
	.param .u64 uceil_f64_param_0,
	.param .u64 uceil_f64_param_1,
	.param .u64 uceil_f64_param_2,
	.param .u64 uceil_f64_param_3,
	.param .u64 uceil_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uceil_f64_param_0];
	ld.param.u64 	%rd27, [uceil_f64_param_1];
	ld.param.u64 	%rd29, [uceil_f64_param_2];
	ld.param.u64 	%rd28, [uceil_f64_param_3];
	ld.param.u64 	%rd30, [uceil_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB44_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB44_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB44_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB44_2;

$L__BB44_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB44_8;
	bra.uni 	$L__BB44_5;

$L__BB44_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB44_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB44_16;

$L__BB44_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB44_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB44_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB44_14;

$L__BB44_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB44_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB44_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	cvt.rpi.f64.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB44_10;
	bra.uni 	$L__BB44_19;

$L__BB44_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB44_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB44_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	cvt.rpi.f64.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB44_7;

$L__BB44_19:
	ret;

$L__BB44_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB44_18;

$L__BB44_17:
	ld.global.f64 	%fd5, [%rd2];
	cvt.rpi.f64.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB44_17;
	bra.uni 	$L__BB44_19;

$L__BB44_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	cvt.rpi.f64.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB44_18;
	bra.uni 	$L__BB44_19;

}
	// .globl	ufloor_f32
.visible .entry ufloor_f32(
	.param .u64 ufloor_f32_param_0,
	.param .u64 ufloor_f32_param_1,
	.param .u64 ufloor_f32_param_2,
	.param .u64 ufloor_f32_param_3,
	.param .u64 ufloor_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [ufloor_f32_param_0];
	ld.param.u64 	%rd27, [ufloor_f32_param_1];
	ld.param.u64 	%rd29, [ufloor_f32_param_2];
	ld.param.u64 	%rd28, [ufloor_f32_param_3];
	ld.param.u64 	%rd30, [ufloor_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB45_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB45_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB45_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB45_2;

$L__BB45_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB45_8;
	bra.uni 	$L__BB45_5;

$L__BB45_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB45_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB45_16;

$L__BB45_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB45_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB45_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB45_14;

$L__BB45_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB45_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB45_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	cvt.rmi.f32.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB45_10;
	bra.uni 	$L__BB45_19;

$L__BB45_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB45_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB45_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	cvt.rmi.f32.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB45_7;

$L__BB45_19:
	ret;

$L__BB45_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB45_18;

$L__BB45_17:
	ld.global.f32 	%f5, [%rd2];
	cvt.rmi.f32.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB45_17;
	bra.uni 	$L__BB45_19;

$L__BB45_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	cvt.rmi.f32.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB45_18;
	bra.uni 	$L__BB45_19;

}
	// .globl	ufloor_f64
.visible .entry ufloor_f64(
	.param .u64 ufloor_f64_param_0,
	.param .u64 ufloor_f64_param_1,
	.param .u64 ufloor_f64_param_2,
	.param .u64 ufloor_f64_param_3,
	.param .u64 ufloor_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [ufloor_f64_param_0];
	ld.param.u64 	%rd27, [ufloor_f64_param_1];
	ld.param.u64 	%rd29, [ufloor_f64_param_2];
	ld.param.u64 	%rd28, [ufloor_f64_param_3];
	ld.param.u64 	%rd30, [ufloor_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB46_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB46_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB46_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB46_2;

$L__BB46_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB46_8;
	bra.uni 	$L__BB46_5;

$L__BB46_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB46_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB46_16;

$L__BB46_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB46_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB46_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB46_14;

$L__BB46_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB46_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB46_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	cvt.rmi.f64.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB46_10;
	bra.uni 	$L__BB46_19;

$L__BB46_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB46_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB46_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	cvt.rmi.f64.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB46_7;

$L__BB46_19:
	ret;

$L__BB46_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB46_18;

$L__BB46_17:
	ld.global.f64 	%fd5, [%rd2];
	cvt.rmi.f64.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB46_17;
	bra.uni 	$L__BB46_19;

$L__BB46_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	cvt.rmi.f64.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB46_18;
	bra.uni 	$L__BB46_19;

}
	// .globl	uround_f32
.visible .entry uround_f32(
	.param .u64 uround_f32_param_0,
	.param .u64 uround_f32_param_1,
	.param .u64 uround_f32_param_2,
	.param .u64 uround_f32_param_3,
	.param .u64 uround_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uround_f32_param_0];
	ld.param.u64 	%rd27, [uround_f32_param_1];
	ld.param.u64 	%rd29, [uround_f32_param_2];
	ld.param.u64 	%rd28, [uround_f32_param_3];
	ld.param.u64 	%rd30, [uround_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB47_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r48, 0;

$L__BB47_2:
	not.b32 	%r22, %r48;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB47_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r48, %r48, 1;
	cvt.u64.u32 	%rd41, %r48;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB47_2;

$L__BB47_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB47_8;
	bra.uni 	$L__BB47_5;

$L__BB47_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB47_19;

	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r29;
	@%p3 bra 	$L__BB47_16;

$L__BB47_10:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r4;
	mov.u32 	%r53, %r51;

$L__BB47_11:
	not.b32 	%r32, %r51;
	cvt.u64.u32 	%rd45, %r32;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r52;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB47_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB47_14;

$L__BB47_13:
	cvt.u32.u64 	%r33, %rd14;
	cvt.u32.u64 	%r34, %rd12;
	div.u32 	%r35, %r34, %r33;
	mul.lo.s32 	%r36, %r35, %r33;
	sub.s32 	%r37, %r34, %r36;
	cvt.u64.u32 	%rd68, %r35;
	cvt.u64.u32 	%rd69, %r37;

$L__BB47_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r38, %rd54;
	add.s32 	%r53, %r53, %r38;
	cvt.u32.u64 	%r52, %rd68;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd55, %r51;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB47_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r53, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f5, [%rd60];
	mov.b32 	%r39, %f5;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, 1056964608;
	mov.b32 	%f6, %r41;
	add.rz.f32 	%f7, %f5, %f6;
	cvt.rzi.f32.f32 	%f8, %f7;
	st.global.f32 	[%rd59], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB47_10;
	bra.uni 	$L__BB47_19;

$L__BB47_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB47_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB47_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	mov.b32 	%r26, %f1;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	%f2, %r28;
	add.rz.f32 	%f3, %f1, %f2;
	cvt.rzi.f32.f32 	%f4, %f3;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB47_7;

$L__BB47_19:
	ret;

$L__BB47_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB47_18;

$L__BB47_17:
	ld.global.f32 	%f9, [%rd2];
	mov.b32 	%r42, %f9;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f10, %r44;
	add.rz.f32 	%f11, %f9, %f10;
	cvt.rzi.f32.f32 	%f12, %f11;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f12;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB47_17;
	bra.uni 	$L__BB47_19;

$L__BB47_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f13, [%rd64];
	mov.b32 	%r45, %f13;
	and.b32  	%r46, %r45, -2147483648;
	or.b32  	%r47, %r46, 1056964608;
	mov.b32 	%f14, %r47;
	add.rz.f32 	%f15, %f13, %f14;
	cvt.rzi.f32.f32 	%f16, %f15;
	st.global.f32 	[%rd64], %f16;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB47_18;
	bra.uni 	$L__BB47_19;

}
	// .globl	uround_f64
.visible .entry uround_f64(
	.param .u64 uround_f64_param_0,
	.param .u64 uround_f64_param_1,
	.param .u64 uround_f64_param_2,
	.param .u64 uround_f64_param_3,
	.param .u64 uround_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uround_f64_param_0];
	ld.param.u64 	%rd27, [uround_f64_param_1];
	ld.param.u64 	%rd29, [uround_f64_param_2];
	ld.param.u64 	%rd28, [uround_f64_param_3];
	ld.param.u64 	%rd30, [uround_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB48_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r52, 0;

$L__BB48_2:
	not.b32 	%r26, %r52;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB48_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r52, %r52, 1;
	cvt.u64.u32 	%rd41, %r52;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB48_2;

$L__BB48_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %tid.x;
	mad.lo.s32 	%r4, %r27, %r3, %r28;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB48_8;
	bra.uni 	$L__BB48_5;

$L__BB48_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB48_19;

	mov.f64 	%fd6, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd6;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd6;
	}
	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r12, %r3, %r33;
	@%p3 bra 	$L__BB48_16;

$L__BB48_10:
	mov.u32 	%r55, 0;
	mov.u32 	%r56, %r4;
	mov.u32 	%r57, %r55;

$L__BB48_11:
	not.b32 	%r36, %r55;
	cvt.u64.u32 	%rd45, %r36;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r56;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB48_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB48_14;

$L__BB48_13:
	cvt.u32.u64 	%r37, %rd14;
	cvt.u32.u64 	%r38, %rd12;
	div.u32 	%r39, %r38, %r37;
	mul.lo.s32 	%r40, %r39, %r37;
	sub.s32 	%r41, %r38, %r40;
	cvt.u64.u32 	%rd68, %r39;
	cvt.u64.u32 	%rd69, %r41;

$L__BB48_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r42, %rd54;
	add.s32 	%r57, %r57, %r42;
	cvt.u32.u64 	%r56, %rd68;
	add.s32 	%r55, %r55, 1;
	cvt.u64.u32 	%rd55, %r55;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB48_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r57, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd7, [%rd60];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd7;
	}
	and.b32  	%r44, %r43, -2147483648;
	or.b32  	%r45, %r11, %r44;
	mov.b64 	%fd8, {%r10, %r45};
	add.rz.f64 	%fd9, %fd7, %fd8;
	cvt.rzi.f64.f64 	%fd10, %fd9;
	st.global.f64 	[%rd59], %fd10;
	add.s32 	%r4, %r4, %r12;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB48_10;
	bra.uni 	$L__BB48_19;

$L__BB48_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB48_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.f64 	%fd1, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r5, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r6}, %fd1;
	}
	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r7, %r3, %r29;

$L__BB48_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd2, [%rd43];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd2;
	}
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r6, %r31;
	mov.b64 	%fd3, {%r5, %r32};
	add.rz.f64 	%fd4, %fd2, %fd3;
	cvt.rzi.f64.f64 	%fd5, %fd4;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd5;
	add.s32 	%r4, %r4, %r7;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB48_7;

$L__BB48_19:
	ret;

$L__BB48_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB48_18;

$L__BB48_17:
	ld.global.f64 	%fd11, [%rd2];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd11;
	}
	and.b32  	%r47, %r46, -2147483648;
	or.b32  	%r48, %r11, %r47;
	mov.b64 	%fd12, {%r10, %r48};
	add.rz.f64 	%fd13, %fd11, %fd12;
	cvt.rzi.f64.f64 	%fd14, %fd13;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd14;
	add.s32 	%r4, %r4, %r12;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB48_17;
	bra.uni 	$L__BB48_19;

$L__BB48_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd15, [%rd64];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd15;
	}
	and.b32  	%r50, %r49, -2147483648;
	or.b32  	%r51, %r11, %r50;
	mov.b64 	%fd16, {%r10, %r51};
	add.rz.f64 	%fd17, %fd15, %fd16;
	cvt.rzi.f64.f64 	%fd18, %fd17;
	st.global.f64 	[%rd64], %fd18;
	add.s32 	%r4, %r4, %r12;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB48_18;
	bra.uni 	$L__BB48_19;

}
	// .globl	unormcdf_f32
.visible .entry unormcdf_f32(
	.param .u64 unormcdf_f32_param_0,
	.param .u64 unormcdf_f32_param_1,
	.param .u64 unormcdf_f32_param_2,
	.param .u64 unormcdf_f32_param_3,
	.param .u64 unormcdf_f32_param_4
)
{
	.reg .pred 	%p<35>;
	.reg .f32 	%f<253>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd26, [unormcdf_f32_param_0];
	ld.param.u64 	%rd27, [unormcdf_f32_param_1];
	ld.param.u64 	%rd29, [unormcdf_f32_param_2];
	ld.param.u64 	%rd28, [unormcdf_f32_param_3];
	ld.param.u64 	%rd30, [unormcdf_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p34, %p2;
	@%p3 bra 	$L__BB49_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r58, 0;

$L__BB49_2:
	not.b32 	%r20, %r58;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd64, %rd37;
	mov.pred 	%p34, -1;
	@%p5 bra 	$L__BB49_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd64, %rd40, %rd64;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd41, %r58;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p34, %p2;
	@%p7 bra 	$L__BB49_2;

$L__BB49_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r59, %r21, %r3, %r22;
	cvt.u64.u32 	%rd65, %r59;
	@%p34 bra 	$L__BB49_10;
	bra.uni 	$L__BB49_5;

$L__BB49_10:
	setp.ge.u64 	%p16, %rd65, %rd26;
	@%p16 bra 	$L__BB49_23;

	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r32;
	@%p3 bra 	$L__BB49_20;

$L__BB49_12:
	mov.u32 	%r61, 0;
	mov.u32 	%r62, %r59;
	mov.u32 	%r63, %r61;

$L__BB49_13:
	not.b32 	%r35, %r61;
	cvt.u64.u32 	%rd46, %r35;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r62;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p18, %rd50, 0;
	@%p18 bra 	$L__BB49_15;

	div.u64 	%rd67, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd67, %rd14;
	sub.s64 	%rd68, %rd12, %rd51;
	bra.uni 	$L__BB49_16;

$L__BB49_15:
	cvt.u32.u64 	%r36, %rd14;
	cvt.u32.u64 	%r37, %rd12;
	div.u32 	%r38, %r37, %r36;
	mul.lo.s32 	%r39, %r38, %r36;
	sub.s32 	%r40, %r37, %r39;
	cvt.u64.u32 	%rd67, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB49_16:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd68;
	cvt.u32.u64 	%r41, %rd55;
	add.s32 	%r63, %r63, %r41;
	cvt.u32.u64 	%r62, %rd67;
	add.s32 	%r61, %r61, 1;
	cvt.u64.u32 	%rd56, %r61;
	setp.lt.u64 	%p19, %rd56, %rd27;
	@%p19 bra 	$L__BB49_13;

	setp.eq.s64 	%p20, %rd28, 0;
	mul.wide.u32 	%rd57, %r63, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd65, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p20;
	ld.global.f32 	%f96, [%rd60];
	abs.f32 	%f97, %f96;
	setp.gt.f32 	%p21, %f97, 0f41680000;
	mov.b32 	%r42, %f96;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1097334784;
	mov.b32 	%f98, %r44;
	selp.f32 	%f99, %f98, %f96, %p21;
	mov.f32 	%f100, 0fBF3504F3;
	mul.rn.f32 	%f7, %f99, %f100;
	neg.f32 	%f101, %f7;
	fma.rn.f32 	%f102, %f99, %f100, %f101;
	mov.f32 	%f103, 0fB24FE77A;
	fma.rn.f32 	%f8, %f99, %f103, %f102;
	add.rn.f32 	%f9, %f7, %f8;
	abs.f32 	%f104, %f9;
	add.f32 	%f105, %f104, 0fC0800000;
	mov.f32 	%f106, 0fC0800000;
	add.f32 	%f107, %f104, 0f40800000;
	rcp.approx.ftz.f32 	%f108, %f107;
	mul.rn.f32 	%f109, %f105, %f108;
	add.f32 	%f110, %f109, 0f3F800000;
	mov.f32 	%f111, 0f3F800000;
	fma.rn.f32 	%f112, %f106, %f110, %f104;
	neg.f32 	%f113, %f109;
	fma.rn.f32 	%f114, %f113, %f104, %f112;
	fma.rn.f32 	%f115, %f108, %f114, %f109;
	mov.f32 	%f116, 0f3BE6E05B;
	mov.f32 	%f117, 0f3A69A091;
	fma.rn.f32 	%f118, %f117, %f115, %f116;
	mov.f32 	%f119, 0fBC81FB4B;
	fma.rn.f32 	%f120, %f118, %f115, %f119;
	mov.f32 	%f121, 0f3D15373B;
	fma.rn.f32 	%f122, %f120, %f115, %f121;
	mov.f32 	%f123, 0fBD887C5A;
	fma.rn.f32 	%f124, %f122, %f115, %f123;
	mov.f32 	%f125, 0f3DC021D5;
	fma.rn.f32 	%f126, %f124, %f115, %f125;
	mov.f32 	%f127, 0fBDCED424;
	fma.rn.f32 	%f128, %f126, %f115, %f127;
	mov.f32 	%f129, 0f3D8B74DE;
	fma.rn.f32 	%f130, %f128, %f115, %f129;
	mov.f32 	%f131, 0f3C7BF170;
	fma.rn.f32 	%f132, %f130, %f115, %f131;
	mov.f32 	%f133, 0fBE0EF8D4;
	fma.rn.f32 	%f134, %f132, %f115, %f133;
	mov.f32 	%f135, 0f3F9DD2C9;
	fma.rn.f32 	%f136, %f134, %f115, %f135;
	mov.f32 	%f137, 0f40000000;
	fma.rn.f32 	%f138, %f137, %f104, %f111;
	rcp.approx.ftz.f32 	%f139, %f138;
	mul.rn.f32 	%f140, %f136, %f139;
	mul.f32 	%f141, %f140, 0fC0000000;
	fma.rn.f32 	%f142, %f104, %f141, %f136;
	sub.f32 	%f143, %f142, %f140;
	fma.rn.f32 	%f144, %f143, %f139, %f140;
	mul.f32 	%f145, %f104, %f104;
	neg.f32 	%f146, %f145;
	mov.f32 	%f147, 0f3FB8AA3B;
	mul.rn.f32 	%f148, %f146, %f147;
	cvt.rzi.f32.f32 	%f149, %f148;
	abs.f32 	%f150, %f149;
	setp.gt.f32 	%p22, %f150, 0f42FC0000;
	mov.b32 	%r45, %f149;
	and.b32  	%r46, %r45, -2147483648;
	or.b32  	%r47, %r46, 1123811328;
	mov.b32 	%f151, %r47;
	selp.f32 	%f152, %f151, %f149, %p22;
	mov.f32 	%f153, 0fBF317218;
	fma.rn.f32 	%f154, %f152, %f153, %f146;
	mov.f32 	%f155, 0f3102E308;
	fma.rn.f32 	%f156, %f152, %f155, %f154;
	mul.f32 	%f157, %f156, 0f3FB8AA3B;
	add.f32 	%f158, %f152, 0f4B40007F;
	mov.b32 	%r48, %f158;
	shl.b32 	%r49, %r48, 23;
	mov.b32 	%f159, %r49;
	ex2.approx.ftz.f32 	%f160, %f157;
	mul.f32 	%f161, %f160, %f159;
	neg.f32 	%f162, %f104;
	fma.rn.f32 	%f163, %f162, %f104, %f145;
	fma.rn.f32 	%f164, %f161, %f163, %f161;
	mul.f32 	%f165, %f144, %f164;
	setp.gt.f32 	%p23, %f104, 0f4120E148;
	selp.f32 	%f166, 0f00000000, %f165, %p23;
	setp.lt.f32 	%p24, %f9, 0f00000000;
	sub.f32 	%f167, %f137, %f166;
	selp.f32 	%f251, %f167, %f166, %p24;
	setp.geu.f32 	%p25, %f99, 0fBF800000;
	@%p25 bra 	$L__BB49_19;

	sub.f32 	%f168, %f7, %f9;
	add.rn.f32 	%f169, %f168, %f8;
	mul.f32 	%f170, %f9, 0fC0000000;
	mul.f32 	%f171, %f170, %f251;
	fma.rn.f32 	%f251, %f171, %f169, %f251;

$L__BB49_19:
	ld.param.u64 	%rd63, [unormcdf_f32_param_0];
	mul.f32 	%f172, %f251, 0f3F000000;
	st.global.f32 	[%rd21], %f172;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p26, %rd65, %rd63;
	@%p26 bra 	$L__BB49_12;
	bra.uni 	$L__BB49_23;

$L__BB49_20:
	shl.b64 	%rd61, %rd65, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p27, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p27;
	ld.global.f32 	%f173, [%rd62];
	abs.f32 	%f174, %f173;
	setp.gt.f32 	%p28, %f174, 0f41680000;
	mov.b32 	%r50, %f173;
	and.b32  	%r51, %r50, -2147483648;
	or.b32  	%r52, %r51, 1097334784;
	mov.b32 	%f175, %r52;
	selp.f32 	%f176, %f175, %f173, %p28;
	mov.f32 	%f177, 0fBF3504F3;
	mul.rn.f32 	%f13, %f176, %f177;
	neg.f32 	%f178, %f13;
	fma.rn.f32 	%f179, %f176, %f177, %f178;
	mov.f32 	%f180, 0fB24FE77A;
	fma.rn.f32 	%f14, %f176, %f180, %f179;
	add.rn.f32 	%f15, %f13, %f14;
	abs.f32 	%f181, %f15;
	add.f32 	%f182, %f181, 0fC0800000;
	mov.f32 	%f183, 0fC0800000;
	add.f32 	%f184, %f181, 0f40800000;
	rcp.approx.ftz.f32 	%f185, %f184;
	mul.rn.f32 	%f186, %f182, %f185;
	add.f32 	%f187, %f186, 0f3F800000;
	mov.f32 	%f188, 0f3F800000;
	fma.rn.f32 	%f189, %f183, %f187, %f181;
	neg.f32 	%f190, %f186;
	fma.rn.f32 	%f191, %f190, %f181, %f189;
	fma.rn.f32 	%f192, %f185, %f191, %f186;
	mov.f32 	%f193, 0f3BE6E05B;
	mov.f32 	%f194, 0f3A69A091;
	fma.rn.f32 	%f195, %f194, %f192, %f193;
	mov.f32 	%f196, 0fBC81FB4B;
	fma.rn.f32 	%f197, %f195, %f192, %f196;
	mov.f32 	%f198, 0f3D15373B;
	fma.rn.f32 	%f199, %f197, %f192, %f198;
	mov.f32 	%f200, 0fBD887C5A;
	fma.rn.f32 	%f201, %f199, %f192, %f200;
	mov.f32 	%f202, 0f3DC021D5;
	fma.rn.f32 	%f203, %f201, %f192, %f202;
	mov.f32 	%f204, 0fBDCED424;
	fma.rn.f32 	%f205, %f203, %f192, %f204;
	mov.f32 	%f206, 0f3D8B74DE;
	fma.rn.f32 	%f207, %f205, %f192, %f206;
	mov.f32 	%f208, 0f3C7BF170;
	fma.rn.f32 	%f209, %f207, %f192, %f208;
	mov.f32 	%f210, 0fBE0EF8D4;
	fma.rn.f32 	%f211, %f209, %f192, %f210;
	mov.f32 	%f212, 0f3F9DD2C9;
	fma.rn.f32 	%f213, %f211, %f192, %f212;
	mov.f32 	%f214, 0f40000000;
	fma.rn.f32 	%f215, %f214, %f181, %f188;
	rcp.approx.ftz.f32 	%f216, %f215;
	mul.rn.f32 	%f217, %f213, %f216;
	mul.f32 	%f218, %f217, 0fC0000000;
	fma.rn.f32 	%f219, %f181, %f218, %f213;
	sub.f32 	%f220, %f219, %f217;
	fma.rn.f32 	%f221, %f220, %f216, %f217;
	mul.f32 	%f222, %f181, %f181;
	neg.f32 	%f223, %f222;
	mov.f32 	%f224, 0f3FB8AA3B;
	mul.rn.f32 	%f225, %f223, %f224;
	cvt.rzi.f32.f32 	%f226, %f225;
	abs.f32 	%f227, %f226;
	setp.gt.f32 	%p29, %f227, 0f42FC0000;
	mov.b32 	%r53, %f226;
	and.b32  	%r54, %r53, -2147483648;
	or.b32  	%r55, %r54, 1123811328;
	mov.b32 	%f228, %r55;
	selp.f32 	%f229, %f228, %f226, %p29;
	mov.f32 	%f230, 0fBF317218;
	fma.rn.f32 	%f231, %f229, %f230, %f223;
	mov.f32 	%f232, 0f3102E308;
	fma.rn.f32 	%f233, %f229, %f232, %f231;
	mul.f32 	%f234, %f233, 0f3FB8AA3B;
	add.f32 	%f235, %f229, 0f4B40007F;
	mov.b32 	%r56, %f235;
	shl.b32 	%r57, %r56, 23;
	mov.b32 	%f236, %r57;
	ex2.approx.ftz.f32 	%f237, %f234;
	mul.f32 	%f238, %f237, %f236;
	neg.f32 	%f239, %f181;
	fma.rn.f32 	%f240, %f239, %f181, %f222;
	fma.rn.f32 	%f241, %f238, %f240, %f238;
	mul.f32 	%f242, %f221, %f241;
	setp.gt.f32 	%p30, %f181, 0f4120E148;
	selp.f32 	%f243, 0f00000000, %f242, %p30;
	setp.lt.f32 	%p31, %f15, 0f00000000;
	sub.f32 	%f244, %f214, %f243;
	selp.f32 	%f252, %f244, %f243, %p31;
	setp.geu.f32 	%p32, %f176, 0fBF800000;
	@%p32 bra 	$L__BB49_22;

	sub.f32 	%f245, %f13, %f15;
	add.rn.f32 	%f246, %f245, %f14;
	mul.f32 	%f247, %f15, 0fC0000000;
	mul.f32 	%f248, %f247, %f252;
	fma.rn.f32 	%f252, %f248, %f246, %f252;

$L__BB49_22:
	mul.f32 	%f249, %f252, 0f3F000000;
	st.global.f32 	[%rd24], %f249;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p33, %rd65, %rd26;
	@%p33 bra 	$L__BB49_20;
	bra.uni 	$L__BB49_23;

$L__BB49_5:
	setp.ge.u64 	%p8, %rd65, %rd26;
	@%p8 bra 	$L__BB49_23;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB49_7:
	shl.b64 	%rd42, %rd65, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f19, [%rd43];
	abs.f32 	%f20, %f19;
	setp.gt.f32 	%p10, %f20, 0f41680000;
	mov.b32 	%r24, %f19;
	and.b32  	%r25, %r24, -2147483648;
	or.b32  	%r26, %r25, 1097334784;
	mov.b32 	%f21, %r26;
	selp.f32 	%f22, %f21, %f19, %p10;
	mov.f32 	%f23, 0fBF3504F3;
	mul.rn.f32 	%f1, %f22, %f23;
	neg.f32 	%f24, %f1;
	fma.rn.f32 	%f25, %f22, %f23, %f24;
	mov.f32 	%f26, 0fB24FE77A;
	fma.rn.f32 	%f2, %f22, %f26, %f25;
	add.rn.f32 	%f3, %f1, %f2;
	abs.f32 	%f27, %f3;
	add.f32 	%f28, %f27, 0fC0800000;
	mov.f32 	%f29, 0fC0800000;
	add.f32 	%f30, %f27, 0f40800000;
	rcp.approx.ftz.f32 	%f31, %f30;
	mul.rn.f32 	%f32, %f28, %f31;
	add.f32 	%f33, %f32, 0f3F800000;
	mov.f32 	%f34, 0f3F800000;
	fma.rn.f32 	%f35, %f29, %f33, %f27;
	neg.f32 	%f36, %f32;
	fma.rn.f32 	%f37, %f36, %f27, %f35;
	fma.rn.f32 	%f38, %f31, %f37, %f32;
	mov.f32 	%f39, 0f3BE6E05B;
	mov.f32 	%f40, 0f3A69A091;
	fma.rn.f32 	%f41, %f40, %f38, %f39;
	mov.f32 	%f42, 0fBC81FB4B;
	fma.rn.f32 	%f43, %f41, %f38, %f42;
	mov.f32 	%f44, 0f3D15373B;
	fma.rn.f32 	%f45, %f43, %f38, %f44;
	mov.f32 	%f46, 0fBD887C5A;
	fma.rn.f32 	%f47, %f45, %f38, %f46;
	mov.f32 	%f48, 0f3DC021D5;
	fma.rn.f32 	%f49, %f47, %f38, %f48;
	mov.f32 	%f50, 0fBDCED424;
	fma.rn.f32 	%f51, %f49, %f38, %f50;
	mov.f32 	%f52, 0f3D8B74DE;
	fma.rn.f32 	%f53, %f51, %f38, %f52;
	mov.f32 	%f54, 0f3C7BF170;
	fma.rn.f32 	%f55, %f53, %f38, %f54;
	mov.f32 	%f56, 0fBE0EF8D4;
	fma.rn.f32 	%f57, %f55, %f38, %f56;
	mov.f32 	%f58, 0f3F9DD2C9;
	fma.rn.f32 	%f59, %f57, %f38, %f58;
	mov.f32 	%f60, 0f40000000;
	fma.rn.f32 	%f61, %f60, %f27, %f34;
	rcp.approx.ftz.f32 	%f62, %f61;
	mul.rn.f32 	%f63, %f59, %f62;
	mul.f32 	%f64, %f63, 0fC0000000;
	fma.rn.f32 	%f65, %f27, %f64, %f59;
	sub.f32 	%f66, %f65, %f63;
	fma.rn.f32 	%f67, %f66, %f62, %f63;
	mul.f32 	%f68, %f27, %f27;
	neg.f32 	%f69, %f68;
	mov.f32 	%f70, 0f3FB8AA3B;
	mul.rn.f32 	%f71, %f69, %f70;
	cvt.rzi.f32.f32 	%f72, %f71;
	abs.f32 	%f73, %f72;
	setp.gt.f32 	%p11, %f73, 0f42FC0000;
	mov.b32 	%r27, %f72;
	and.b32  	%r28, %r27, -2147483648;
	or.b32  	%r29, %r28, 1123811328;
	mov.b32 	%f74, %r29;
	selp.f32 	%f75, %f74, %f72, %p11;
	mov.f32 	%f76, 0fBF317218;
	fma.rn.f32 	%f77, %f75, %f76, %f69;
	mov.f32 	%f78, 0f3102E308;
	fma.rn.f32 	%f79, %f75, %f78, %f77;
	mul.f32 	%f80, %f79, 0f3FB8AA3B;
	add.f32 	%f81, %f75, 0f4B40007F;
	mov.b32 	%r30, %f81;
	shl.b32 	%r31, %r30, 23;
	mov.b32 	%f82, %r31;
	ex2.approx.ftz.f32 	%f83, %f80;
	mul.f32 	%f84, %f83, %f82;
	neg.f32 	%f85, %f27;
	fma.rn.f32 	%f86, %f85, %f27, %f68;
	fma.rn.f32 	%f87, %f84, %f86, %f84;
	mul.f32 	%f88, %f67, %f87;
	setp.gt.f32 	%p12, %f27, 0f4120E148;
	selp.f32 	%f89, 0f00000000, %f88, %p12;
	setp.lt.f32 	%p13, %f3, 0f00000000;
	sub.f32 	%f90, %f60, %f89;
	selp.f32 	%f250, %f90, %f89, %p13;
	setp.geu.f32 	%p14, %f22, 0fBF800000;
	@%p14 bra 	$L__BB49_9;

	sub.f32 	%f91, %f1, %f3;
	add.rn.f32 	%f92, %f91, %f2;
	mul.f32 	%f93, %f3, 0fC0000000;
	mul.f32 	%f94, %f93, %f250;
	fma.rn.f32 	%f250, %f94, %f92, %f250;

$L__BB49_9:
	add.s64 	%rd45, %rd1, %rd42;
	mul.f32 	%f95, %f250, 0f3F000000;
	st.global.f32 	[%rd45], %f95;
	add.s32 	%r59, %r59, %r5;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p15, %rd65, %rd26;
	@%p15 bra 	$L__BB49_7;

$L__BB49_23:
	ret;

}
	// .globl	unormcdf_f64
.visible .entry unormcdf_f64(
	.param .u64 unormcdf_f64_param_0,
	.param .u64 unormcdf_f64_param_1,
	.param .u64 unormcdf_f64_param_2,
	.param .u64 unormcdf_f64_param_3,
	.param .u64 unormcdf_f64_param_4
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<84>;
	.reg .f64 	%fd<301>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd23, [unormcdf_f64_param_0];
	ld.param.u64 	%rd24, [unormcdf_f64_param_1];
	ld.param.u64 	%rd26, [unormcdf_f64_param_2];
	ld.param.u64 	%rd25, [unormcdf_f64_param_3];
	ld.param.u64 	%rd27, [unormcdf_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd25;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p35, %p2;
	@%p3 bra 	$L__BB50_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r78, 0;

$L__BB50_2:
	not.b32 	%r25, %r78;
	cvt.u64.u32 	%rd29, %r25;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd67, %rd34;
	mov.pred 	%p35, -1;
	@%p5 bra 	$L__BB50_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd67, %rd37, %rd67;
	add.s32 	%r78, %r78, 1;
	cvt.u64.u32 	%rd38, %r78;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p35, %p2;
	@%p7 bra 	$L__BB50_2;

$L__BB50_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %tid.x;
	mad.lo.s32 	%r79, %r26, %r3, %r27;
	cvt.u64.u32 	%rd68, %r79;
	@%p35 bra 	$L__BB50_15;
	bra.uni 	$L__BB50_5;

$L__BB50_15:
	setp.ge.u64 	%p20, %rd68, %rd23;
	@%p20 bra 	$L__BB50_30;

	mov.u32 	%r48, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r48;

$L__BB50_17:
	mov.u32 	%r81, 0;
	mov.u32 	%r82, %r79;
	mov.u32 	%r83, %r81;
	@%p3 bra 	$L__BB50_22;

$L__BB50_18:
	not.b32 	%r52, %r81;
	cvt.u64.u32 	%rd43, %r52;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd12, %r82;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p22, %rd47, 0;
	@%p22 bra 	$L__BB50_20;

	div.u64 	%rd70, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd70, %rd14;
	sub.s64 	%rd71, %rd12, %rd48;
	bra.uni 	$L__BB50_21;

$L__BB50_20:
	cvt.u32.u64 	%r53, %rd14;
	cvt.u32.u64 	%r54, %rd12;
	div.u32 	%r55, %r54, %r53;
	mul.lo.s32 	%r56, %r55, %r53;
	sub.s32 	%r57, %r54, %r56;
	cvt.u64.u32 	%rd70, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB50_21:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd71;
	cvt.u32.u64 	%r58, %rd52;
	add.s32 	%r83, %r83, %r58;
	cvt.u32.u64 	%r82, %rd70;
	add.s32 	%r81, %r81, 1;
	cvt.u64.u32 	%rd53, %r81;
	setp.lt.u64 	%p23, %rd53, %rd24;
	@%p23 bra 	$L__BB50_18;

$L__BB50_22:
	ld.param.u64 	%rd64, [unormcdf_f64_param_4];
	cvta.to.global.u64 	%rd63, %rd64;
	mul.wide.u32 	%rd54, %r83, 8;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd68, 3;
	add.s64 	%rd21, %rd63, %rd56;
	setp.eq.s64 	%p24, %rd25, 0;
	selp.b64 	%rd57, %rd21, %rd55, %p24;
	ld.global.f64 	%fd298, [%rd57];
	abs.f64 	%fd151, %fd298;
	setp.leu.f64 	%p25, %fd151, 0d4043400000000000;
	@%p25 bra 	$L__BB50_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd298;
	}
	and.b32  	%r60, %r59, -2147483648;
	mov.f64 	%fd152, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd152;
	}
	and.b32  	%r62, %r61, 2147483647;
	or.b32  	%r63, %r62, %r60;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r64, %temp}, %fd152;
	}
	mov.b64 	%fd298, {%r64, %r63};

$L__BB50_24:
	mov.f64 	%fd153, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd15, %fd298, %fd153;
	neg.f64 	%fd154, %fd15;
	fma.rn.f64 	%fd155, %fd298, %fd153, %fd154;
	mov.f64 	%fd156, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd16, %fd298, %fd156, %fd155;
	add.rn.f64 	%fd17, %fd15, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd17;
	}
	and.b32  	%r21, %r20, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd17;
	}
	setp.lt.u32 	%p26, %r21, 2146435072;
	@%p26 bra 	$L__BB50_26;
	bra.uni 	$L__BB50_25;

$L__BB50_26:
	mov.b64 	%fd159, {%r22, %r21};
	add.f64 	%fd160, %fd159, 0dC010000000000000;
	mov.f64 	%fd161, 0dC010000000000000;
	add.f64 	%fd162, %fd159, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd163, %fd162;
	neg.f64 	%fd164, %fd162;
	mov.f64 	%fd165, 0d3FF0000000000000;
	fma.rn.f64 	%fd166, %fd164, %fd163, %fd165;
	fma.rn.f64 	%fd167, %fd166, %fd166, %fd166;
	fma.rn.f64 	%fd168, %fd167, %fd163, %fd163;
	mul.f64 	%fd169, %fd160, %fd168;
	add.rn.f64 	%fd170, %fd169, %fd165;
	fma.rn.f64 	%fd171, %fd161, %fd170, %fd159;
	neg.f64 	%fd172, %fd169;
	fma.rn.f64 	%fd173, %fd172, %fd159, %fd171;
	fma.rn.f64 	%fd174, %fd168, %fd173, %fd169;
	mov.f64 	%fd175, 0dBE44E1C6FD03D328;
	mov.f64 	%fd176, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd177, %fd176, %fd174, %fd175;
	mov.f64 	%fd178, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd179, %fd177, %fd174, %fd178;
	mov.f64 	%fd180, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd181, %fd179, %fd174, %fd180;
	mov.f64 	%fd182, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd183, %fd181, %fd174, %fd182;
	mov.f64 	%fd184, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd185, %fd183, %fd174, %fd184;
	mov.f64 	%fd186, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd187, %fd185, %fd174, %fd186;
	mov.f64 	%fd188, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd189, %fd187, %fd174, %fd188;
	mov.f64 	%fd190, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd191, %fd189, %fd174, %fd190;
	mov.f64 	%fd192, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd193, %fd191, %fd174, %fd192;
	mov.f64 	%fd194, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd195, %fd193, %fd174, %fd194;
	mov.f64 	%fd196, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd197, %fd195, %fd174, %fd196;
	mov.f64 	%fd198, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd199, %fd197, %fd174, %fd198;
	mov.f64 	%fd200, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd201, %fd199, %fd174, %fd200;
	mov.f64 	%fd202, 0dBF9096238568E357;
	fma.rn.f64 	%fd203, %fd201, %fd174, %fd202;
	mov.f64 	%fd204, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd205, %fd203, %fd174, %fd204;
	mov.f64 	%fd206, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd207, %fd205, %fd174, %fd206;
	mov.f64 	%fd208, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd209, %fd207, %fd174, %fd208;
	mov.f64 	%fd210, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd211, %fd209, %fd174, %fd210;
	mov.f64 	%fd212, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd213, %fd211, %fd174, %fd212;
	mov.f64 	%fd214, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd215, %fd213, %fd174, %fd214;
	mov.f64 	%fd216, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd217, %fd215, %fd174, %fd216;
	mov.f64 	%fd218, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd219, %fd217, %fd174, %fd218;
	mov.f64 	%fd220, 0d4000000000000000;
	fma.rn.f64 	%fd221, %fd220, %fd159, %fd165;
	rcp.approx.ftz.f64 	%fd222, %fd221;
	neg.f64 	%fd223, %fd221;
	fma.rn.f64 	%fd224, %fd223, %fd222, %fd165;
	fma.rn.f64 	%fd225, %fd224, %fd224, %fd224;
	fma.rn.f64 	%fd226, %fd225, %fd222, %fd222;
	mul.f64 	%fd227, %fd219, %fd226;
	mul.f64 	%fd228, %fd227, 0dC000000000000000;
	fma.rn.f64 	%fd229, %fd159, %fd228, %fd219;
	neg.f64 	%fd230, %fd227;
	add.rn.f64 	%fd231, %fd229, %fd230;
	fma.rn.f64 	%fd232, %fd231, %fd226, %fd227;
	mul.f64 	%fd233, %fd159, %fd159;
	neg.f64 	%fd234, %fd233;
	mov.f64 	%fd235, 0d4338000000000000;
	mov.f64 	%fd236, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd237, %fd234, %fd236, %fd235;
	mov.f64 	%fd238, 0dC338000000000000;
	add.rn.f64 	%fd239, %fd237, %fd238;
	mov.f64 	%fd240, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd241, %fd239, %fd240, %fd234;
	mov.f64 	%fd242, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd243, %fd239, %fd242, %fd241;
	mov.f64 	%fd244, 0d3E928AF3FCA213EA;
	mov.f64 	%fd245, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd246, %fd245, %fd243, %fd244;
	mov.f64 	%fd247, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd248, %fd246, %fd243, %fd247;
	mov.f64 	%fd249, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd250, %fd248, %fd243, %fd249;
	mov.f64 	%fd251, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd252, %fd250, %fd243, %fd251;
	mov.f64 	%fd253, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd254, %fd252, %fd243, %fd253;
	mov.f64 	%fd255, 0d3F81111111122322;
	fma.rn.f64 	%fd256, %fd254, %fd243, %fd255;
	mov.f64 	%fd257, 0d3FA55555555502A1;
	fma.rn.f64 	%fd258, %fd256, %fd243, %fd257;
	mov.f64 	%fd259, 0d3FC5555555555511;
	fma.rn.f64 	%fd260, %fd258, %fd243, %fd259;
	mov.f64 	%fd261, 0d3FE000000000000B;
	fma.rn.f64 	%fd262, %fd260, %fd243, %fd261;
	fma.rn.f64 	%fd263, %fd262, %fd243, %fd165;
	fma.rn.f64 	%fd264, %fd263, %fd243, %fd165;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd237;
	}
	shr.u32 	%r66, %r65, 31;
	add.s32 	%r67, %r65, %r66;
	shr.s32 	%r68, %r67, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r69, %temp}, %fd264;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd264;
	}
	shl.b32 	%r71, %r68, 20;
	add.s32 	%r72, %r70, %r71;
	mov.b64 	%fd265, {%r69, %r72};
	sub.s32 	%r73, %r65, %r68;
	shl.b32 	%r74, %r73, 20;
	add.s32 	%r75, %r74, 1072693248;
	mov.u32 	%r76, 0;
	mov.b64 	%fd266, {%r76, %r75};
	mul.f64 	%fd267, %fd265, %fd266;
	neg.f64 	%fd268, %fd159;
	fma.rn.f64 	%fd269, %fd268, %fd159, %fd233;
	fma.rn.f64 	%fd270, %fd267, %fd269, %fd267;
	mul.f64 	%fd271, %fd232, %fd270;
	setp.gt.u32 	%p31, %r21, 1077624832;
	selp.f64 	%fd272, 0d0000000000000000, %fd271, %p31;
	sub.f64 	%fd273, %fd220, %fd272;
	setp.lt.s32 	%p32, %r20, 0;
	selp.f64 	%fd300, %fd273, %fd272, %p32;
	bra.uni 	$L__BB50_27;

$L__BB50_25:
	setp.eq.s32 	%p27, %r21, 2146435072;
	setp.eq.s32 	%p28, %r22, 0;
	and.pred  	%p29, %p27, %p28;
	setp.lt.s32 	%p30, %r20, 0;
	selp.f64 	%fd157, 0d4000000000000000, 0d0000000000000000, %p30;
	add.f64 	%fd158, %fd17, %fd17;
	selp.f64 	%fd300, %fd157, %fd158, %p29;

$L__BB50_27:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd298;
	}
	setp.lt.u32 	%p33, %r77, -1074790399;
	@%p33 bra 	$L__BB50_29;

	mov.f64 	%fd288, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd287, %fd298, %fd288;
	neg.f64 	%fd286, %fd287;
	fma.rn.f64 	%fd285, %fd298, %fd288, %fd286;
	mov.f64 	%fd284, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd283, %fd298, %fd284, %fd285;
	mov.f64 	%fd280, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd279, %fd298, %fd280;
	sub.f64 	%fd274, %fd279, %fd17;
	add.rn.f64 	%fd275, %fd274, %fd283;
	mul.f64 	%fd276, %fd17, 0dC000000000000000;
	mul.f64 	%fd277, %fd276, %fd300;
	fma.rn.f64 	%fd300, %fd277, %fd275, %fd300;

$L__BB50_29:
	ld.param.u64 	%rd62, [unormcdf_f64_param_4];
	shl.b64 	%rd61, %rd68, 3;
	cvta.to.global.u64 	%rd60, %rd62;
	add.s64 	%rd59, %rd60, %rd61;
	ld.param.u64 	%rd58, [unormcdf_f64_param_0];
	mul.f64 	%fd278, %fd300, 0d3FE0000000000000;
	st.global.f64 	[%rd59], %fd278;
	add.s32 	%r79, %r79, %r11;
	cvt.u64.u32 	%rd68, %r79;
	setp.lt.u64 	%p34, %rd68, %rd58;
	@%p34 bra 	$L__BB50_17;
	bra.uni 	$L__BB50_30;

$L__BB50_5:
	setp.ge.u64 	%p8, %rd68, %rd23;
	@%p8 bra 	$L__BB50_30;

	setp.eq.s64 	%p9, %rd25, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r28;

$L__BB50_7:
	shl.b64 	%rd39, %rd68, 3;
	add.s64 	%rd40, %rd8, %rd39;
	ld.global.f64 	%fd295, [%rd40];
	abs.f64 	%fd23, %fd295;
	setp.leu.f64 	%p10, %fd23, 0d4043400000000000;
	@%p10 bra 	$L__BB50_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd295;
	}
	and.b32  	%r30, %r29, -2147483648;
	mov.f64 	%fd24, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd24;
	}
	and.b32  	%r32, %r31, 2147483647;
	or.b32  	%r33, %r32, %r30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd24;
	}
	mov.b64 	%fd295, {%r34, %r33};

$L__BB50_9:
	mov.f64 	%fd25, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd4, %fd295, %fd25;
	neg.f64 	%fd26, %fd4;
	fma.rn.f64 	%fd27, %fd295, %fd25, %fd26;
	mov.f64 	%fd28, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd5, %fd295, %fd28, %fd27;
	add.rn.f64 	%fd6, %fd4, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd6;
	}
	and.b32  	%r8, %r7, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r9, %temp}, %fd6;
	}
	setp.lt.u32 	%p11, %r8, 2146435072;
	@%p11 bra 	$L__BB50_11;
	bra.uni 	$L__BB50_10;

$L__BB50_11:
	mov.b64 	%fd31, {%r9, %r8};
	add.f64 	%fd32, %fd31, 0dC010000000000000;
	mov.f64 	%fd33, 0dC010000000000000;
	add.f64 	%fd34, %fd31, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd35, %fd34;
	neg.f64 	%fd36, %fd34;
	mov.f64 	%fd37, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd35, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd38, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd35, %fd35;
	mul.f64 	%fd41, %fd32, %fd40;
	add.rn.f64 	%fd42, %fd41, %fd37;
	fma.rn.f64 	%fd43, %fd33, %fd42, %fd31;
	neg.f64 	%fd44, %fd41;
	fma.rn.f64 	%fd45, %fd44, %fd31, %fd43;
	fma.rn.f64 	%fd46, %fd40, %fd45, %fd41;
	mov.f64 	%fd47, 0dBE44E1C6FD03D328;
	mov.f64 	%fd48, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd49, %fd48, %fd46, %fd47;
	mov.f64 	%fd50, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd51, %fd49, %fd46, %fd50;
	mov.f64 	%fd52, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd53, %fd51, %fd46, %fd52;
	mov.f64 	%fd54, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd55, %fd53, %fd46, %fd54;
	mov.f64 	%fd56, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd57, %fd55, %fd46, %fd56;
	mov.f64 	%fd58, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd59, %fd57, %fd46, %fd58;
	mov.f64 	%fd60, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd61, %fd59, %fd46, %fd60;
	mov.f64 	%fd62, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd63, %fd61, %fd46, %fd62;
	mov.f64 	%fd64, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd65, %fd63, %fd46, %fd64;
	mov.f64 	%fd66, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd67, %fd65, %fd46, %fd66;
	mov.f64 	%fd68, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd69, %fd67, %fd46, %fd68;
	mov.f64 	%fd70, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd71, %fd69, %fd46, %fd70;
	mov.f64 	%fd72, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd73, %fd71, %fd46, %fd72;
	mov.f64 	%fd74, 0dBF9096238568E357;
	fma.rn.f64 	%fd75, %fd73, %fd46, %fd74;
	mov.f64 	%fd76, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd77, %fd75, %fd46, %fd76;
	mov.f64 	%fd78, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd79, %fd77, %fd46, %fd78;
	mov.f64 	%fd80, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd81, %fd79, %fd46, %fd80;
	mov.f64 	%fd82, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd83, %fd81, %fd46, %fd82;
	mov.f64 	%fd84, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd85, %fd83, %fd46, %fd84;
	mov.f64 	%fd86, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd87, %fd85, %fd46, %fd86;
	mov.f64 	%fd88, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd89, %fd87, %fd46, %fd88;
	mov.f64 	%fd90, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd91, %fd89, %fd46, %fd90;
	mov.f64 	%fd92, 0d4000000000000000;
	fma.rn.f64 	%fd93, %fd92, %fd31, %fd37;
	rcp.approx.ftz.f64 	%fd94, %fd93;
	neg.f64 	%fd95, %fd93;
	fma.rn.f64 	%fd96, %fd95, %fd94, %fd37;
	fma.rn.f64 	%fd97, %fd96, %fd96, %fd96;
	fma.rn.f64 	%fd98, %fd97, %fd94, %fd94;
	mul.f64 	%fd99, %fd91, %fd98;
	mul.f64 	%fd100, %fd99, 0dC000000000000000;
	fma.rn.f64 	%fd101, %fd31, %fd100, %fd91;
	neg.f64 	%fd102, %fd99;
	add.rn.f64 	%fd103, %fd101, %fd102;
	fma.rn.f64 	%fd104, %fd103, %fd98, %fd99;
	mul.f64 	%fd105, %fd31, %fd31;
	neg.f64 	%fd106, %fd105;
	mov.f64 	%fd107, 0d4338000000000000;
	mov.f64 	%fd108, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd109, %fd106, %fd108, %fd107;
	mov.f64 	%fd110, 0dC338000000000000;
	add.rn.f64 	%fd111, %fd109, %fd110;
	mov.f64 	%fd112, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd113, %fd111, %fd112, %fd106;
	mov.f64 	%fd114, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd115, %fd111, %fd114, %fd113;
	mov.f64 	%fd116, 0d3E928AF3FCA213EA;
	mov.f64 	%fd117, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd118, %fd117, %fd115, %fd116;
	mov.f64 	%fd119, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd120, %fd118, %fd115, %fd119;
	mov.f64 	%fd121, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd122, %fd120, %fd115, %fd121;
	mov.f64 	%fd123, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd124, %fd122, %fd115, %fd123;
	mov.f64 	%fd125, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd126, %fd124, %fd115, %fd125;
	mov.f64 	%fd127, 0d3F81111111122322;
	fma.rn.f64 	%fd128, %fd126, %fd115, %fd127;
	mov.f64 	%fd129, 0d3FA55555555502A1;
	fma.rn.f64 	%fd130, %fd128, %fd115, %fd129;
	mov.f64 	%fd131, 0d3FC5555555555511;
	fma.rn.f64 	%fd132, %fd130, %fd115, %fd131;
	mov.f64 	%fd133, 0d3FE000000000000B;
	fma.rn.f64 	%fd134, %fd132, %fd115, %fd133;
	fma.rn.f64 	%fd135, %fd134, %fd115, %fd37;
	fma.rn.f64 	%fd136, %fd135, %fd115, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd109;
	}
	shr.u32 	%r36, %r35, 31;
	add.s32 	%r37, %r35, %r36;
	shr.s32 	%r38, %r37, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd136;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd136;
	}
	shl.b32 	%r41, %r38, 20;
	add.s32 	%r42, %r40, %r41;
	mov.b64 	%fd137, {%r39, %r42};
	sub.s32 	%r43, %r35, %r38;
	shl.b32 	%r44, %r43, 20;
	add.s32 	%r45, %r44, 1072693248;
	mov.u32 	%r46, 0;
	mov.b64 	%fd138, {%r46, %r45};
	mul.f64 	%fd139, %fd137, %fd138;
	neg.f64 	%fd140, %fd31;
	fma.rn.f64 	%fd141, %fd140, %fd31, %fd105;
	fma.rn.f64 	%fd142, %fd139, %fd141, %fd139;
	mul.f64 	%fd143, %fd104, %fd142;
	setp.gt.u32 	%p16, %r8, 1077624832;
	selp.f64 	%fd144, 0d0000000000000000, %fd143, %p16;
	sub.f64 	%fd145, %fd92, %fd144;
	setp.lt.s32 	%p17, %r7, 0;
	selp.f64 	%fd297, %fd145, %fd144, %p17;
	bra.uni 	$L__BB50_12;

$L__BB50_10:
	setp.eq.s32 	%p12, %r8, 2146435072;
	setp.eq.s32 	%p13, %r9, 0;
	and.pred  	%p14, %p12, %p13;
	setp.lt.s32 	%p15, %r7, 0;
	selp.f64 	%fd29, 0d4000000000000000, 0d0000000000000000, %p15;
	add.f64 	%fd30, %fd6, %fd6;
	selp.f64 	%fd297, %fd29, %fd30, %p14;

$L__BB50_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd295;
	}
	setp.lt.u32 	%p18, %r47, -1074790399;
	@%p18 bra 	$L__BB50_14;

	mov.f64 	%fd294, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd293, %fd295, %fd294;
	neg.f64 	%fd292, %fd293;
	fma.rn.f64 	%fd291, %fd295, %fd294, %fd292;
	mov.f64 	%fd290, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd289, %fd295, %fd290, %fd291;
	mov.f64 	%fd282, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd281, %fd295, %fd282;
	sub.f64 	%fd146, %fd281, %fd6;
	add.rn.f64 	%fd147, %fd146, %fd289;
	mul.f64 	%fd148, %fd6, 0dC000000000000000;
	mul.f64 	%fd149, %fd148, %fd297;
	fma.rn.f64 	%fd297, %fd149, %fd147, %fd297;

$L__BB50_14:
	shl.b64 	%rd66, %rd68, 3;
	ld.param.u64 	%rd65, [unormcdf_f64_param_0];
	add.s64 	%rd42, %rd1, %rd66;
	mul.f64 	%fd150, %fd297, 0d3FE0000000000000;
	st.global.f64 	[%rd42], %fd150;
	add.s32 	%r79, %r79, %r5;
	cvt.u64.u32 	%rd68, %r79;
	setp.lt.u64 	%p19, %rd68, %rd65;
	@%p19 bra 	$L__BB50_7;

$L__BB50_30:
	ret;

}
	// .globl	uabs_f32
.visible .entry uabs_f32(
	.param .u64 uabs_f32_param_0,
	.param .u64 uabs_f32_param_1,
	.param .u64 uabs_f32_param_2,
	.param .u64 uabs_f32_param_3,
	.param .u64 uabs_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uabs_f32_param_0];
	ld.param.u64 	%rd27, [uabs_f32_param_1];
	ld.param.u64 	%rd29, [uabs_f32_param_2];
	ld.param.u64 	%rd28, [uabs_f32_param_3];
	ld.param.u64 	%rd30, [uabs_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB51_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB51_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB51_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB51_2;

$L__BB51_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB51_8;
	bra.uni 	$L__BB51_5;

$L__BB51_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB51_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB51_16;

$L__BB51_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB51_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB51_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB51_14;

$L__BB51_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB51_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB51_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	abs.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB51_10;
	bra.uni 	$L__BB51_19;

$L__BB51_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB51_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB51_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	abs.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB51_7;

$L__BB51_19:
	ret;

$L__BB51_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB51_18;

$L__BB51_17:
	ld.global.f32 	%f5, [%rd2];
	abs.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB51_17;
	bra.uni 	$L__BB51_19;

$L__BB51_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	abs.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB51_18;
	bra.uni 	$L__BB51_19;

}
	// .globl	uabs_f64
.visible .entry uabs_f64(
	.param .u64 uabs_f64_param_0,
	.param .u64 uabs_f64_param_1,
	.param .u64 uabs_f64_param_2,
	.param .u64 uabs_f64_param_3,
	.param .u64 uabs_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [uabs_f64_param_0];
	ld.param.u64 	%rd27, [uabs_f64_param_1];
	ld.param.u64 	%rd29, [uabs_f64_param_2];
	ld.param.u64 	%rd28, [uabs_f64_param_3];
	ld.param.u64 	%rd30, [uabs_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB52_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB52_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB52_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB52_2;

$L__BB52_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB52_8;
	bra.uni 	$L__BB52_5;

$L__BB52_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB52_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB52_16;

$L__BB52_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB52_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB52_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB52_14;

$L__BB52_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB52_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB52_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	abs.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB52_10;
	bra.uni 	$L__BB52_19;

$L__BB52_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB52_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB52_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	abs.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB52_7;

$L__BB52_19:
	ret;

$L__BB52_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB52_18;

$L__BB52_17:
	ld.global.f64 	%fd5, [%rd2];
	abs.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB52_17;
	bra.uni 	$L__BB52_19;

$L__BB52_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	abs.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB52_18;
	bra.uni 	$L__BB52_19;

}
	// .globl	usqr_f32
.visible .entry usqr_f32(
	.param .u64 usqr_f32_param_0,
	.param .u64 usqr_f32_param_1,
	.param .u64 usqr_f32_param_2,
	.param .u64 usqr_f32_param_3,
	.param .u64 usqr_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [usqr_f32_param_0];
	ld.param.u64 	%rd27, [usqr_f32_param_1];
	ld.param.u64 	%rd29, [usqr_f32_param_2];
	ld.param.u64 	%rd28, [usqr_f32_param_3];
	ld.param.u64 	%rd30, [usqr_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB53_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB53_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB53_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB53_2;

$L__BB53_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB53_8;
	bra.uni 	$L__BB53_5;

$L__BB53_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB53_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB53_16;

$L__BB53_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB53_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB53_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB53_14;

$L__BB53_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB53_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB53_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	mul.f32 	%f4, %f3, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB53_10;
	bra.uni 	$L__BB53_19;

$L__BB53_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB53_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB53_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	mul.f32 	%f2, %f1, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB53_7;

$L__BB53_19:
	ret;

$L__BB53_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB53_18;

$L__BB53_17:
	ld.global.f32 	%f5, [%rd2];
	mul.f32 	%f6, %f5, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB53_17;
	bra.uni 	$L__BB53_19;

$L__BB53_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	mul.f32 	%f8, %f7, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB53_18;
	bra.uni 	$L__BB53_19;

}
	// .globl	usqr_f64
.visible .entry usqr_f64(
	.param .u64 usqr_f64_param_0,
	.param .u64 usqr_f64_param_1,
	.param .u64 usqr_f64_param_2,
	.param .u64 usqr_f64_param_3,
	.param .u64 usqr_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [usqr_f64_param_0];
	ld.param.u64 	%rd27, [usqr_f64_param_1];
	ld.param.u64 	%rd29, [usqr_f64_param_2];
	ld.param.u64 	%rd28, [usqr_f64_param_3];
	ld.param.u64 	%rd30, [usqr_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB54_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB54_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB54_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB54_2;

$L__BB54_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB54_8;
	bra.uni 	$L__BB54_5;

$L__BB54_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB54_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB54_16;

$L__BB54_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB54_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB54_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB54_14;

$L__BB54_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB54_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB54_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	mul.f64 	%fd4, %fd3, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB54_10;
	bra.uni 	$L__BB54_19;

$L__BB54_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB54_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB54_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	mul.f64 	%fd2, %fd1, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB54_7;

$L__BB54_19:
	ret;

$L__BB54_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB54_18;

$L__BB54_17:
	ld.global.f64 	%fd5, [%rd2];
	mul.f64 	%fd6, %fd5, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB54_17;
	bra.uni 	$L__BB54_19;

$L__BB54_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	mul.f64 	%fd8, %fd7, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB54_18;
	bra.uni 	$L__BB54_19;

}
	// .globl	usqrt_f32
.visible .entry usqrt_f32(
	.param .u64 usqrt_f32_param_0,
	.param .u64 usqrt_f32_param_1,
	.param .u64 usqrt_f32_param_2,
	.param .u64 usqrt_f32_param_3,
	.param .u64 usqrt_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [usqrt_f32_param_0];
	ld.param.u64 	%rd27, [usqrt_f32_param_1];
	ld.param.u64 	%rd29, [usqrt_f32_param_2];
	ld.param.u64 	%rd28, [usqrt_f32_param_3];
	ld.param.u64 	%rd30, [usqrt_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB55_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB55_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB55_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB55_2;

$L__BB55_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB55_8;
	bra.uni 	$L__BB55_5;

$L__BB55_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB55_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB55_16;

$L__BB55_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB55_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB55_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB55_14;

$L__BB55_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB55_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB55_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f3, [%rd60];
	sqrt.rn.f32 	%f4, %f3;
	st.global.f32 	[%rd59], %f4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB55_10;
	bra.uni 	$L__BB55_19;

$L__BB55_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB55_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB55_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	sqrt.rn.f32 	%f2, %f1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB55_7;

$L__BB55_19:
	ret;

$L__BB55_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB55_18;

$L__BB55_17:
	ld.global.f32 	%f5, [%rd2];
	sqrt.rn.f32 	%f6, %f5;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB55_17;
	bra.uni 	$L__BB55_19;

$L__BB55_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f7, [%rd64];
	sqrt.rn.f32 	%f8, %f7;
	st.global.f32 	[%rd64], %f8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB55_18;
	bra.uni 	$L__BB55_19;

}
	// .globl	usqrt_f64
.visible .entry usqrt_f64(
	.param .u64 usqrt_f64_param_0,
	.param .u64 usqrt_f64_param_1,
	.param .u64 usqrt_f64_param_2,
	.param .u64 usqrt_f64_param_3,
	.param .u64 usqrt_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [usqrt_f64_param_0];
	ld.param.u64 	%rd27, [usqrt_f64_param_1];
	ld.param.u64 	%rd29, [usqrt_f64_param_2];
	ld.param.u64 	%rd28, [usqrt_f64_param_3];
	ld.param.u64 	%rd30, [usqrt_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB56_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB56_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB56_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB56_2;

$L__BB56_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB56_8;
	bra.uni 	$L__BB56_5;

$L__BB56_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB56_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB56_16;

$L__BB56_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB56_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB56_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB56_14;

$L__BB56_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB56_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB56_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd3, [%rd60];
	sqrt.rn.f64 	%fd4, %fd3;
	st.global.f64 	[%rd59], %fd4;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB56_10;
	bra.uni 	$L__BB56_19;

$L__BB56_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB56_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB56_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	sqrt.rn.f64 	%fd2, %fd1;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd2;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB56_7;

$L__BB56_19:
	ret;

$L__BB56_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB56_18;

$L__BB56_17:
	ld.global.f64 	%fd5, [%rd2];
	sqrt.rn.f64 	%fd6, %fd5;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB56_17;
	bra.uni 	$L__BB56_19;

$L__BB56_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd7, [%rd64];
	sqrt.rn.f64 	%fd8, %fd7;
	st.global.f64 	[%rd64], %fd8;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB56_18;
	bra.uni 	$L__BB56_19;

}
	// .globl	ugelu_f32
.visible .entry ugelu_f32(
	.param .u64 ugelu_f32_param_0,
	.param .u64 ugelu_f32_param_1,
	.param .u64 ugelu_f32_param_2,
	.param .u64 ugelu_f32_param_3,
	.param .u64 ugelu_f32_param_4
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<94>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [ugelu_f32_param_0];
	ld.param.u64 	%rd27, [ugelu_f32_param_1];
	ld.param.u64 	%rd29, [ugelu_f32_param_2];
	ld.param.u64 	%rd28, [ugelu_f32_param_3];
	ld.param.u64 	%rd30, [ugelu_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p25, %p2;
	@%p3 bra 	$L__BB57_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r46, 0;

$L__BB57_2:
	not.b32 	%r20, %r46;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p25, -1;
	@%p5 bra 	$L__BB57_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd41, %r46;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p25, %p2;
	@%p7 bra 	$L__BB57_2;

$L__BB57_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r47, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r47;
	@%p25 bra 	$L__BB57_11;
	bra.uni 	$L__BB57_5;

$L__BB57_11:
	setp.ge.u64 	%p13, %rd64, %rd26;
	@%p13 bra 	$L__BB57_26;

	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r28;
	@%p3 bra 	$L__BB57_22;

$L__BB57_13:
	mov.u32 	%r49, 0;
	mov.u32 	%r50, %r47;
	mov.u32 	%r51, %r49;

$L__BB57_14:
	not.b32 	%r31, %r49;
	cvt.u64.u32 	%rd46, %r31;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r50;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB57_16;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB57_17;

$L__BB57_16:
	cvt.u32.u64 	%r32, %rd14;
	cvt.u32.u64 	%r33, %rd12;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd66, %r34;
	cvt.u64.u32 	%rd67, %r36;

$L__BB57_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r37, %rd55;
	add.s32 	%r51, %r51, %r37;
	cvt.u32.u64 	%r50, %rd66;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd56, %r49;
	setp.lt.u64 	%p16, %rd56, %rd27;
	@%p16 bra 	$L__BB57_14;

	setp.eq.s64 	%p17, %rd28, 0;
	mul.wide.u32 	%rd57, %r51, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p17;
	ld.global.f32 	%f7, [%rd60];
	mul.f32 	%f43, %f7, %f7;
	mul.f32 	%f44, %f7, %f43;
	fma.rn.f32 	%f45, %f44, 0f3D372713, %f7;
	mul.f32 	%f8, %f45, 0f3F4C422A;
	abs.f32 	%f9, %f8;
	setp.ltu.f32 	%p18, %f9, 0f3F19999A;
	@%p18 bra 	$L__BB57_20;
	bra.uni 	$L__BB57_19;

$L__BB57_20:
	mul.f32 	%f54, %f8, %f8;
	mov.f32 	%f55, 0fBD563CAE;
	mov.f32 	%f56, 0f3C80F082;
	fma.rn.f32 	%f57, %f56, %f54, %f55;
	mov.f32 	%f58, 0f3E085941;
	fma.rn.f32 	%f59, %f57, %f54, %f58;
	mov.f32 	%f60, 0fBEAAA9ED;
	fma.rn.f32 	%f61, %f59, %f54, %f60;
	mov.f32 	%f62, 0f00000000;
	fma.rn.f32 	%f63, %f61, %f54, %f62;
	fma.rn.f32 	%f92, %f63, %f8, %f8;
	bra.uni 	$L__BB57_21;

$L__BB57_19:
	mul.f32 	%f46, %f9, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f47, %f46;
	add.f32 	%f48, %f47, 0f3F800000;
	mov.f32 	%f49, 0f3F800000;
	rcp.approx.ftz.f32 	%f50, %f48;
	mov.f32 	%f51, 0fC0000000;
	fma.rn.f32 	%f52, %f50, %f51, %f49;
	setp.ge.f32 	%p19, %f9, 0f41102CB4;
	selp.f32 	%f53, 0f3F800000, %f52, %p19;
	mov.b32 	%r38, %f53;
	mov.b32 	%r39, %f8;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, %r38;
	mov.b32 	%f92, %r41;

$L__BB57_21:
	add.f32 	%f64, %f92, 0f3F800000;
	mul.f32 	%f65, %f7, 0f3F000000;
	mul.f32 	%f66, %f65, %f64;
	st.global.f32 	[%rd21], %f66;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p20, %rd64, %rd26;
	@%p20 bra 	$L__BB57_13;
	bra.uni 	$L__BB57_26;

$L__BB57_22:
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p21, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p21;
	ld.global.f32 	%f13, [%rd62];
	mul.f32 	%f67, %f13, %f13;
	mul.f32 	%f68, %f13, %f67;
	fma.rn.f32 	%f69, %f68, 0f3D372713, %f13;
	mul.f32 	%f14, %f69, 0f3F4C422A;
	abs.f32 	%f15, %f14;
	setp.ltu.f32 	%p22, %f15, 0f3F19999A;
	@%p22 bra 	$L__BB57_24;
	bra.uni 	$L__BB57_23;

$L__BB57_24:
	mul.f32 	%f78, %f14, %f14;
	mov.f32 	%f79, 0fBD563CAE;
	mov.f32 	%f80, 0f3C80F082;
	fma.rn.f32 	%f81, %f80, %f78, %f79;
	mov.f32 	%f82, 0f3E085941;
	fma.rn.f32 	%f83, %f81, %f78, %f82;
	mov.f32 	%f84, 0fBEAAA9ED;
	fma.rn.f32 	%f85, %f83, %f78, %f84;
	mov.f32 	%f86, 0f00000000;
	fma.rn.f32 	%f87, %f85, %f78, %f86;
	fma.rn.f32 	%f93, %f87, %f14, %f14;
	bra.uni 	$L__BB57_25;

$L__BB57_23:
	mul.f32 	%f70, %f15, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f71, %f70;
	add.f32 	%f72, %f71, 0f3F800000;
	mov.f32 	%f73, 0f3F800000;
	rcp.approx.ftz.f32 	%f74, %f72;
	mov.f32 	%f75, 0fC0000000;
	fma.rn.f32 	%f76, %f74, %f75, %f73;
	setp.ge.f32 	%p23, %f15, 0f41102CB4;
	selp.f32 	%f77, 0f3F800000, %f76, %p23;
	mov.b32 	%r42, %f77;
	mov.b32 	%r43, %f14;
	and.b32  	%r44, %r43, -2147483648;
	or.b32  	%r45, %r44, %r42;
	mov.b32 	%f93, %r45;

$L__BB57_25:
	add.f32 	%f88, %f93, 0f3F800000;
	mul.f32 	%f89, %f13, 0f3F000000;
	mul.f32 	%f90, %f89, %f88;
	st.global.f32 	[%rd24], %f90;
	add.s32 	%r47, %r47, %r8;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB57_22;
	bra.uni 	$L__BB57_26;

$L__BB57_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB57_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB57_7:
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	mul.f32 	%f19, %f1, %f1;
	mul.f32 	%f20, %f1, %f19;
	fma.rn.f32 	%f21, %f20, 0f3D372713, %f1;
	mul.f32 	%f2, %f21, 0f3F4C422A;
	abs.f32 	%f3, %f2;
	setp.ltu.f32 	%p10, %f3, 0f3F19999A;
	@%p10 bra 	$L__BB57_9;
	bra.uni 	$L__BB57_8;

$L__BB57_9:
	mul.f32 	%f30, %f2, %f2;
	mov.f32 	%f31, 0fBD563CAE;
	mov.f32 	%f32, 0f3C80F082;
	fma.rn.f32 	%f33, %f32, %f30, %f31;
	mov.f32 	%f34, 0f3E085941;
	fma.rn.f32 	%f35, %f33, %f30, %f34;
	mov.f32 	%f36, 0fBEAAA9ED;
	fma.rn.f32 	%f37, %f35, %f30, %f36;
	mov.f32 	%f38, 0f00000000;
	fma.rn.f32 	%f39, %f37, %f30, %f38;
	fma.rn.f32 	%f91, %f39, %f2, %f2;
	bra.uni 	$L__BB57_10;

$L__BB57_8:
	mul.f32 	%f22, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f23, %f22;
	add.f32 	%f24, %f23, 0f3F800000;
	mov.f32 	%f25, 0f3F800000;
	rcp.approx.ftz.f32 	%f26, %f24;
	mov.f32 	%f27, 0fC0000000;
	fma.rn.f32 	%f28, %f26, %f27, %f25;
	setp.ge.f32 	%p11, %f3, 0f41102CB4;
	selp.f32 	%f29, 0f3F800000, %f28, %p11;
	mov.b32 	%r24, %f29;
	mov.b32 	%r25, %f2;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, %r24;
	mov.b32 	%f91, %r27;

$L__BB57_10:
	add.f32 	%f40, %f91, 0f3F800000;
	mul.f32 	%f41, %f1, 0f3F000000;
	mul.f32 	%f42, %f41, %f40;
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f42;
	add.s32 	%r47, %r47, %r5;
	cvt.u64.u32 	%rd64, %r47;
	setp.lt.u64 	%p12, %rd64, %rd26;
	@%p12 bra 	$L__BB57_7;

$L__BB57_26:
	ret;

}
	// .globl	ugelu_f64
.visible .entry ugelu_f64(
	.param .u64 ugelu_f64_param_0,
	.param .u64 ugelu_f64_param_1,
	.param .u64 ugelu_f64_param_2,
	.param .u64 ugelu_f64_param_3,
	.param .u64 ugelu_f64_param_4
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<235>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd26, [ugelu_f64_param_0];
	ld.param.u64 	%rd27, [ugelu_f64_param_1];
	ld.param.u64 	%rd29, [ugelu_f64_param_2];
	ld.param.u64 	%rd28, [ugelu_f64_param_3];
	ld.param.u64 	%rd30, [ugelu_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p25, %p2;
	@%p3 bra 	$L__BB58_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r55, 0;

$L__BB58_2:
	not.b32 	%r26, %r55;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd64, %rd37;
	mov.pred 	%p25, -1;
	@%p5 bra 	$L__BB58_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd64, %rd40, %rd64;
	add.s32 	%r55, %r55, 1;
	cvt.u64.u32 	%rd41, %r55;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p25, %p2;
	@%p7 bra 	$L__BB58_2;

$L__BB58_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %tid.x;
	mad.lo.s32 	%r56, %r27, %r3, %r28;
	cvt.u64.u32 	%rd65, %r56;
	@%p25 bra 	$L__BB58_11;
	bra.uni 	$L__BB58_5;

$L__BB58_11:
	setp.ge.u64 	%p13, %rd65, %rd26;
	@%p13 bra 	$L__BB58_26;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r35;
	@%p3 bra 	$L__BB58_22;

$L__BB58_13:
	mov.u32 	%r58, 0;
	mov.u32 	%r59, %r56;
	mov.u32 	%r60, %r58;

$L__BB58_14:
	not.b32 	%r38, %r58;
	cvt.u64.u32 	%rd46, %r38;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r59;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB58_16;

	div.u64 	%rd67, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd67, %rd14;
	sub.s64 	%rd68, %rd12, %rd51;
	bra.uni 	$L__BB58_17;

$L__BB58_16:
	cvt.u32.u64 	%r39, %rd14;
	cvt.u32.u64 	%r40, %rd12;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd67, %r41;
	cvt.u64.u32 	%rd68, %r43;

$L__BB58_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd68;
	cvt.u32.u64 	%r44, %rd55;
	add.s32 	%r60, %r60, %r44;
	cvt.u32.u64 	%r59, %rd67;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd56, %r58;
	setp.lt.u64 	%p16, %rd56, %rd27;
	@%p16 bra 	$L__BB58_14;

	setp.eq.s64 	%p17, %rd28, 0;
	mul.wide.u32 	%rd57, %r60, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd65, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p17;
	ld.global.f64 	%fd7, [%rd60];
	mul.f64 	%fd90, %fd7, %fd7;
	mul.f64 	%fd91, %fd7, %fd90;
	fma.rn.f64 	%fd92, %fd91, 0d3FA6E4E26D4801F7, %fd7;
	mul.f64 	%fd8, %fd92, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd8;
	}
	and.b32  	%r19, %r18, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd8;
	}
	mov.b64 	%fd9, {%r45, %r19};
	setp.ltu.f64 	%p18, %fd9, 0d3FE4F92224DD2F1A;
	@%p18 bra 	$L__BB58_20;
	bra.uni 	$L__BB58_19;

$L__BB58_20:
	mul.f64 	%fd134, %fd8, %fd8;
	mov.f64 	%fd135, 0d3F14359F420AFC3D;
	mov.f64 	%fd136, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd137, %fd136, %fd134, %fd135;
	mov.f64 	%fd138, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd139, %fd137, %fd134, %fd138;
	mov.f64 	%fd140, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd141, %fd139, %fd134, %fd140;
	mov.f64 	%fd142, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd143, %fd141, %fd134, %fd142;
	mov.f64 	%fd144, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd145, %fd143, %fd134, %fd144;
	mov.f64 	%fd146, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd147, %fd145, %fd134, %fd146;
	mov.f64 	%fd148, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd149, %fd147, %fd134, %fd148;
	mov.f64 	%fd150, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd151, %fd149, %fd134, %fd150;
	mov.f64 	%fd152, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd153, %fd151, %fd134, %fd152;
	mov.f64 	%fd154, 0dBFD5555555555550;
	fma.rn.f64 	%fd155, %fd153, %fd134, %fd154;
	mov.f64 	%fd156, 0d0000000000000000;
	fma.rn.f64 	%fd157, %fd155, %fd134, %fd156;
	fma.rn.f64 	%fd233, %fd157, %fd8, %fd8;
	bra.uni 	$L__BB58_21;

$L__BB58_19:
	add.f64 	%fd93, %fd9, %fd9;
	mov.f64 	%fd94, 0d4000000000000000;
	cvt.rn.f32.f64 	%f5, %fd93;
	mul.f32 	%f6, %f5, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f7, %f6;
	cvt.f64.f32 	%fd95, %f7;
	neg.f64 	%fd96, %fd95;
	mov.f64 	%fd97, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd98, %fd96, %fd97, %fd93;
	mov.f64 	%fd99, 0d3E928A27F89B6999;
	mov.f64 	%fd100, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd101, %fd100, %fd98, %fd99;
	mov.f64 	%fd102, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd103, %fd101, %fd98, %fd102;
	mov.f64 	%fd104, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd105, %fd103, %fd98, %fd104;
	mov.f64 	%fd106, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd107, %fd105, %fd98, %fd106;
	mov.f64 	%fd108, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd109, %fd107, %fd98, %fd108;
	mov.f64 	%fd110, 0d3F811111111173C4;
	fma.rn.f64 	%fd111, %fd109, %fd98, %fd110;
	mov.f64 	%fd112, 0d3FA555555555211A;
	fma.rn.f64 	%fd113, %fd111, %fd98, %fd112;
	mov.f64 	%fd114, 0d3FC5555555555540;
	fma.rn.f64 	%fd115, %fd113, %fd98, %fd114;
	mov.f64 	%fd116, 0d3FE0000000000005;
	fma.rn.f64 	%fd117, %fd115, %fd98, %fd116;
	mul.f64 	%fd118, %fd98, %fd117;
	fma.rn.f64 	%fd119, %fd118, %fd98, %fd98;
	ex2.approx.ftz.f32 	%f8, %f7;
	cvt.f64.f32 	%fd120, %f8;
	mov.f64 	%fd121, 0d3FF0000000000000;
	sub.f64 	%fd122, %fd121, %fd120;
	neg.f64 	%fd123, %fd119;
	fma.rn.f64 	%fd124, %fd123, %fd120, %fd122;
	sub.f64 	%fd125, %fd94, %fd124;
	rcp.approx.ftz.f64 	%fd126, %fd125;
	neg.f64 	%fd127, %fd125;
	fma.rn.f64 	%fd128, %fd127, %fd126, %fd121;
	fma.rn.f64 	%fd129, %fd128, %fd128, %fd128;
	fma.rn.f64 	%fd130, %fd129, %fd126, %fd126;
	neg.f64 	%fd131, %fd130;
	fma.rn.f64 	%fd132, %fd94, %fd131, %fd121;
	setp.gt.u32 	%p19, %r19, 1077088193;
	selp.f64 	%fd133, 0d3FF0000000000000, %fd132, %p19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd133;
	}
	and.b32  	%r48, %r18, -2147483648;
	or.b32  	%r49, %r47, %r48;
	mov.b64 	%fd233, {%r46, %r49};

$L__BB58_21:
	ld.param.u64 	%rd63, [ugelu_f64_param_0];
	add.f64 	%fd158, %fd233, 0d3FF0000000000000;
	mul.f64 	%fd159, %fd7, 0d3FE0000000000000;
	mul.f64 	%fd160, %fd159, %fd158;
	st.global.f64 	[%rd21], %fd160;
	add.s32 	%r56, %r56, %r10;
	cvt.u64.u32 	%rd65, %r56;
	setp.lt.u64 	%p20, %rd65, %rd63;
	@%p20 bra 	$L__BB58_13;
	bra.uni 	$L__BB58_26;

$L__BB58_22:
	shl.b64 	%rd61, %rd65, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p21, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p21;
	ld.global.f64 	%fd13, [%rd62];
	mul.f64 	%fd161, %fd13, %fd13;
	mul.f64 	%fd162, %fd13, %fd161;
	fma.rn.f64 	%fd163, %fd162, 0d3FA6E4E26D4801F7, %fd13;
	mul.f64 	%fd14, %fd163, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd14;
	}
	and.b32  	%r23, %r22, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd14;
	}
	mov.b64 	%fd15, {%r50, %r23};
	setp.ltu.f64 	%p22, %fd15, 0d3FE4F92224DD2F1A;
	@%p22 bra 	$L__BB58_24;
	bra.uni 	$L__BB58_23;

$L__BB58_24:
	mul.f64 	%fd205, %fd14, %fd14;
	mov.f64 	%fd206, 0d3F14359F420AFC3D;
	mov.f64 	%fd207, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd208, %fd207, %fd205, %fd206;
	mov.f64 	%fd209, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd210, %fd208, %fd205, %fd209;
	mov.f64 	%fd211, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd212, %fd210, %fd205, %fd211;
	mov.f64 	%fd213, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd214, %fd212, %fd205, %fd213;
	mov.f64 	%fd215, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd216, %fd214, %fd205, %fd215;
	mov.f64 	%fd217, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd218, %fd216, %fd205, %fd217;
	mov.f64 	%fd219, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd220, %fd218, %fd205, %fd219;
	mov.f64 	%fd221, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd222, %fd220, %fd205, %fd221;
	mov.f64 	%fd223, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd224, %fd222, %fd205, %fd223;
	mov.f64 	%fd225, 0dBFD5555555555550;
	fma.rn.f64 	%fd226, %fd224, %fd205, %fd225;
	mov.f64 	%fd227, 0d0000000000000000;
	fma.rn.f64 	%fd228, %fd226, %fd205, %fd227;
	fma.rn.f64 	%fd234, %fd228, %fd14, %fd14;
	bra.uni 	$L__BB58_25;

$L__BB58_23:
	add.f64 	%fd164, %fd15, %fd15;
	mov.f64 	%fd165, 0d4000000000000000;
	cvt.rn.f32.f64 	%f9, %fd164;
	mul.f32 	%f10, %f9, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f11, %f10;
	cvt.f64.f32 	%fd166, %f11;
	neg.f64 	%fd167, %fd166;
	mov.f64 	%fd168, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd169, %fd167, %fd168, %fd164;
	mov.f64 	%fd170, 0d3E928A27F89B6999;
	mov.f64 	%fd171, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd172, %fd171, %fd169, %fd170;
	mov.f64 	%fd173, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd174, %fd172, %fd169, %fd173;
	mov.f64 	%fd175, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd176, %fd174, %fd169, %fd175;
	mov.f64 	%fd177, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd178, %fd176, %fd169, %fd177;
	mov.f64 	%fd179, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd180, %fd178, %fd169, %fd179;
	mov.f64 	%fd181, 0d3F811111111173C4;
	fma.rn.f64 	%fd182, %fd180, %fd169, %fd181;
	mov.f64 	%fd183, 0d3FA555555555211A;
	fma.rn.f64 	%fd184, %fd182, %fd169, %fd183;
	mov.f64 	%fd185, 0d3FC5555555555540;
	fma.rn.f64 	%fd186, %fd184, %fd169, %fd185;
	mov.f64 	%fd187, 0d3FE0000000000005;
	fma.rn.f64 	%fd188, %fd186, %fd169, %fd187;
	mul.f64 	%fd189, %fd169, %fd188;
	fma.rn.f64 	%fd190, %fd189, %fd169, %fd169;
	ex2.approx.ftz.f32 	%f12, %f11;
	cvt.f64.f32 	%fd191, %f12;
	mov.f64 	%fd192, 0d3FF0000000000000;
	sub.f64 	%fd193, %fd192, %fd191;
	neg.f64 	%fd194, %fd190;
	fma.rn.f64 	%fd195, %fd194, %fd191, %fd193;
	sub.f64 	%fd196, %fd165, %fd195;
	rcp.approx.ftz.f64 	%fd197, %fd196;
	neg.f64 	%fd198, %fd196;
	fma.rn.f64 	%fd199, %fd198, %fd197, %fd192;
	fma.rn.f64 	%fd200, %fd199, %fd199, %fd199;
	fma.rn.f64 	%fd201, %fd200, %fd197, %fd197;
	neg.f64 	%fd202, %fd201;
	fma.rn.f64 	%fd203, %fd165, %fd202, %fd192;
	setp.gt.u32 	%p23, %r23, 1077088193;
	selp.f64 	%fd204, 0d3FF0000000000000, %fd203, %p23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd204;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd204;
	}
	and.b32  	%r53, %r22, -2147483648;
	or.b32  	%r54, %r52, %r53;
	mov.b64 	%fd234, {%r51, %r54};

$L__BB58_25:
	add.f64 	%fd229, %fd234, 0d3FF0000000000000;
	mul.f64 	%fd230, %fd13, 0d3FE0000000000000;
	mul.f64 	%fd231, %fd230, %fd229;
	st.global.f64 	[%rd24], %fd231;
	add.s32 	%r56, %r56, %r10;
	cvt.u64.u32 	%rd65, %r56;
	setp.lt.u64 	%p24, %rd65, %rd26;
	@%p24 bra 	$L__BB58_22;
	bra.uni 	$L__BB58_26;

$L__BB58_5:
	setp.ge.u64 	%p8, %rd65, %rd26;
	@%p8 bra 	$L__BB58_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r29;

$L__BB58_7:
	shl.b64 	%rd42, %rd65, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	mul.f64 	%fd19, %fd1, %fd1;
	mul.f64 	%fd20, %fd1, %fd19;
	fma.rn.f64 	%fd21, %fd20, 0d3FA6E4E26D4801F7, %fd1;
	mul.f64 	%fd2, %fd21, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd2;
	}
	and.b32  	%r8, %r7, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd2;
	}
	mov.b64 	%fd3, {%r30, %r8};
	setp.ltu.f64 	%p10, %fd3, 0d3FE4F92224DD2F1A;
	@%p10 bra 	$L__BB58_9;
	bra.uni 	$L__BB58_8;

$L__BB58_9:
	mul.f64 	%fd63, %fd2, %fd2;
	mov.f64 	%fd64, 0d3F14359F420AFC3D;
	mov.f64 	%fd65, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	mov.f64 	%fd79, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd80, %fd78, %fd63, %fd79;
	mov.f64 	%fd81, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd82, %fd80, %fd63, %fd81;
	mov.f64 	%fd83, 0dBFD5555555555550;
	fma.rn.f64 	%fd84, %fd82, %fd63, %fd83;
	mov.f64 	%fd85, 0d0000000000000000;
	fma.rn.f64 	%fd86, %fd84, %fd63, %fd85;
	fma.rn.f64 	%fd232, %fd86, %fd2, %fd2;
	bra.uni 	$L__BB58_10;

$L__BB58_8:
	add.f64 	%fd22, %fd3, %fd3;
	mov.f64 	%fd23, 0d4000000000000000;
	cvt.rn.f32.f64 	%f1, %fd22;
	mul.f32 	%f2, %f1, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f3, %f2;
	cvt.f64.f32 	%fd24, %f3;
	neg.f64 	%fd25, %fd24;
	mov.f64 	%fd26, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd27, %fd25, %fd26, %fd22;
	mov.f64 	%fd28, 0d3E928A27F89B6999;
	mov.f64 	%fd29, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F811111111173C4;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	mov.f64 	%fd41, 0d3FA555555555211A;
	fma.rn.f64 	%fd42, %fd40, %fd27, %fd41;
	mov.f64 	%fd43, 0d3FC5555555555540;
	fma.rn.f64 	%fd44, %fd42, %fd27, %fd43;
	mov.f64 	%fd45, 0d3FE0000000000005;
	fma.rn.f64 	%fd46, %fd44, %fd27, %fd45;
	mul.f64 	%fd47, %fd27, %fd46;
	fma.rn.f64 	%fd48, %fd47, %fd27, %fd27;
	ex2.approx.ftz.f32 	%f4, %f3;
	cvt.f64.f32 	%fd49, %f4;
	mov.f64 	%fd50, 0d3FF0000000000000;
	sub.f64 	%fd51, %fd50, %fd49;
	neg.f64 	%fd52, %fd48;
	fma.rn.f64 	%fd53, %fd52, %fd49, %fd51;
	sub.f64 	%fd54, %fd23, %fd53;
	rcp.approx.ftz.f64 	%fd55, %fd54;
	neg.f64 	%fd56, %fd54;
	fma.rn.f64 	%fd57, %fd56, %fd55, %fd50;
	fma.rn.f64 	%fd58, %fd57, %fd57, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd55, %fd55;
	neg.f64 	%fd60, %fd59;
	fma.rn.f64 	%fd61, %fd23, %fd60, %fd50;
	setp.gt.u32 	%p11, %r8, 1077088193;
	selp.f64 	%fd62, 0d3FF0000000000000, %fd61, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd62;
	}
	and.b32  	%r33, %r7, -2147483648;
	or.b32  	%r34, %r32, %r33;
	mov.b64 	%fd232, {%r31, %r34};

$L__BB58_10:
	add.f64 	%fd87, %fd232, 0d3FF0000000000000;
	mul.f64 	%fd88, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd89, %fd88, %fd87;
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd89;
	add.s32 	%r56, %r56, %r5;
	cvt.u64.u32 	%rd65, %r56;
	setp.lt.u64 	%p12, %rd65, %rd26;
	@%p12 bra 	$L__BB58_7;

$L__BB58_26:
	ret;

}
	// .globl	ugelu_erf_f32
.visible .entry ugelu_erf_f32(
	.param .u64 ugelu_erf_f32_param_0,
	.param .u64 ugelu_erf_f32_param_1,
	.param .u64 ugelu_erf_f32_param_2,
	.param .u64 ugelu_erf_f32_param_3,
	.param .u64 ugelu_erf_f32_param_4
)
{
	.reg .pred 	%p<35>;
	.reg .f32 	%f<256>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd26, [ugelu_erf_f32_param_0];
	ld.param.u64 	%rd27, [ugelu_erf_f32_param_1];
	ld.param.u64 	%rd29, [ugelu_erf_f32_param_2];
	ld.param.u64 	%rd28, [ugelu_erf_f32_param_3];
	ld.param.u64 	%rd30, [ugelu_erf_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p34, %p2;
	@%p3 bra 	$L__BB59_4;

	mov.u64 	%rd64, 1;
	mov.u32 	%r58, 0;

$L__BB59_2:
	not.b32 	%r20, %r58;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd64, %rd37;
	mov.pred 	%p34, -1;
	@%p5 bra 	$L__BB59_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd64, %rd40, %rd64;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd41, %r58;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p34, %p2;
	@%p7 bra 	$L__BB59_2;

$L__BB59_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r59, %r21, %r3, %r22;
	cvt.u64.u32 	%rd65, %r59;
	@%p34 bra 	$L__BB59_10;
	bra.uni 	$L__BB59_5;

$L__BB59_10:
	setp.ge.u64 	%p16, %rd65, %rd26;
	@%p16 bra 	$L__BB59_23;

	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r32;
	@%p3 bra 	$L__BB59_20;

$L__BB59_12:
	mov.u32 	%r61, 0;
	mov.u32 	%r62, %r59;
	mov.u32 	%r63, %r61;

$L__BB59_13:
	not.b32 	%r35, %r61;
	cvt.u64.u32 	%rd46, %r35;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r62;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p18, %rd50, 0;
	@%p18 bra 	$L__BB59_15;

	div.u64 	%rd67, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd67, %rd14;
	sub.s64 	%rd68, %rd12, %rd51;
	bra.uni 	$L__BB59_16;

$L__BB59_15:
	cvt.u32.u64 	%r36, %rd14;
	cvt.u32.u64 	%r37, %rd12;
	div.u32 	%r38, %r37, %r36;
	mul.lo.s32 	%r39, %r38, %r36;
	sub.s32 	%r40, %r37, %r39;
	cvt.u64.u32 	%rd67, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB59_16:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd68;
	cvt.u32.u64 	%r41, %rd55;
	add.s32 	%r63, %r63, %r41;
	cvt.u32.u64 	%r62, %rd67;
	add.s32 	%r61, %r61, 1;
	cvt.u64.u32 	%rd56, %r61;
	setp.lt.u64 	%p19, %rd56, %rd27;
	@%p19 bra 	$L__BB59_13;

	setp.eq.s64 	%p20, %rd28, 0;
	mul.wide.u32 	%rd57, %r63, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd65, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p20;
	ld.global.f32 	%f8, [%rd60];
	abs.f32 	%f99, %f8;
	setp.gt.f32 	%p21, %f99, 0f41680000;
	mov.b32 	%r42, %f8;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1097334784;
	mov.b32 	%f100, %r44;
	selp.f32 	%f101, %f100, %f8, %p21;
	mov.f32 	%f102, 0fBF3504F3;
	mul.rn.f32 	%f9, %f101, %f102;
	neg.f32 	%f103, %f9;
	fma.rn.f32 	%f104, %f101, %f102, %f103;
	mov.f32 	%f105, 0fB24FE77A;
	fma.rn.f32 	%f10, %f101, %f105, %f104;
	add.rn.f32 	%f11, %f9, %f10;
	abs.f32 	%f106, %f11;
	add.f32 	%f107, %f106, 0fC0800000;
	mov.f32 	%f108, 0fC0800000;
	add.f32 	%f109, %f106, 0f40800000;
	rcp.approx.ftz.f32 	%f110, %f109;
	mul.rn.f32 	%f111, %f107, %f110;
	add.f32 	%f112, %f111, 0f3F800000;
	mov.f32 	%f113, 0f3F800000;
	fma.rn.f32 	%f114, %f108, %f112, %f106;
	neg.f32 	%f115, %f111;
	fma.rn.f32 	%f116, %f115, %f106, %f114;
	fma.rn.f32 	%f117, %f110, %f116, %f111;
	mov.f32 	%f118, 0f3BE6E05B;
	mov.f32 	%f119, 0f3A69A091;
	fma.rn.f32 	%f120, %f119, %f117, %f118;
	mov.f32 	%f121, 0fBC81FB4B;
	fma.rn.f32 	%f122, %f120, %f117, %f121;
	mov.f32 	%f123, 0f3D15373B;
	fma.rn.f32 	%f124, %f122, %f117, %f123;
	mov.f32 	%f125, 0fBD887C5A;
	fma.rn.f32 	%f126, %f124, %f117, %f125;
	mov.f32 	%f127, 0f3DC021D5;
	fma.rn.f32 	%f128, %f126, %f117, %f127;
	mov.f32 	%f129, 0fBDCED424;
	fma.rn.f32 	%f130, %f128, %f117, %f129;
	mov.f32 	%f131, 0f3D8B74DE;
	fma.rn.f32 	%f132, %f130, %f117, %f131;
	mov.f32 	%f133, 0f3C7BF170;
	fma.rn.f32 	%f134, %f132, %f117, %f133;
	mov.f32 	%f135, 0fBE0EF8D4;
	fma.rn.f32 	%f136, %f134, %f117, %f135;
	mov.f32 	%f137, 0f3F9DD2C9;
	fma.rn.f32 	%f138, %f136, %f117, %f137;
	mov.f32 	%f139, 0f40000000;
	fma.rn.f32 	%f140, %f139, %f106, %f113;
	rcp.approx.ftz.f32 	%f141, %f140;
	mul.rn.f32 	%f142, %f138, %f141;
	mul.f32 	%f143, %f142, 0fC0000000;
	fma.rn.f32 	%f144, %f106, %f143, %f138;
	sub.f32 	%f145, %f144, %f142;
	fma.rn.f32 	%f146, %f145, %f141, %f142;
	mul.f32 	%f147, %f106, %f106;
	neg.f32 	%f148, %f147;
	mov.f32 	%f149, 0f3FB8AA3B;
	mul.rn.f32 	%f150, %f148, %f149;
	cvt.rzi.f32.f32 	%f151, %f150;
	abs.f32 	%f152, %f151;
	setp.gt.f32 	%p22, %f152, 0f42FC0000;
	mov.b32 	%r45, %f151;
	and.b32  	%r46, %r45, -2147483648;
	or.b32  	%r47, %r46, 1123811328;
	mov.b32 	%f153, %r47;
	selp.f32 	%f154, %f153, %f151, %p22;
	mov.f32 	%f155, 0fBF317218;
	fma.rn.f32 	%f156, %f154, %f155, %f148;
	mov.f32 	%f157, 0f3102E308;
	fma.rn.f32 	%f158, %f154, %f157, %f156;
	mul.f32 	%f159, %f158, 0f3FB8AA3B;
	add.f32 	%f160, %f154, 0f4B40007F;
	mov.b32 	%r48, %f160;
	shl.b32 	%r49, %r48, 23;
	mov.b32 	%f161, %r49;
	ex2.approx.ftz.f32 	%f162, %f159;
	mul.f32 	%f163, %f162, %f161;
	neg.f32 	%f164, %f106;
	fma.rn.f32 	%f165, %f164, %f106, %f147;
	fma.rn.f32 	%f166, %f163, %f165, %f163;
	mul.f32 	%f167, %f146, %f166;
	setp.gt.f32 	%p23, %f106, 0f4120E148;
	selp.f32 	%f168, 0f00000000, %f167, %p23;
	setp.lt.f32 	%p24, %f11, 0f00000000;
	sub.f32 	%f169, %f139, %f168;
	selp.f32 	%f254, %f169, %f168, %p24;
	setp.geu.f32 	%p25, %f101, 0fBF800000;
	@%p25 bra 	$L__BB59_19;

	sub.f32 	%f170, %f9, %f11;
	add.rn.f32 	%f171, %f170, %f10;
	mul.f32 	%f172, %f11, 0fC0000000;
	mul.f32 	%f173, %f172, %f254;
	fma.rn.f32 	%f254, %f173, %f171, %f254;

$L__BB59_19:
	ld.param.u64 	%rd63, [ugelu_erf_f32_param_0];
	mul.f32 	%f174, %f254, 0f3F000000;
	mul.f32 	%f175, %f8, %f174;
	st.global.f32 	[%rd21], %f175;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p26, %rd65, %rd63;
	@%p26 bra 	$L__BB59_12;
	bra.uni 	$L__BB59_23;

$L__BB59_20:
	shl.b64 	%rd61, %rd65, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p27, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p27;
	ld.global.f32 	%f15, [%rd62];
	abs.f32 	%f176, %f15;
	setp.gt.f32 	%p28, %f176, 0f41680000;
	mov.b32 	%r50, %f15;
	and.b32  	%r51, %r50, -2147483648;
	or.b32  	%r52, %r51, 1097334784;
	mov.b32 	%f177, %r52;
	selp.f32 	%f178, %f177, %f15, %p28;
	mov.f32 	%f179, 0fBF3504F3;
	mul.rn.f32 	%f16, %f178, %f179;
	neg.f32 	%f180, %f16;
	fma.rn.f32 	%f181, %f178, %f179, %f180;
	mov.f32 	%f182, 0fB24FE77A;
	fma.rn.f32 	%f17, %f178, %f182, %f181;
	add.rn.f32 	%f18, %f16, %f17;
	abs.f32 	%f183, %f18;
	add.f32 	%f184, %f183, 0fC0800000;
	mov.f32 	%f185, 0fC0800000;
	add.f32 	%f186, %f183, 0f40800000;
	rcp.approx.ftz.f32 	%f187, %f186;
	mul.rn.f32 	%f188, %f184, %f187;
	add.f32 	%f189, %f188, 0f3F800000;
	mov.f32 	%f190, 0f3F800000;
	fma.rn.f32 	%f191, %f185, %f189, %f183;
	neg.f32 	%f192, %f188;
	fma.rn.f32 	%f193, %f192, %f183, %f191;
	fma.rn.f32 	%f194, %f187, %f193, %f188;
	mov.f32 	%f195, 0f3BE6E05B;
	mov.f32 	%f196, 0f3A69A091;
	fma.rn.f32 	%f197, %f196, %f194, %f195;
	mov.f32 	%f198, 0fBC81FB4B;
	fma.rn.f32 	%f199, %f197, %f194, %f198;
	mov.f32 	%f200, 0f3D15373B;
	fma.rn.f32 	%f201, %f199, %f194, %f200;
	mov.f32 	%f202, 0fBD887C5A;
	fma.rn.f32 	%f203, %f201, %f194, %f202;
	mov.f32 	%f204, 0f3DC021D5;
	fma.rn.f32 	%f205, %f203, %f194, %f204;
	mov.f32 	%f206, 0fBDCED424;
	fma.rn.f32 	%f207, %f205, %f194, %f206;
	mov.f32 	%f208, 0f3D8B74DE;
	fma.rn.f32 	%f209, %f207, %f194, %f208;
	mov.f32 	%f210, 0f3C7BF170;
	fma.rn.f32 	%f211, %f209, %f194, %f210;
	mov.f32 	%f212, 0fBE0EF8D4;
	fma.rn.f32 	%f213, %f211, %f194, %f212;
	mov.f32 	%f214, 0f3F9DD2C9;
	fma.rn.f32 	%f215, %f213, %f194, %f214;
	mov.f32 	%f216, 0f40000000;
	fma.rn.f32 	%f217, %f216, %f183, %f190;
	rcp.approx.ftz.f32 	%f218, %f217;
	mul.rn.f32 	%f219, %f215, %f218;
	mul.f32 	%f220, %f219, 0fC0000000;
	fma.rn.f32 	%f221, %f183, %f220, %f215;
	sub.f32 	%f222, %f221, %f219;
	fma.rn.f32 	%f223, %f222, %f218, %f219;
	mul.f32 	%f224, %f183, %f183;
	neg.f32 	%f225, %f224;
	mov.f32 	%f226, 0f3FB8AA3B;
	mul.rn.f32 	%f227, %f225, %f226;
	cvt.rzi.f32.f32 	%f228, %f227;
	abs.f32 	%f229, %f228;
	setp.gt.f32 	%p29, %f229, 0f42FC0000;
	mov.b32 	%r53, %f228;
	and.b32  	%r54, %r53, -2147483648;
	or.b32  	%r55, %r54, 1123811328;
	mov.b32 	%f230, %r55;
	selp.f32 	%f231, %f230, %f228, %p29;
	mov.f32 	%f232, 0fBF317218;
	fma.rn.f32 	%f233, %f231, %f232, %f225;
	mov.f32 	%f234, 0f3102E308;
	fma.rn.f32 	%f235, %f231, %f234, %f233;
	mul.f32 	%f236, %f235, 0f3FB8AA3B;
	add.f32 	%f237, %f231, 0f4B40007F;
	mov.b32 	%r56, %f237;
	shl.b32 	%r57, %r56, 23;
	mov.b32 	%f238, %r57;
	ex2.approx.ftz.f32 	%f239, %f236;
	mul.f32 	%f240, %f239, %f238;
	neg.f32 	%f241, %f183;
	fma.rn.f32 	%f242, %f241, %f183, %f224;
	fma.rn.f32 	%f243, %f240, %f242, %f240;
	mul.f32 	%f244, %f223, %f243;
	setp.gt.f32 	%p30, %f183, 0f4120E148;
	selp.f32 	%f245, 0f00000000, %f244, %p30;
	setp.lt.f32 	%p31, %f18, 0f00000000;
	sub.f32 	%f246, %f216, %f245;
	selp.f32 	%f255, %f246, %f245, %p31;
	setp.geu.f32 	%p32, %f178, 0fBF800000;
	@%p32 bra 	$L__BB59_22;

	sub.f32 	%f247, %f16, %f18;
	add.rn.f32 	%f248, %f247, %f17;
	mul.f32 	%f249, %f18, 0fC0000000;
	mul.f32 	%f250, %f249, %f255;
	fma.rn.f32 	%f255, %f250, %f248, %f255;

$L__BB59_22:
	mul.f32 	%f251, %f255, 0f3F000000;
	mul.f32 	%f252, %f15, %f251;
	st.global.f32 	[%rd24], %f252;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p33, %rd65, %rd26;
	@%p33 bra 	$L__BB59_20;
	bra.uni 	$L__BB59_23;

$L__BB59_5:
	setp.ge.u64 	%p8, %rd65, %rd26;
	@%p8 bra 	$L__BB59_23;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB59_7:
	shl.b64 	%rd42, %rd65, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	abs.f32 	%f22, %f1;
	setp.gt.f32 	%p10, %f22, 0f41680000;
	mov.b32 	%r24, %f1;
	and.b32  	%r25, %r24, -2147483648;
	or.b32  	%r26, %r25, 1097334784;
	mov.b32 	%f23, %r26;
	selp.f32 	%f24, %f23, %f1, %p10;
	mov.f32 	%f25, 0fBF3504F3;
	mul.rn.f32 	%f2, %f24, %f25;
	neg.f32 	%f26, %f2;
	fma.rn.f32 	%f27, %f24, %f25, %f26;
	mov.f32 	%f28, 0fB24FE77A;
	fma.rn.f32 	%f3, %f24, %f28, %f27;
	add.rn.f32 	%f4, %f2, %f3;
	abs.f32 	%f29, %f4;
	add.f32 	%f30, %f29, 0fC0800000;
	mov.f32 	%f31, 0fC0800000;
	add.f32 	%f32, %f29, 0f40800000;
	rcp.approx.ftz.f32 	%f33, %f32;
	mul.rn.f32 	%f34, %f30, %f33;
	add.f32 	%f35, %f34, 0f3F800000;
	mov.f32 	%f36, 0f3F800000;
	fma.rn.f32 	%f37, %f31, %f35, %f29;
	neg.f32 	%f38, %f34;
	fma.rn.f32 	%f39, %f38, %f29, %f37;
	fma.rn.f32 	%f40, %f33, %f39, %f34;
	mov.f32 	%f41, 0f3BE6E05B;
	mov.f32 	%f42, 0f3A69A091;
	fma.rn.f32 	%f43, %f42, %f40, %f41;
	mov.f32 	%f44, 0fBC81FB4B;
	fma.rn.f32 	%f45, %f43, %f40, %f44;
	mov.f32 	%f46, 0f3D15373B;
	fma.rn.f32 	%f47, %f45, %f40, %f46;
	mov.f32 	%f48, 0fBD887C5A;
	fma.rn.f32 	%f49, %f47, %f40, %f48;
	mov.f32 	%f50, 0f3DC021D5;
	fma.rn.f32 	%f51, %f49, %f40, %f50;
	mov.f32 	%f52, 0fBDCED424;
	fma.rn.f32 	%f53, %f51, %f40, %f52;
	mov.f32 	%f54, 0f3D8B74DE;
	fma.rn.f32 	%f55, %f53, %f40, %f54;
	mov.f32 	%f56, 0f3C7BF170;
	fma.rn.f32 	%f57, %f55, %f40, %f56;
	mov.f32 	%f58, 0fBE0EF8D4;
	fma.rn.f32 	%f59, %f57, %f40, %f58;
	mov.f32 	%f60, 0f3F9DD2C9;
	fma.rn.f32 	%f61, %f59, %f40, %f60;
	mov.f32 	%f62, 0f40000000;
	fma.rn.f32 	%f63, %f62, %f29, %f36;
	rcp.approx.ftz.f32 	%f64, %f63;
	mul.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, 0fC0000000;
	fma.rn.f32 	%f67, %f29, %f66, %f61;
	sub.f32 	%f68, %f67, %f65;
	fma.rn.f32 	%f69, %f68, %f64, %f65;
	mul.f32 	%f70, %f29, %f29;
	neg.f32 	%f71, %f70;
	mov.f32 	%f72, 0f3FB8AA3B;
	mul.rn.f32 	%f73, %f71, %f72;
	cvt.rzi.f32.f32 	%f74, %f73;
	abs.f32 	%f75, %f74;
	setp.gt.f32 	%p11, %f75, 0f42FC0000;
	mov.b32 	%r27, %f74;
	and.b32  	%r28, %r27, -2147483648;
	or.b32  	%r29, %r28, 1123811328;
	mov.b32 	%f76, %r29;
	selp.f32 	%f77, %f76, %f74, %p11;
	mov.f32 	%f78, 0fBF317218;
	fma.rn.f32 	%f79, %f77, %f78, %f71;
	mov.f32 	%f80, 0f3102E308;
	fma.rn.f32 	%f81, %f77, %f80, %f79;
	mul.f32 	%f82, %f81, 0f3FB8AA3B;
	add.f32 	%f83, %f77, 0f4B40007F;
	mov.b32 	%r30, %f83;
	shl.b32 	%r31, %r30, 23;
	mov.b32 	%f84, %r31;
	ex2.approx.ftz.f32 	%f85, %f82;
	mul.f32 	%f86, %f85, %f84;
	neg.f32 	%f87, %f29;
	fma.rn.f32 	%f88, %f87, %f29, %f70;
	fma.rn.f32 	%f89, %f86, %f88, %f86;
	mul.f32 	%f90, %f69, %f89;
	setp.gt.f32 	%p12, %f29, 0f4120E148;
	selp.f32 	%f91, 0f00000000, %f90, %p12;
	setp.lt.f32 	%p13, %f4, 0f00000000;
	sub.f32 	%f92, %f62, %f91;
	selp.f32 	%f253, %f92, %f91, %p13;
	setp.geu.f32 	%p14, %f24, 0fBF800000;
	@%p14 bra 	$L__BB59_9;

	sub.f32 	%f93, %f2, %f4;
	add.rn.f32 	%f94, %f93, %f3;
	mul.f32 	%f95, %f4, 0fC0000000;
	mul.f32 	%f96, %f95, %f253;
	fma.rn.f32 	%f253, %f96, %f94, %f253;

$L__BB59_9:
	mul.f32 	%f97, %f253, 0f3F000000;
	mul.f32 	%f98, %f1, %f97;
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f98;
	add.s32 	%r59, %r59, %r5;
	cvt.u64.u32 	%rd65, %r59;
	setp.lt.u64 	%p15, %rd65, %rd26;
	@%p15 bra 	$L__BB59_7;

$L__BB59_23:
	ret;

}
	// .globl	ugelu_erf_f64
.visible .entry ugelu_erf_f64(
	.param .u64 ugelu_erf_f64_param_0,
	.param .u64 ugelu_erf_f64_param_1,
	.param .u64 ugelu_erf_f64_param_2,
	.param .u64 ugelu_erf_f64_param_3,
	.param .u64 ugelu_erf_f64_param_4
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<84>;
	.reg .f64 	%fd<303>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd23, [ugelu_erf_f64_param_0];
	ld.param.u64 	%rd24, [ugelu_erf_f64_param_1];
	ld.param.u64 	%rd26, [ugelu_erf_f64_param_2];
	ld.param.u64 	%rd25, [ugelu_erf_f64_param_3];
	ld.param.u64 	%rd27, [ugelu_erf_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd25;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p35, %p2;
	@%p3 bra 	$L__BB60_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r78, 0;

$L__BB60_2:
	not.b32 	%r25, %r78;
	cvt.u64.u32 	%rd29, %r25;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd67, %rd34;
	mov.pred 	%p35, -1;
	@%p5 bra 	$L__BB60_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd67, %rd37, %rd67;
	add.s32 	%r78, %r78, 1;
	cvt.u64.u32 	%rd38, %r78;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p35, %p2;
	@%p7 bra 	$L__BB60_2;

$L__BB60_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %tid.x;
	mad.lo.s32 	%r79, %r26, %r3, %r27;
	cvt.u64.u32 	%rd68, %r79;
	@%p35 bra 	$L__BB60_15;
	bra.uni 	$L__BB60_5;

$L__BB60_15:
	setp.ge.u64 	%p20, %rd68, %rd23;
	@%p20 bra 	$L__BB60_30;

	mov.u32 	%r48, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r48;

$L__BB60_17:
	mov.u32 	%r81, 0;
	mov.u32 	%r82, %r79;
	mov.u32 	%r83, %r81;
	@%p3 bra 	$L__BB60_22;

$L__BB60_18:
	not.b32 	%r52, %r81;
	cvt.u64.u32 	%rd43, %r52;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd12, %r82;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p22, %rd47, 0;
	@%p22 bra 	$L__BB60_20;

	div.u64 	%rd70, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd70, %rd14;
	sub.s64 	%rd71, %rd12, %rd48;
	bra.uni 	$L__BB60_21;

$L__BB60_20:
	cvt.u32.u64 	%r53, %rd14;
	cvt.u32.u64 	%r54, %rd12;
	div.u32 	%r55, %r54, %r53;
	mul.lo.s32 	%r56, %r55, %r53;
	sub.s32 	%r57, %r54, %r56;
	cvt.u64.u32 	%rd70, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB60_21:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd71;
	cvt.u32.u64 	%r58, %rd52;
	add.s32 	%r83, %r83, %r58;
	cvt.u32.u64 	%r82, %rd70;
	add.s32 	%r81, %r81, 1;
	cvt.u64.u32 	%rd53, %r81;
	setp.lt.u64 	%p23, %rd53, %rd24;
	@%p23 bra 	$L__BB60_18;

$L__BB60_22:
	ld.param.u64 	%rd64, [ugelu_erf_f64_param_4];
	cvta.to.global.u64 	%rd63, %rd64;
	mul.wide.u32 	%rd54, %r83, 8;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd68, 3;
	add.s64 	%rd21, %rd63, %rd56;
	setp.eq.s64 	%p24, %rd25, 0;
	selp.b64 	%rd57, %rd21, %rd55, %p24;
	ld.global.f64 	%fd12, [%rd57];
	abs.f64 	%fd152, %fd12;
	setp.leu.f64 	%p25, %fd152, 0d4043400000000000;
	mov.f64 	%fd300, %fd12;
	@%p25 bra 	$L__BB60_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd12;
	}
	and.b32  	%r60, %r59, -2147483648;
	mov.f64 	%fd153, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd153;
	}
	and.b32  	%r62, %r61, 2147483647;
	or.b32  	%r63, %r62, %r60;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r64, %temp}, %fd153;
	}
	mov.b64 	%fd300, {%r64, %r63};

$L__BB60_24:
	mov.f64 	%fd154, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd15, %fd300, %fd154;
	neg.f64 	%fd155, %fd15;
	fma.rn.f64 	%fd156, %fd300, %fd154, %fd155;
	mov.f64 	%fd157, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd16, %fd300, %fd157, %fd156;
	add.rn.f64 	%fd17, %fd15, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd17;
	}
	and.b32  	%r21, %r20, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd17;
	}
	setp.lt.u32 	%p26, %r21, 2146435072;
	@%p26 bra 	$L__BB60_26;
	bra.uni 	$L__BB60_25;

$L__BB60_26:
	mov.b64 	%fd160, {%r22, %r21};
	add.f64 	%fd161, %fd160, 0dC010000000000000;
	mov.f64 	%fd162, 0dC010000000000000;
	add.f64 	%fd163, %fd160, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd164, %fd163;
	neg.f64 	%fd165, %fd163;
	mov.f64 	%fd166, 0d3FF0000000000000;
	fma.rn.f64 	%fd167, %fd165, %fd164, %fd166;
	fma.rn.f64 	%fd168, %fd167, %fd167, %fd167;
	fma.rn.f64 	%fd169, %fd168, %fd164, %fd164;
	mul.f64 	%fd170, %fd161, %fd169;
	add.rn.f64 	%fd171, %fd170, %fd166;
	fma.rn.f64 	%fd172, %fd162, %fd171, %fd160;
	neg.f64 	%fd173, %fd170;
	fma.rn.f64 	%fd174, %fd173, %fd160, %fd172;
	fma.rn.f64 	%fd175, %fd169, %fd174, %fd170;
	mov.f64 	%fd176, 0dBE44E1C6FD03D328;
	mov.f64 	%fd177, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd178, %fd177, %fd175, %fd176;
	mov.f64 	%fd179, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd180, %fd178, %fd175, %fd179;
	mov.f64 	%fd181, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd182, %fd180, %fd175, %fd181;
	mov.f64 	%fd183, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd184, %fd182, %fd175, %fd183;
	mov.f64 	%fd185, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd186, %fd184, %fd175, %fd185;
	mov.f64 	%fd187, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd188, %fd186, %fd175, %fd187;
	mov.f64 	%fd189, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd190, %fd188, %fd175, %fd189;
	mov.f64 	%fd191, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd192, %fd190, %fd175, %fd191;
	mov.f64 	%fd193, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd194, %fd192, %fd175, %fd193;
	mov.f64 	%fd195, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd196, %fd194, %fd175, %fd195;
	mov.f64 	%fd197, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd198, %fd196, %fd175, %fd197;
	mov.f64 	%fd199, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd200, %fd198, %fd175, %fd199;
	mov.f64 	%fd201, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd202, %fd200, %fd175, %fd201;
	mov.f64 	%fd203, 0dBF9096238568E357;
	fma.rn.f64 	%fd204, %fd202, %fd175, %fd203;
	mov.f64 	%fd205, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd206, %fd204, %fd175, %fd205;
	mov.f64 	%fd207, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd208, %fd206, %fd175, %fd207;
	mov.f64 	%fd209, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd210, %fd208, %fd175, %fd209;
	mov.f64 	%fd211, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd212, %fd210, %fd175, %fd211;
	mov.f64 	%fd213, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd214, %fd212, %fd175, %fd213;
	mov.f64 	%fd215, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd216, %fd214, %fd175, %fd215;
	mov.f64 	%fd217, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd218, %fd216, %fd175, %fd217;
	mov.f64 	%fd219, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd220, %fd218, %fd175, %fd219;
	mov.f64 	%fd221, 0d4000000000000000;
	fma.rn.f64 	%fd222, %fd221, %fd160, %fd166;
	rcp.approx.ftz.f64 	%fd223, %fd222;
	neg.f64 	%fd224, %fd222;
	fma.rn.f64 	%fd225, %fd224, %fd223, %fd166;
	fma.rn.f64 	%fd226, %fd225, %fd225, %fd225;
	fma.rn.f64 	%fd227, %fd226, %fd223, %fd223;
	mul.f64 	%fd228, %fd220, %fd227;
	mul.f64 	%fd229, %fd228, 0dC000000000000000;
	fma.rn.f64 	%fd230, %fd160, %fd229, %fd220;
	neg.f64 	%fd231, %fd228;
	add.rn.f64 	%fd232, %fd230, %fd231;
	fma.rn.f64 	%fd233, %fd232, %fd227, %fd228;
	mul.f64 	%fd234, %fd160, %fd160;
	neg.f64 	%fd235, %fd234;
	mov.f64 	%fd236, 0d4338000000000000;
	mov.f64 	%fd237, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd238, %fd235, %fd237, %fd236;
	mov.f64 	%fd239, 0dC338000000000000;
	add.rn.f64 	%fd240, %fd238, %fd239;
	mov.f64 	%fd241, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd242, %fd240, %fd241, %fd235;
	mov.f64 	%fd243, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd244, %fd240, %fd243, %fd242;
	mov.f64 	%fd245, 0d3E928AF3FCA213EA;
	mov.f64 	%fd246, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd247, %fd246, %fd244, %fd245;
	mov.f64 	%fd248, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd249, %fd247, %fd244, %fd248;
	mov.f64 	%fd250, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd251, %fd249, %fd244, %fd250;
	mov.f64 	%fd252, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd253, %fd251, %fd244, %fd252;
	mov.f64 	%fd254, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd255, %fd253, %fd244, %fd254;
	mov.f64 	%fd256, 0d3F81111111122322;
	fma.rn.f64 	%fd257, %fd255, %fd244, %fd256;
	mov.f64 	%fd258, 0d3FA55555555502A1;
	fma.rn.f64 	%fd259, %fd257, %fd244, %fd258;
	mov.f64 	%fd260, 0d3FC5555555555511;
	fma.rn.f64 	%fd261, %fd259, %fd244, %fd260;
	mov.f64 	%fd262, 0d3FE000000000000B;
	fma.rn.f64 	%fd263, %fd261, %fd244, %fd262;
	fma.rn.f64 	%fd264, %fd263, %fd244, %fd166;
	fma.rn.f64 	%fd265, %fd264, %fd244, %fd166;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd238;
	}
	shr.u32 	%r66, %r65, 31;
	add.s32 	%r67, %r65, %r66;
	shr.s32 	%r68, %r67, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r69, %temp}, %fd265;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd265;
	}
	shl.b32 	%r71, %r68, 20;
	add.s32 	%r72, %r70, %r71;
	mov.b64 	%fd266, {%r69, %r72};
	sub.s32 	%r73, %r65, %r68;
	shl.b32 	%r74, %r73, 20;
	add.s32 	%r75, %r74, 1072693248;
	mov.u32 	%r76, 0;
	mov.b64 	%fd267, {%r76, %r75};
	mul.f64 	%fd268, %fd266, %fd267;
	neg.f64 	%fd269, %fd160;
	fma.rn.f64 	%fd270, %fd269, %fd160, %fd234;
	fma.rn.f64 	%fd271, %fd268, %fd270, %fd268;
	mul.f64 	%fd272, %fd233, %fd271;
	setp.gt.u32 	%p31, %r21, 1077624832;
	selp.f64 	%fd273, 0d0000000000000000, %fd272, %p31;
	sub.f64 	%fd274, %fd221, %fd273;
	setp.lt.s32 	%p32, %r20, 0;
	selp.f64 	%fd302, %fd274, %fd273, %p32;
	bra.uni 	$L__BB60_27;

$L__BB60_25:
	setp.eq.s32 	%p27, %r21, 2146435072;
	setp.eq.s32 	%p28, %r22, 0;
	and.pred  	%p29, %p27, %p28;
	setp.lt.s32 	%p30, %r20, 0;
	selp.f64 	%fd158, 0d4000000000000000, 0d0000000000000000, %p30;
	add.f64 	%fd159, %fd17, %fd17;
	selp.f64 	%fd302, %fd158, %fd159, %p29;

$L__BB60_27:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd300;
	}
	setp.lt.u32 	%p33, %r77, -1074790399;
	@%p33 bra 	$L__BB60_29;

	mov.f64 	%fd290, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd289, %fd300, %fd290;
	neg.f64 	%fd288, %fd289;
	fma.rn.f64 	%fd287, %fd300, %fd290, %fd288;
	mov.f64 	%fd286, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd285, %fd300, %fd286, %fd287;
	mov.f64 	%fd282, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd281, %fd300, %fd282;
	sub.f64 	%fd275, %fd281, %fd17;
	add.rn.f64 	%fd276, %fd275, %fd285;
	mul.f64 	%fd277, %fd17, 0dC000000000000000;
	mul.f64 	%fd278, %fd277, %fd302;
	fma.rn.f64 	%fd302, %fd278, %fd276, %fd302;

$L__BB60_29:
	ld.param.u64 	%rd62, [ugelu_erf_f64_param_4];
	shl.b64 	%rd61, %rd68, 3;
	cvta.to.global.u64 	%rd60, %rd62;
	add.s64 	%rd59, %rd60, %rd61;
	ld.param.u64 	%rd58, [ugelu_erf_f64_param_0];
	mul.f64 	%fd279, %fd302, 0d3FE0000000000000;
	mul.f64 	%fd280, %fd12, %fd279;
	st.global.f64 	[%rd59], %fd280;
	add.s32 	%r79, %r79, %r11;
	cvt.u64.u32 	%rd68, %r79;
	setp.lt.u64 	%p34, %rd68, %rd58;
	@%p34 bra 	$L__BB60_17;
	bra.uni 	$L__BB60_30;

$L__BB60_5:
	setp.ge.u64 	%p8, %rd68, %rd23;
	@%p8 bra 	$L__BB60_30;

	setp.eq.s64 	%p9, %rd25, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r28;

$L__BB60_7:
	shl.b64 	%rd39, %rd68, 3;
	add.s64 	%rd40, %rd8, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	abs.f64 	%fd23, %fd1;
	setp.leu.f64 	%p10, %fd23, 0d4043400000000000;
	mov.f64 	%fd297, %fd1;
	@%p10 bra 	$L__BB60_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd1;
	}
	and.b32  	%r30, %r29, -2147483648;
	mov.f64 	%fd24, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd24;
	}
	and.b32  	%r32, %r31, 2147483647;
	or.b32  	%r33, %r32, %r30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd24;
	}
	mov.b64 	%fd297, {%r34, %r33};

$L__BB60_9:
	mov.f64 	%fd25, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd4, %fd297, %fd25;
	neg.f64 	%fd26, %fd4;
	fma.rn.f64 	%fd27, %fd297, %fd25, %fd26;
	mov.f64 	%fd28, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd5, %fd297, %fd28, %fd27;
	add.rn.f64 	%fd6, %fd4, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd6;
	}
	and.b32  	%r8, %r7, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r9, %temp}, %fd6;
	}
	setp.lt.u32 	%p11, %r8, 2146435072;
	@%p11 bra 	$L__BB60_11;
	bra.uni 	$L__BB60_10;

$L__BB60_11:
	mov.b64 	%fd31, {%r9, %r8};
	add.f64 	%fd32, %fd31, 0dC010000000000000;
	mov.f64 	%fd33, 0dC010000000000000;
	add.f64 	%fd34, %fd31, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd35, %fd34;
	neg.f64 	%fd36, %fd34;
	mov.f64 	%fd37, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd35, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd38, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd35, %fd35;
	mul.f64 	%fd41, %fd32, %fd40;
	add.rn.f64 	%fd42, %fd41, %fd37;
	fma.rn.f64 	%fd43, %fd33, %fd42, %fd31;
	neg.f64 	%fd44, %fd41;
	fma.rn.f64 	%fd45, %fd44, %fd31, %fd43;
	fma.rn.f64 	%fd46, %fd40, %fd45, %fd41;
	mov.f64 	%fd47, 0dBE44E1C6FD03D328;
	mov.f64 	%fd48, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd49, %fd48, %fd46, %fd47;
	mov.f64 	%fd50, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd51, %fd49, %fd46, %fd50;
	mov.f64 	%fd52, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd53, %fd51, %fd46, %fd52;
	mov.f64 	%fd54, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd55, %fd53, %fd46, %fd54;
	mov.f64 	%fd56, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd57, %fd55, %fd46, %fd56;
	mov.f64 	%fd58, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd59, %fd57, %fd46, %fd58;
	mov.f64 	%fd60, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd61, %fd59, %fd46, %fd60;
	mov.f64 	%fd62, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd63, %fd61, %fd46, %fd62;
	mov.f64 	%fd64, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd65, %fd63, %fd46, %fd64;
	mov.f64 	%fd66, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd67, %fd65, %fd46, %fd66;
	mov.f64 	%fd68, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd69, %fd67, %fd46, %fd68;
	mov.f64 	%fd70, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd71, %fd69, %fd46, %fd70;
	mov.f64 	%fd72, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd73, %fd71, %fd46, %fd72;
	mov.f64 	%fd74, 0dBF9096238568E357;
	fma.rn.f64 	%fd75, %fd73, %fd46, %fd74;
	mov.f64 	%fd76, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd77, %fd75, %fd46, %fd76;
	mov.f64 	%fd78, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd79, %fd77, %fd46, %fd78;
	mov.f64 	%fd80, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd81, %fd79, %fd46, %fd80;
	mov.f64 	%fd82, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd83, %fd81, %fd46, %fd82;
	mov.f64 	%fd84, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd85, %fd83, %fd46, %fd84;
	mov.f64 	%fd86, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd87, %fd85, %fd46, %fd86;
	mov.f64 	%fd88, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd89, %fd87, %fd46, %fd88;
	mov.f64 	%fd90, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd91, %fd89, %fd46, %fd90;
	mov.f64 	%fd92, 0d4000000000000000;
	fma.rn.f64 	%fd93, %fd92, %fd31, %fd37;
	rcp.approx.ftz.f64 	%fd94, %fd93;
	neg.f64 	%fd95, %fd93;
	fma.rn.f64 	%fd96, %fd95, %fd94, %fd37;
	fma.rn.f64 	%fd97, %fd96, %fd96, %fd96;
	fma.rn.f64 	%fd98, %fd97, %fd94, %fd94;
	mul.f64 	%fd99, %fd91, %fd98;
	mul.f64 	%fd100, %fd99, 0dC000000000000000;
	fma.rn.f64 	%fd101, %fd31, %fd100, %fd91;
	neg.f64 	%fd102, %fd99;
	add.rn.f64 	%fd103, %fd101, %fd102;
	fma.rn.f64 	%fd104, %fd103, %fd98, %fd99;
	mul.f64 	%fd105, %fd31, %fd31;
	neg.f64 	%fd106, %fd105;
	mov.f64 	%fd107, 0d4338000000000000;
	mov.f64 	%fd108, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd109, %fd106, %fd108, %fd107;
	mov.f64 	%fd110, 0dC338000000000000;
	add.rn.f64 	%fd111, %fd109, %fd110;
	mov.f64 	%fd112, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd113, %fd111, %fd112, %fd106;
	mov.f64 	%fd114, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd115, %fd111, %fd114, %fd113;
	mov.f64 	%fd116, 0d3E928AF3FCA213EA;
	mov.f64 	%fd117, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd118, %fd117, %fd115, %fd116;
	mov.f64 	%fd119, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd120, %fd118, %fd115, %fd119;
	mov.f64 	%fd121, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd122, %fd120, %fd115, %fd121;
	mov.f64 	%fd123, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd124, %fd122, %fd115, %fd123;
	mov.f64 	%fd125, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd126, %fd124, %fd115, %fd125;
	mov.f64 	%fd127, 0d3F81111111122322;
	fma.rn.f64 	%fd128, %fd126, %fd115, %fd127;
	mov.f64 	%fd129, 0d3FA55555555502A1;
	fma.rn.f64 	%fd130, %fd128, %fd115, %fd129;
	mov.f64 	%fd131, 0d3FC5555555555511;
	fma.rn.f64 	%fd132, %fd130, %fd115, %fd131;
	mov.f64 	%fd133, 0d3FE000000000000B;
	fma.rn.f64 	%fd134, %fd132, %fd115, %fd133;
	fma.rn.f64 	%fd135, %fd134, %fd115, %fd37;
	fma.rn.f64 	%fd136, %fd135, %fd115, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd109;
	}
	shr.u32 	%r36, %r35, 31;
	add.s32 	%r37, %r35, %r36;
	shr.s32 	%r38, %r37, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd136;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd136;
	}
	shl.b32 	%r41, %r38, 20;
	add.s32 	%r42, %r40, %r41;
	mov.b64 	%fd137, {%r39, %r42};
	sub.s32 	%r43, %r35, %r38;
	shl.b32 	%r44, %r43, 20;
	add.s32 	%r45, %r44, 1072693248;
	mov.u32 	%r46, 0;
	mov.b64 	%fd138, {%r46, %r45};
	mul.f64 	%fd139, %fd137, %fd138;
	neg.f64 	%fd140, %fd31;
	fma.rn.f64 	%fd141, %fd140, %fd31, %fd105;
	fma.rn.f64 	%fd142, %fd139, %fd141, %fd139;
	mul.f64 	%fd143, %fd104, %fd142;
	setp.gt.u32 	%p16, %r8, 1077624832;
	selp.f64 	%fd144, 0d0000000000000000, %fd143, %p16;
	sub.f64 	%fd145, %fd92, %fd144;
	setp.lt.s32 	%p17, %r7, 0;
	selp.f64 	%fd299, %fd145, %fd144, %p17;
	bra.uni 	$L__BB60_12;

$L__BB60_10:
	setp.eq.s32 	%p12, %r8, 2146435072;
	setp.eq.s32 	%p13, %r9, 0;
	and.pred  	%p14, %p12, %p13;
	setp.lt.s32 	%p15, %r7, 0;
	selp.f64 	%fd29, 0d4000000000000000, 0d0000000000000000, %p15;
	add.f64 	%fd30, %fd6, %fd6;
	selp.f64 	%fd299, %fd29, %fd30, %p14;

$L__BB60_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd297;
	}
	setp.lt.u32 	%p18, %r47, -1074790399;
	@%p18 bra 	$L__BB60_14;

	mov.f64 	%fd296, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd295, %fd297, %fd296;
	neg.f64 	%fd294, %fd295;
	fma.rn.f64 	%fd293, %fd297, %fd296, %fd294;
	mov.f64 	%fd292, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd291, %fd297, %fd292, %fd293;
	mov.f64 	%fd284, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd283, %fd297, %fd284;
	sub.f64 	%fd146, %fd283, %fd6;
	add.rn.f64 	%fd147, %fd146, %fd291;
	mul.f64 	%fd148, %fd6, 0dC000000000000000;
	mul.f64 	%fd149, %fd148, %fd299;
	fma.rn.f64 	%fd299, %fd149, %fd147, %fd299;

$L__BB60_14:
	shl.b64 	%rd66, %rd68, 3;
	ld.param.u64 	%rd65, [ugelu_erf_f64_param_0];
	mul.f64 	%fd150, %fd299, 0d3FE0000000000000;
	mul.f64 	%fd151, %fd1, %fd150;
	add.s64 	%rd42, %rd1, %rd66;
	st.global.f64 	[%rd42], %fd151;
	add.s32 	%r79, %r79, %r5;
	cvt.u64.u32 	%rd68, %r79;
	setp.lt.u64 	%p19, %rd68, %rd65;
	@%p19 bra 	$L__BB60_7;

$L__BB60_30:
	ret;

}
	// .globl	urelu_f32
.visible .entry urelu_f32(
	.param .u64 urelu_f32_param_0,
	.param .u64 urelu_f32_param_1,
	.param .u64 urelu_f32_param_2,
	.param .u64 urelu_f32_param_3,
	.param .u64 urelu_f32_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [urelu_f32_param_0];
	ld.param.u64 	%rd27, [urelu_f32_param_1];
	ld.param.u64 	%rd29, [urelu_f32_param_2];
	ld.param.u64 	%rd28, [urelu_f32_param_3];
	ld.param.u64 	%rd30, [urelu_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB61_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB61_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB61_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB61_2;

$L__BB61_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB61_8;
	bra.uni 	$L__BB61_5;

$L__BB61_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB61_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB61_16;

$L__BB61_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB61_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB61_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB61_14;

$L__BB61_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB61_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB61_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 4;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 2;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f32 	%f4, [%rd60];
	mov.f32 	%f5, 0f00000000;
	max.f32 	%f6, %f4, %f5;
	st.global.f32 	[%rd59], %f6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB61_10;
	bra.uni 	$L__BB61_19;

$L__BB61_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB61_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB61_7:
	shl.b64 	%rd42, %rd7, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f1, [%rd43];
	mov.f32 	%f2, 0f00000000;
	max.f32 	%f3, %f1, %f2;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f32 	[%rd44], %f3;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB61_7;

$L__BB61_19:
	ret;

$L__BB61_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB61_18;

$L__BB61_17:
	ld.global.f32 	%f7, [%rd2];
	mov.f32 	%f8, 0f00000000;
	max.f32 	%f9, %f7, %f8;
	shl.b64 	%rd61, %rd7, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f9;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB61_17;
	bra.uni 	$L__BB61_19;

$L__BB61_18:
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f32 	%f10, [%rd64];
	mov.f32 	%f11, 0f00000000;
	max.f32 	%f12, %f10, %f11;
	st.global.f32 	[%rd64], %f12;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB61_18;
	bra.uni 	$L__BB61_19;

}
	// .globl	urelu_f64
.visible .entry urelu_f64(
	.param .u64 urelu_f64_param_0,
	.param .u64 urelu_f64_param_1,
	.param .u64 urelu_f64_param_2,
	.param .u64 urelu_f64_param_3,
	.param .u64 urelu_f64_param_4
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd26, [urelu_f64_param_0];
	ld.param.u64 	%rd27, [urelu_f64_param_1];
	ld.param.u64 	%rd29, [urelu_f64_param_2];
	ld.param.u64 	%rd28, [urelu_f64_param_3];
	ld.param.u64 	%rd30, [urelu_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p20, %p2;
	@%p3 bra 	$L__BB62_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r36, 0;

$L__BB62_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd32, %r22;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p20, -1;
	@%p5 bra 	$L__BB62_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd41, %r36;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p20, %p2;
	@%p7 bra 	$L__BB62_2;

$L__BB62_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p20 bra 	$L__BB62_8;
	bra.uni 	$L__BB62_5;

$L__BB62_8:
	setp.ge.u64 	%p11, %rd7, %rd26;
	@%p11 bra 	$L__BB62_19;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB62_16;

$L__BB62_10:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r4;
	mov.u32 	%r41, %r39;

$L__BB62_11:
	not.b32 	%r29, %r39;
	cvt.u64.u32 	%rd45, %r29;
	add.s64 	%rd46, %rd45, %rd27;
	cvt.u64.u32 	%rd12, %r40;
	shl.b64 	%rd47, %rd46, 3;
	and.b64  	%rd48, %rd47, 34359738360;
	add.s64 	%rd13, %rd3, %rd48;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd49, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd49, 0;
	@%p13 bra 	$L__BB62_13;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd50, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd50;
	bra.uni 	$L__BB62_14;

$L__BB62_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd68, %r32;
	cvt.u64.u32 	%rd69, %r34;

$L__BB62_14:
	shl.b64 	%rd51, %rd27, 3;
	add.s64 	%rd52, %rd13, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	mul.lo.s64 	%rd54, %rd53, %rd69;
	cvt.u32.u64 	%r35, %rd54;
	add.s32 	%r41, %r41, %r35;
	cvt.u32.u64 	%r40, %rd68;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd55, %r39;
	setp.lt.u64 	%p14, %rd55, %rd27;
	@%p14 bra 	$L__BB62_11;

	setp.eq.s64 	%p15, %rd28, 0;
	mul.wide.u32 	%rd56, %r41, 8;
	add.s64 	%rd57, %rd2, %rd56;
	shl.b64 	%rd58, %rd7, 3;
	add.s64 	%rd59, %rd1, %rd58;
	selp.b64 	%rd60, %rd59, %rd57, %p15;
	ld.global.f64 	%fd4, [%rd60];
	mov.f64 	%fd5, 0d0000000000000000;
	max.f64 	%fd6, %fd4, %fd5;
	st.global.f64 	[%rd59], %fd6;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p16, %rd7, %rd26;
	@%p16 bra 	$L__BB62_10;
	bra.uni 	$L__BB62_19;

$L__BB62_5:
	setp.ge.u64 	%p8, %rd7, %rd26;
	@%p8 bra 	$L__BB62_19;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;

$L__BB62_7:
	shl.b64 	%rd42, %rd7, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	mov.f64 	%fd2, 0d0000000000000000;
	max.f64 	%fd3, %fd1, %fd2;
	add.s64 	%rd44, %rd1, %rd42;
	st.global.f64 	[%rd44], %fd3;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p10, %rd7, %rd26;
	@%p10 bra 	$L__BB62_7;

$L__BB62_19:
	ret;

$L__BB62_16:
	setp.eq.s64 	%p17, %rd28, 0;
	@%p17 bra 	$L__BB62_18;

$L__BB62_17:
	ld.global.f64 	%fd7, [%rd2];
	mov.f64 	%fd8, 0d0000000000000000;
	max.f64 	%fd9, %fd7, %fd8;
	shl.b64 	%rd61, %rd7, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f64 	[%rd62], %fd9;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p18, %rd7, %rd26;
	@%p18 bra 	$L__BB62_17;
	bra.uni 	$L__BB62_19;

$L__BB62_18:
	shl.b64 	%rd63, %rd7, 3;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.f64 	%fd10, [%rd64];
	mov.f64 	%fd11, 0d0000000000000000;
	max.f64 	%fd12, %fd10, %fd11;
	st.global.f64 	[%rd64], %fd12;
	add.s32 	%r4, %r4, %r8;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p19, %rd7, %rd26;
	@%p19 bra 	$L__BB62_18;
	bra.uni 	$L__BB62_19;

}
	// .globl	uelu_f32
.visible .entry uelu_f32(
	.param .u64 uelu_f32_param_0,
	.param .u64 uelu_f32_param_1,
	.param .u64 uelu_f32_param_2,
	.param .f32 uelu_f32_param_3,
	.param .u64 uelu_f32_param_4,
	.param .u64 uelu_f32_param_5
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<62>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [uelu_f32_param_0];
	ld.param.u64 	%rd27, [uelu_f32_param_1];
	ld.param.u64 	%rd29, [uelu_f32_param_2];
	ld.param.f32 	%f10, [uelu_f32_param_3];
	ld.param.u64 	%rd28, [uelu_f32_param_4];
	ld.param.u64 	%rd30, [uelu_f32_param_5];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p22, %p2;
	@%p3 bra 	$L__BB63_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r40, 0;

$L__BB63_2:
	not.b32 	%r20, %r40;
	cvt.u64.u32 	%rd32, %r20;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p22, -1;
	@%p5 bra 	$L__BB63_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd41, %r40;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p22, %p2;
	@%p7 bra 	$L__BB63_2;

$L__BB63_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r41, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r41;
	@%p22 bra 	$L__BB63_10;
	bra.uni 	$L__BB63_5;

$L__BB63_10:
	setp.ge.u64 	%p12, %rd64, %rd26;
	@%p12 bra 	$L__BB63_23;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB63_20;

$L__BB63_12:
	mov.u32 	%r43, 0;
	mov.u32 	%r44, %r41;
	mov.u32 	%r45, %r43;

$L__BB63_13:
	not.b32 	%r29, %r43;
	cvt.u64.u32 	%rd46, %r29;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r44;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p14, %rd50, 0;
	@%p14 bra 	$L__BB63_15;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB63_16;

$L__BB63_15:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd66, %r32;
	cvt.u64.u32 	%rd67, %r34;

$L__BB63_16:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r35, %rd55;
	add.s32 	%r45, %r45, %r35;
	cvt.u32.u64 	%r44, %rd66;
	add.s32 	%r43, %r43, 1;
	cvt.u64.u32 	%rd56, %r43;
	setp.lt.u64 	%p15, %rd56, %rd27;
	@%p15 bra 	$L__BB63_13;

	setp.eq.s64 	%p16, %rd28, 0;
	mul.wide.u32 	%rd57, %r45, 4;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 2;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p16;
	ld.global.f32 	%f60, [%rd60];
	setp.gt.f32 	%p17, %f60, 0f00000000;
	@%p17 bra 	$L__BB63_19;

	mov.f32 	%f27, 0f3F000000;
	mov.f32 	%f28, 0f3BBB989D;
	fma.rn.f32 	%f29, %f60, %f28, %f27;
	cvt.sat.f32.f32 	%f30, %f29;
	mov.f32 	%f31, 0f4B400001;
	mov.f32 	%f32, 0f437C0000;
	fma.rm.f32 	%f33, %f30, %f32, %f31;
	add.f32 	%f34, %f33, 0fCB40007F;
	neg.f32 	%f35, %f34;
	mov.f32 	%f36, 0f3FB8AA3B;
	fma.rn.f32 	%f37, %f60, %f36, %f35;
	mov.f32 	%f38, 0f32A57060;
	fma.rn.f32 	%f39, %f60, %f38, %f37;
	mov.b32 	%r36, %f33;
	shl.b32 	%r37, %r36, 23;
	mov.b32 	%f40, %r37;
	ex2.approx.ftz.f32 	%f41, %f39;
	fma.rn.f32 	%f42, %f41, %f40, 0fBF800000;
	mul.f32 	%f60, %f42, %f10;

$L__BB63_19:
	st.global.f32 	[%rd21], %f60;
	add.s32 	%r41, %r41, %r8;
	cvt.u64.u32 	%rd64, %r41;
	setp.lt.u64 	%p18, %rd64, %rd26;
	@%p18 bra 	$L__BB63_12;
	bra.uni 	$L__BB63_23;

$L__BB63_20:
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p19, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p19;
	ld.global.f32 	%f61, [%rd62];
	setp.gt.f32 	%p20, %f61, 0f00000000;
	@%p20 bra 	$L__BB63_22;

	mov.f32 	%f43, 0f3F000000;
	mov.f32 	%f44, 0f3BBB989D;
	fma.rn.f32 	%f45, %f61, %f44, %f43;
	cvt.sat.f32.f32 	%f46, %f45;
	mov.f32 	%f47, 0f4B400001;
	mov.f32 	%f48, 0f437C0000;
	fma.rm.f32 	%f49, %f46, %f48, %f47;
	add.f32 	%f50, %f49, 0fCB40007F;
	neg.f32 	%f51, %f50;
	mov.f32 	%f52, 0f3FB8AA3B;
	fma.rn.f32 	%f53, %f61, %f52, %f51;
	mov.f32 	%f54, 0f32A57060;
	fma.rn.f32 	%f55, %f61, %f54, %f53;
	mov.b32 	%r38, %f49;
	shl.b32 	%r39, %r38, 23;
	mov.b32 	%f56, %r39;
	ex2.approx.ftz.f32 	%f57, %f55;
	fma.rn.f32 	%f58, %f57, %f56, 0fBF800000;
	mul.f32 	%f61, %f58, %f10;

$L__BB63_22:
	st.global.f32 	[%rd24], %f61;
	add.s32 	%r41, %r41, %r8;
	cvt.u64.u32 	%rd64, %r41;
	setp.lt.u64 	%p21, %rd64, %rd26;
	@%p21 bra 	$L__BB63_20;
	bra.uni 	$L__BB63_23;

$L__BB63_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB63_23;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB63_7:
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f32 	%f59, [%rd43];
	setp.gt.f32 	%p10, %f59, 0f00000000;
	@%p10 bra 	$L__BB63_9;

	mov.f32 	%f11, 0f3F000000;
	mov.f32 	%f12, 0f3BBB989D;
	fma.rn.f32 	%f13, %f59, %f12, %f11;
	cvt.sat.f32.f32 	%f14, %f13;
	mov.f32 	%f15, 0f4B400001;
	mov.f32 	%f16, 0f437C0000;
	fma.rm.f32 	%f17, %f14, %f16, %f15;
	add.f32 	%f18, %f17, 0fCB40007F;
	neg.f32 	%f19, %f18;
	mov.f32 	%f20, 0f3FB8AA3B;
	fma.rn.f32 	%f21, %f59, %f20, %f19;
	mov.f32 	%f22, 0f32A57060;
	fma.rn.f32 	%f23, %f59, %f22, %f21;
	mov.b32 	%r24, %f17;
	shl.b32 	%r25, %r24, 23;
	mov.b32 	%f24, %r25;
	ex2.approx.ftz.f32 	%f25, %f23;
	fma.rn.f32 	%f26, %f25, %f24, 0fBF800000;
	mul.f32 	%f59, %f26, %f10;

$L__BB63_9:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f32 	[%rd45], %f59;
	add.s32 	%r41, %r41, %r5;
	cvt.u64.u32 	%rd64, %r41;
	setp.lt.u64 	%p11, %rd64, %rd26;
	@%p11 bra 	$L__BB63_7;

$L__BB63_23:
	ret;

}
	// .globl	uelu_f64
.visible .entry uelu_f64(
	.param .u64 uelu_f64_param_0,
	.param .u64 uelu_f64_param_1,
	.param .u64 uelu_f64_param_2,
	.param .f64 uelu_f64_param_3,
	.param .u64 uelu_f64_param_4,
	.param .u64 uelu_f64_param_5
)
{
	.reg .pred 	%p<32>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<134>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd26, [uelu_f64_param_0];
	ld.param.u64 	%rd27, [uelu_f64_param_1];
	ld.param.u64 	%rd29, [uelu_f64_param_2];
	ld.param.f64 	%fd22, [uelu_f64_param_3];
	ld.param.u64 	%rd28, [uelu_f64_param_4];
	ld.param.u64 	%rd30, [uelu_f64_param_5];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p31, %p2;
	@%p3 bra 	$L__BB64_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r79, 0;

$L__BB64_2:
	not.b32 	%r29, %r79;
	cvt.u64.u32 	%rd32, %r29;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd63, %rd37;
	mov.pred 	%p31, -1;
	@%p5 bra 	$L__BB64_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd63, %rd40, %rd63;
	add.s32 	%r79, %r79, 1;
	cvt.u64.u32 	%rd41, %r79;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p31, %p2;
	@%p7 bra 	$L__BB64_2;

$L__BB64_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %tid.x;
	mad.lo.s32 	%r80, %r30, %r3, %r31;
	cvt.u64.u32 	%rd64, %r80;
	@%p31 bra 	$L__BB64_13;
	bra.uni 	$L__BB64_5;

$L__BB64_13:
	setp.ge.u64 	%p15, %rd64, %rd26;
	@%p15 bra 	$L__BB64_32;

	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r45;
	@%p3 bra 	$L__BB64_26;

$L__BB64_15:
	mov.u32 	%r82, 0;
	mov.u32 	%r83, %r80;
	mov.u32 	%r84, %r82;

$L__BB64_16:
	not.b32 	%r48, %r82;
	cvt.u64.u32 	%rd46, %r48;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r83;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p17, %rd50, 0;
	@%p17 bra 	$L__BB64_18;

	div.u64 	%rd66, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd66, %rd14;
	sub.s64 	%rd67, %rd12, %rd51;
	bra.uni 	$L__BB64_19;

$L__BB64_18:
	cvt.u32.u64 	%r49, %rd14;
	cvt.u32.u64 	%r50, %rd12;
	div.u32 	%r51, %r50, %r49;
	mul.lo.s32 	%r52, %r51, %r49;
	sub.s32 	%r53, %r50, %r52;
	cvt.u64.u32 	%rd66, %r51;
	cvt.u64.u32 	%rd67, %r53;

$L__BB64_19:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd67;
	cvt.u32.u64 	%r54, %rd55;
	add.s32 	%r84, %r84, %r54;
	cvt.u32.u64 	%r83, %rd66;
	add.s32 	%r82, %r82, 1;
	cvt.u64.u32 	%rd56, %r82;
	setp.lt.u64 	%p18, %rd56, %rd27;
	@%p18 bra 	$L__BB64_16;

	setp.eq.s64 	%p19, %rd28, 0;
	mul.wide.u32 	%rd57, %r84, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd64, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p19;
	ld.global.f64 	%fd131, [%rd60];
	setp.gt.f64 	%p20, %fd131, 0d0000000000000000;
	@%p20 bra 	$L__BB64_25;

	mov.f64 	%fd58, 0d4338000000000000;
	mov.f64 	%fd59, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd60, %fd131, %fd59, %fd58;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd60;
	}
	mov.f64 	%fd61, 0dC338000000000000;
	add.rn.f64 	%fd62, %fd60, %fd61;
	mov.f64 	%fd63, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd64, %fd62, %fd63, %fd131;
	mov.f64 	%fd65, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd66, %fd62, %fd65, %fd64;
	mov.f64 	%fd67, 0d3E928AF3FCA213EA;
	mov.f64 	%fd68, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mov.f64 	%fd70, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd71, %fd69, %fd66, %fd70;
	mov.f64 	%fd72, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd73, %fd71, %fd66, %fd72;
	mov.f64 	%fd74, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd75, %fd73, %fd66, %fd74;
	mov.f64 	%fd76, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd77, %fd75, %fd66, %fd76;
	mov.f64 	%fd78, 0d3F81111111122322;
	fma.rn.f64 	%fd79, %fd77, %fd66, %fd78;
	mov.f64 	%fd80, 0d3FA55555555502A1;
	fma.rn.f64 	%fd81, %fd79, %fd66, %fd80;
	mov.f64 	%fd82, 0d3FC5555555555511;
	fma.rn.f64 	%fd83, %fd81, %fd66, %fd82;
	mov.f64 	%fd84, 0d3FE000000000000B;
	fma.rn.f64 	%fd85, %fd83, %fd66, %fd84;
	mov.f64 	%fd86, 0d3FF0000000000000;
	fma.rn.f64 	%fd87, %fd85, %fd66, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd66, %fd86;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd88;
	}
	shl.b32 	%r55, %r19, 20;
	add.s32 	%r56, %r21, %r55;
	mov.b64 	%fd130, {%r20, %r56};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd131;
	}
	mov.b32 	%f5, %r57;
	abs.f32 	%f2, %f5;
	setp.lt.f32 	%p21, %f2, 0f4086232B;
	@%p21 bra 	$L__BB64_24;

	setp.lt.f64 	%p22, %fd131, 0d0000000000000000;
	add.f64 	%fd89, %fd131, 0d7FF0000000000000;
	selp.f64 	%fd130, 0d0000000000000000, %fd89, %p22;
	setp.geu.f32 	%p23, %f2, 0f40874800;
	@%p23 bra 	$L__BB64_24;

	shr.u32 	%r58, %r19, 31;
	add.s32 	%r59, %r19, %r58;
	shr.s32 	%r60, %r59, 1;
	shl.b32 	%r61, %r60, 20;
	add.s32 	%r62, %r21, %r61;
	mov.b64 	%fd90, {%r20, %r62};
	sub.s32 	%r63, %r19, %r60;
	shl.b32 	%r64, %r63, 20;
	add.s32 	%r65, %r64, 1072693248;
	mov.u32 	%r66, 0;
	mov.b64 	%fd91, {%r66, %r65};
	mul.f64 	%fd130, %fd90, %fd91;

$L__BB64_24:
	add.f64 	%fd92, %fd130, 0dBFF0000000000000;
	mul.f64 	%fd131, %fd92, %fd22;

$L__BB64_25:
	st.global.f64 	[%rd21], %fd131;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd64, %r80;
	setp.lt.u64 	%p24, %rd64, %rd26;
	@%p24 bra 	$L__BB64_15;
	bra.uni 	$L__BB64_32;

$L__BB64_26:
	shl.b64 	%rd61, %rd64, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p25, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p25;
	ld.global.f64 	%fd133, [%rd62];
	setp.gt.f64 	%p26, %fd133, 0d0000000000000000;
	@%p26 bra 	$L__BB64_31;

	mov.f64 	%fd93, 0d4338000000000000;
	mov.f64 	%fd94, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd95, %fd133, %fd94, %fd93;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd95;
	}
	mov.f64 	%fd96, 0dC338000000000000;
	add.rn.f64 	%fd97, %fd95, %fd96;
	mov.f64 	%fd98, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd99, %fd97, %fd98, %fd133;
	mov.f64 	%fd100, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd101, %fd97, %fd100, %fd99;
	mov.f64 	%fd102, 0d3E928AF3FCA213EA;
	mov.f64 	%fd103, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3F81111111122322;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FA55555555502A1;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FC5555555555511;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FE000000000000B;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FF0000000000000;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	fma.rn.f64 	%fd123, %fd122, %fd101, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd123;
	}
	shl.b32 	%r67, %r24, 20;
	add.s32 	%r68, %r26, %r67;
	mov.b64 	%fd132, {%r25, %r68};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd133;
	}
	mov.b32 	%f6, %r69;
	abs.f32 	%f3, %f6;
	setp.lt.f32 	%p27, %f3, 0f4086232B;
	@%p27 bra 	$L__BB64_30;

	setp.lt.f64 	%p28, %fd133, 0d0000000000000000;
	add.f64 	%fd124, %fd133, 0d7FF0000000000000;
	selp.f64 	%fd132, 0d0000000000000000, %fd124, %p28;
	setp.geu.f32 	%p29, %f3, 0f40874800;
	@%p29 bra 	$L__BB64_30;

	shr.u32 	%r70, %r24, 31;
	add.s32 	%r71, %r24, %r70;
	shr.s32 	%r72, %r71, 1;
	shl.b32 	%r73, %r72, 20;
	add.s32 	%r74, %r26, %r73;
	mov.b64 	%fd125, {%r25, %r74};
	sub.s32 	%r75, %r24, %r72;
	shl.b32 	%r76, %r75, 20;
	add.s32 	%r77, %r76, 1072693248;
	mov.u32 	%r78, 0;
	mov.b64 	%fd126, {%r78, %r77};
	mul.f64 	%fd132, %fd125, %fd126;

$L__BB64_30:
	add.f64 	%fd127, %fd132, 0dBFF0000000000000;
	mul.f64 	%fd133, %fd127, %fd22;

$L__BB64_31:
	st.global.f64 	[%rd24], %fd133;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd64, %r80;
	setp.lt.u64 	%p30, %rd64, %rd26;
	@%p30 bra 	$L__BB64_26;
	bra.uni 	$L__BB64_32;

$L__BB64_5:
	setp.ge.u64 	%p8, %rd64, %rd26;
	@%p8 bra 	$L__BB64_32;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r32;

$L__BB64_7:
	shl.b64 	%rd42, %rd64, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd129, [%rd43];
	setp.gt.f64 	%p10, %fd129, 0d0000000000000000;
	@%p10 bra 	$L__BB64_12;

	mov.f64 	%fd23, 0d4338000000000000;
	mov.f64 	%fd24, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd25, %fd129, %fd24, %fd23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd25;
	}
	mov.f64 	%fd26, 0dC338000000000000;
	add.rn.f64 	%fd27, %fd25, %fd26;
	mov.f64 	%fd28, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd29, %fd27, %fd28, %fd129;
	mov.f64 	%fd30, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd31, %fd27, %fd30, %fd29;
	mov.f64 	%fd32, 0d3E928AF3FCA213EA;
	mov.f64 	%fd33, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd34, %fd33, %fd31, %fd32;
	mov.f64 	%fd35, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd36, %fd34, %fd31, %fd35;
	mov.f64 	%fd37, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd38, %fd36, %fd31, %fd37;
	mov.f64 	%fd39, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd40, %fd38, %fd31, %fd39;
	mov.f64 	%fd41, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd42, %fd40, %fd31, %fd41;
	mov.f64 	%fd43, 0d3F81111111122322;
	fma.rn.f64 	%fd44, %fd42, %fd31, %fd43;
	mov.f64 	%fd45, 0d3FA55555555502A1;
	fma.rn.f64 	%fd46, %fd44, %fd31, %fd45;
	mov.f64 	%fd47, 0d3FC5555555555511;
	fma.rn.f64 	%fd48, %fd46, %fd31, %fd47;
	mov.f64 	%fd49, 0d3FE000000000000B;
	fma.rn.f64 	%fd50, %fd48, %fd31, %fd49;
	mov.f64 	%fd51, 0d3FF0000000000000;
	fma.rn.f64 	%fd52, %fd50, %fd31, %fd51;
	fma.rn.f64 	%fd53, %fd52, %fd31, %fd51;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd53;
	}
	shl.b32 	%r33, %r7, 20;
	add.s32 	%r34, %r9, %r33;
	mov.b64 	%fd128, {%r8, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd129;
	}
	mov.b32 	%f4, %r35;
	abs.f32 	%f1, %f4;
	setp.lt.f32 	%p11, %f1, 0f4086232B;
	@%p11 bra 	$L__BB64_11;

	setp.lt.f64 	%p12, %fd129, 0d0000000000000000;
	add.f64 	%fd54, %fd129, 0d7FF0000000000000;
	selp.f64 	%fd128, 0d0000000000000000, %fd54, %p12;
	setp.geu.f32 	%p13, %f1, 0f40874800;
	@%p13 bra 	$L__BB64_11;

	shr.u32 	%r36, %r7, 31;
	add.s32 	%r37, %r7, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r9, %r39;
	mov.b64 	%fd55, {%r8, %r40};
	sub.s32 	%r41, %r7, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd56, {%r44, %r43};
	mul.f64 	%fd128, %fd55, %fd56;

$L__BB64_11:
	add.f64 	%fd57, %fd128, 0dBFF0000000000000;
	mul.f64 	%fd129, %fd57, %fd22;

$L__BB64_12:
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd129;
	add.s32 	%r80, %r80, %r5;
	cvt.u64.u32 	%rd64, %r80;
	setp.lt.u64 	%p14, %rd64, %rd26;
	@%p14 bra 	$L__BB64_7;

$L__BB64_32:
	ret;

}
	// .globl	usilu_f32
.visible .entry usilu_f32(
	.param .u64 usilu_f32_param_0,
	.param .u64 usilu_f32_param_1,
	.param .u64 usilu_f32_param_2,
	.param .u64 usilu_f32_param_3,
	.param .u64 usilu_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<58>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [usilu_f32_param_0];
	ld.param.u64 	%rd25, [usilu_f32_param_1];
	ld.param.u64 	%rd27, [usilu_f32_param_2];
	ld.param.u64 	%rd26, [usilu_f32_param_3];
	ld.param.u64 	%rd28, [usilu_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd27;
	setp.eq.s64 	%p3, %rd25, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p19, %p2;
	@%p3 bra 	$L__BB65_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r40, 0;

$L__BB65_2:
	not.b32 	%r20, %r40;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	and.b64  	%rd5, %rd31, 4294967295;
	add.s64 	%rd32, %rd5, %rd25;
	shl.b64 	%rd33, %rd32, 3;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.u64 	%rd35, [%rd34];
	setp.ne.s64 	%p5, %rd62, %rd35;
	mov.pred 	%p19, -1;
	@%p5 bra 	$L__BB65_4;

	shl.b64 	%rd36, %rd5, 3;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u64 	%rd38, [%rd37];
	mul.lo.s64 	%rd62, %rd38, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd39, %r40;
	setp.lt.u64 	%p7, %rd39, %rd25;
	mov.pred 	%p19, %p2;
	@%p7 bra 	$L__BB65_2;

$L__BB65_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r41, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r41;
	@%p19 bra 	$L__BB65_8;
	bra.uni 	$L__BB65_5;

$L__BB65_8:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB65_17;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r26;
	@%p3 bra 	$L__BB65_16;

$L__BB65_10:
	mov.u32 	%r43, 0;
	mov.u32 	%r44, %r41;
	mov.u32 	%r45, %r43;

$L__BB65_11:
	not.b32 	%r29, %r43;
	cvt.u64.u32 	%rd43, %r29;
	add.s64 	%rd44, %rd43, %rd25;
	cvt.u64.u32 	%rd12, %r44;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd13, %rd3, %rd46;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd47, %rd14, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB65_13;

	div.u64 	%rd65, %rd12, %rd14;
	mul.lo.s64 	%rd48, %rd65, %rd14;
	sub.s64 	%rd66, %rd12, %rd48;
	bra.uni 	$L__BB65_14;

$L__BB65_13:
	cvt.u32.u64 	%r30, %rd14;
	cvt.u32.u64 	%r31, %rd12;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd65, %r32;
	cvt.u64.u32 	%rd66, %r34;

$L__BB65_14:
	shl.b64 	%rd49, %rd25, 3;
	add.s64 	%rd50, %rd13, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r35, %rd52;
	add.s32 	%r45, %r45, %r35;
	cvt.u32.u64 	%r44, %rd65;
	add.s32 	%r43, %r43, 1;
	cvt.u64.u32 	%rd53, %r43;
	setp.lt.u64 	%p14, %rd53, %rd25;
	@%p14 bra 	$L__BB65_11;

	setp.eq.s64 	%p15, %rd26, 0;
	mul.wide.u32 	%rd54, %r45, 4;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd63, 2;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p15;
	ld.global.f32 	%f20, [%rd58];
	neg.f32 	%f21, %f20;
	mov.f32 	%f22, 0f3F000000;
	mov.f32 	%f23, 0f3BBB989D;
	fma.rn.f32 	%f24, %f21, %f23, %f22;
	cvt.sat.f32.f32 	%f25, %f24;
	mov.f32 	%f26, 0f4B400001;
	mov.f32 	%f27, 0f437C0000;
	fma.rm.f32 	%f28, %f25, %f27, %f26;
	add.f32 	%f29, %f28, 0fCB40007F;
	neg.f32 	%f30, %f29;
	mov.f32 	%f31, 0f3FB8AA3B;
	fma.rn.f32 	%f32, %f21, %f31, %f30;
	mov.f32 	%f33, 0f32A57060;
	fma.rn.f32 	%f34, %f21, %f33, %f32;
	mov.b32 	%r36, %f28;
	shl.b32 	%r37, %r36, 23;
	mov.b32 	%f35, %r37;
	ex2.approx.ftz.f32 	%f36, %f34;
	fma.rn.f32 	%f37, %f36, %f35, 0f3F800000;
	div.rn.f32 	%f38, %f20, %f37;
	st.global.f32 	[%rd57], %f38;
	add.s32 	%r41, %r41, %r8;
	cvt.u64.u32 	%rd63, %r41;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB65_10;
	bra.uni 	$L__BB65_17;

$L__BB65_16:
	shl.b64 	%rd59, %rd63, 2;
	add.s64 	%rd60, %rd1, %rd59;
	setp.eq.s64 	%p17, %rd26, 0;
	selp.b64 	%rd61, %rd60, %rd2, %p17;
	ld.global.f32 	%f39, [%rd61];
	neg.f32 	%f40, %f39;
	mov.f32 	%f41, 0f3F000000;
	mov.f32 	%f42, 0f3BBB989D;
	fma.rn.f32 	%f43, %f40, %f42, %f41;
	cvt.sat.f32.f32 	%f44, %f43;
	mov.f32 	%f45, 0f4B400001;
	mov.f32 	%f46, 0f437C0000;
	fma.rm.f32 	%f47, %f44, %f46, %f45;
	add.f32 	%f48, %f47, 0fCB40007F;
	neg.f32 	%f49, %f48;
	mov.f32 	%f50, 0f3FB8AA3B;
	fma.rn.f32 	%f51, %f40, %f50, %f49;
	mov.f32 	%f52, 0f32A57060;
	fma.rn.f32 	%f53, %f40, %f52, %f51;
	mov.b32 	%r38, %f47;
	shl.b32 	%r39, %r38, 23;
	mov.b32 	%f54, %r39;
	ex2.approx.ftz.f32 	%f55, %f53;
	fma.rn.f32 	%f56, %f55, %f54, 0f3F800000;
	div.rn.f32 	%f57, %f39, %f56;
	st.global.f32 	[%rd60], %f57;
	add.s32 	%r41, %r41, %r8;
	cvt.u64.u32 	%rd63, %r41;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB65_16;
	bra.uni 	$L__BB65_17;

$L__BB65_5:
	setp.ge.u64 	%p8, %rd63, %rd24;
	@%p8 bra 	$L__BB65_17;

	setp.eq.s64 	%p9, %rd26, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB65_7:
	shl.b64 	%rd40, %rd63, 2;
	add.s64 	%rd41, %rd8, %rd40;
	ld.global.f32 	%f1, [%rd41];
	neg.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3F000000;
	mov.f32 	%f4, 0f3BBB989D;
	fma.rn.f32 	%f5, %f2, %f4, %f3;
	cvt.sat.f32.f32 	%f6, %f5;
	mov.f32 	%f7, 0f4B400001;
	mov.f32 	%f8, 0f437C0000;
	fma.rm.f32 	%f9, %f6, %f8, %f7;
	add.f32 	%f10, %f9, 0fCB40007F;
	neg.f32 	%f11, %f10;
	mov.f32 	%f12, 0f3FB8AA3B;
	fma.rn.f32 	%f13, %f2, %f12, %f11;
	mov.f32 	%f14, 0f32A57060;
	fma.rn.f32 	%f15, %f2, %f14, %f13;
	mov.b32 	%r24, %f9;
	shl.b32 	%r25, %r24, 23;
	mov.b32 	%f16, %r25;
	ex2.approx.ftz.f32 	%f17, %f15;
	fma.rn.f32 	%f18, %f17, %f16, 0f3F800000;
	div.rn.f32 	%f19, %f1, %f18;
	add.s64 	%rd42, %rd1, %rd40;
	st.global.f32 	[%rd42], %f19;
	add.s32 	%r41, %r41, %r5;
	cvt.u64.u32 	%rd63, %r41;
	setp.lt.u64 	%p10, %rd63, %rd24;
	@%p10 bra 	$L__BB65_7;

$L__BB65_17:
	ret;

}
	// .globl	usilu_f64
.visible .entry usilu_f64(
	.param .u64 usilu_f64_param_0,
	.param .u64 usilu_f64_param_1,
	.param .u64 usilu_f64_param_2,
	.param .u64 usilu_f64_param_3,
	.param .u64 usilu_f64_param_4
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<133>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd26, [usilu_f64_param_0];
	ld.param.u64 	%rd27, [usilu_f64_param_1];
	ld.param.u64 	%rd29, [usilu_f64_param_2];
	ld.param.u64 	%rd28, [usilu_f64_param_3];
	ld.param.u64 	%rd30, [usilu_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd30;
	cvta.to.global.u64 	%rd2, %rd28;
	cvta.to.global.u64 	%rd3, %rd29;
	setp.eq.s64 	%p3, %rd27, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p28, %p2;
	@%p3 bra 	$L__BB66_4;

	mov.u64 	%rd65, 1;
	mov.u32 	%r79, 0;

$L__BB66_2:
	not.b32 	%r29, %r79;
	cvt.u64.u32 	%rd32, %r29;
	add.s64 	%rd33, %rd32, %rd27;
	and.b64  	%rd5, %rd33, 4294967295;
	add.s64 	%rd34, %rd5, %rd27;
	shl.b64 	%rd35, %rd34, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	setp.ne.s64 	%p5, %rd65, %rd37;
	mov.pred 	%p28, -1;
	@%p5 bra 	$L__BB66_4;

	shl.b64 	%rd38, %rd5, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd65, %rd40, %rd65;
	add.s32 	%r79, %r79, 1;
	cvt.u64.u32 	%rd41, %r79;
	setp.lt.u64 	%p7, %rd41, %rd27;
	mov.pred 	%p28, %p2;
	@%p7 bra 	$L__BB66_2;

$L__BB66_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %tid.x;
	mad.lo.s32 	%r80, %r30, %r3, %r31;
	cvt.u64.u32 	%rd66, %r80;
	@%p28 bra 	$L__BB66_11;
	bra.uni 	$L__BB66_5;

$L__BB66_11:
	setp.ge.u64 	%p14, %rd66, %rd26;
	@%p14 bra 	$L__BB66_26;

	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r11, %r3, %r45;
	@%p3 bra 	$L__BB66_22;

$L__BB66_13:
	mov.u32 	%r82, 0;
	mov.u32 	%r83, %r80;
	mov.u32 	%r84, %r82;

$L__BB66_14:
	not.b32 	%r48, %r82;
	cvt.u64.u32 	%rd46, %r48;
	add.s64 	%rd47, %rd46, %rd27;
	cvt.u64.u32 	%rd12, %r83;
	shl.b64 	%rd48, %rd47, 3;
	and.b64  	%rd49, %rd48, 34359738360;
	add.s64 	%rd13, %rd3, %rd49;
	ld.global.u64 	%rd14, [%rd13];
	and.b64  	%rd50, %rd14, -4294967296;
	setp.eq.s64 	%p16, %rd50, 0;
	@%p16 bra 	$L__BB66_16;

	div.u64 	%rd68, %rd12, %rd14;
	mul.lo.s64 	%rd51, %rd68, %rd14;
	sub.s64 	%rd69, %rd12, %rd51;
	bra.uni 	$L__BB66_17;

$L__BB66_16:
	cvt.u32.u64 	%r49, %rd14;
	cvt.u32.u64 	%r50, %rd12;
	div.u32 	%r51, %r50, %r49;
	mul.lo.s32 	%r52, %r51, %r49;
	sub.s32 	%r53, %r50, %r52;
	cvt.u64.u32 	%rd68, %r51;
	cvt.u64.u32 	%rd69, %r53;

$L__BB66_17:
	shl.b64 	%rd52, %rd27, 3;
	add.s64 	%rd53, %rd13, %rd52;
	ld.global.u64 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd69;
	cvt.u32.u64 	%r54, %rd55;
	add.s32 	%r84, %r84, %r54;
	cvt.u32.u64 	%r83, %rd68;
	add.s32 	%r82, %r82, 1;
	cvt.u64.u32 	%rd56, %r82;
	setp.lt.u64 	%p17, %rd56, %rd27;
	@%p17 bra 	$L__BB66_14;

	ld.param.u64 	%rd64, [usilu_f64_param_3];
	setp.eq.s64 	%p18, %rd64, 0;
	mul.wide.u32 	%rd57, %r84, 8;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd66, 3;
	add.s64 	%rd21, %rd1, %rd59;
	selp.b64 	%rd60, %rd21, %rd58, %p18;
	ld.global.f64 	%fd6, [%rd60];
	neg.f64 	%fd54, %fd6;
	mov.f64 	%fd55, 0d4338000000000000;
	mov.f64 	%fd56, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd57, %fd54, %fd56, %fd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd57;
	}
	mov.f64 	%fd58, 0dC338000000000000;
	add.rn.f64 	%fd59, %fd57, %fd58;
	mov.f64 	%fd60, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd61, %fd59, %fd60, %fd54;
	mov.f64 	%fd62, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd63, %fd59, %fd62, %fd61;
	mov.f64 	%fd64, 0d3E928AF3FCA213EA;
	mov.f64 	%fd65, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3F81111111122322;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3FA55555555502A1;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	mov.f64 	%fd79, 0d3FC5555555555511;
	fma.rn.f64 	%fd80, %fd78, %fd63, %fd79;
	mov.f64 	%fd81, 0d3FE000000000000B;
	fma.rn.f64 	%fd82, %fd80, %fd63, %fd81;
	mov.f64 	%fd83, 0d3FF0000000000000;
	fma.rn.f64 	%fd84, %fd82, %fd63, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd63, %fd83;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd85;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd85;
	}
	shl.b32 	%r55, %r19, 20;
	add.s32 	%r56, %r21, %r55;
	mov.b64 	%fd131, {%r20, %r56};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd54;
	}
	mov.b32 	%f5, %r57;
	abs.f32 	%f2, %f5;
	setp.lt.f32 	%p19, %f2, 0f4086232B;
	@%p19 bra 	$L__BB66_21;

	setp.gt.f64 	%p20, %fd6, 0d8000000000000000;
	mov.f64 	%fd86, 0d7FF0000000000000;
	sub.f64 	%fd87, %fd86, %fd6;
	selp.f64 	%fd131, 0d0000000000000000, %fd87, %p20;
	setp.geu.f32 	%p21, %f2, 0f40874800;
	@%p21 bra 	$L__BB66_21;

	shr.u32 	%r58, %r19, 31;
	add.s32 	%r59, %r19, %r58;
	shr.s32 	%r60, %r59, 1;
	shl.b32 	%r61, %r60, 20;
	add.s32 	%r62, %r21, %r61;
	mov.b64 	%fd88, {%r20, %r62};
	sub.s32 	%r63, %r19, %r60;
	shl.b32 	%r64, %r63, 20;
	add.s32 	%r65, %r64, 1072693248;
	mov.u32 	%r66, 0;
	mov.b64 	%fd89, {%r66, %r65};
	mul.f64 	%fd131, %fd88, %fd89;

$L__BB66_21:
	ld.param.u64 	%rd63, [usilu_f64_param_0];
	add.f64 	%fd90, %fd131, 0d3FF0000000000000;
	div.rn.f64 	%fd91, %fd6, %fd90;
	st.global.f64 	[%rd21], %fd91;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd66, %r80;
	setp.lt.u64 	%p22, %rd66, %rd63;
	@%p22 bra 	$L__BB66_13;
	bra.uni 	$L__BB66_26;

$L__BB66_22:
	shl.b64 	%rd61, %rd66, 3;
	add.s64 	%rd24, %rd1, %rd61;
	setp.eq.s64 	%p23, %rd28, 0;
	selp.b64 	%rd62, %rd24, %rd2, %p23;
	ld.global.f64 	%fd11, [%rd62];
	neg.f64 	%fd92, %fd11;
	mov.f64 	%fd93, 0d4338000000000000;
	mov.f64 	%fd94, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd95, %fd92, %fd94, %fd93;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd95;
	}
	mov.f64 	%fd96, 0dC338000000000000;
	add.rn.f64 	%fd97, %fd95, %fd96;
	mov.f64 	%fd98, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd99, %fd97, %fd98, %fd92;
	mov.f64 	%fd100, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd101, %fd97, %fd100, %fd99;
	mov.f64 	%fd102, 0d3E928AF3FCA213EA;
	mov.f64 	%fd103, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3F81111111122322;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FA55555555502A1;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FC5555555555511;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FE000000000000B;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FF0000000000000;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	fma.rn.f64 	%fd123, %fd122, %fd101, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd123;
	}
	shl.b32 	%r67, %r24, 20;
	add.s32 	%r68, %r26, %r67;
	mov.b64 	%fd132, {%r25, %r68};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd92;
	}
	mov.b32 	%f6, %r69;
	abs.f32 	%f3, %f6;
	setp.lt.f32 	%p24, %f3, 0f4086232B;
	@%p24 bra 	$L__BB66_25;

	setp.gt.f64 	%p25, %fd11, 0d8000000000000000;
	mov.f64 	%fd124, 0d7FF0000000000000;
	sub.f64 	%fd125, %fd124, %fd11;
	selp.f64 	%fd132, 0d0000000000000000, %fd125, %p25;
	setp.geu.f32 	%p26, %f3, 0f40874800;
	@%p26 bra 	$L__BB66_25;

	shr.u32 	%r70, %r24, 31;
	add.s32 	%r71, %r24, %r70;
	shr.s32 	%r72, %r71, 1;
	shl.b32 	%r73, %r72, 20;
	add.s32 	%r74, %r26, %r73;
	mov.b64 	%fd126, {%r25, %r74};
	sub.s32 	%r75, %r24, %r72;
	shl.b32 	%r76, %r75, 20;
	add.s32 	%r77, %r76, 1072693248;
	mov.u32 	%r78, 0;
	mov.b64 	%fd127, {%r78, %r77};
	mul.f64 	%fd132, %fd126, %fd127;

$L__BB66_25:
	add.f64 	%fd128, %fd132, 0d3FF0000000000000;
	div.rn.f64 	%fd129, %fd11, %fd128;
	st.global.f64 	[%rd24], %fd129;
	add.s32 	%r80, %r80, %r11;
	cvt.u64.u32 	%rd66, %r80;
	setp.lt.u64 	%p27, %rd66, %rd26;
	@%p27 bra 	$L__BB66_22;
	bra.uni 	$L__BB66_26;

$L__BB66_5:
	setp.ge.u64 	%p8, %rd66, %rd26;
	@%p8 bra 	$L__BB66_26;

	setp.eq.s64 	%p9, %rd28, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r32;

$L__BB66_7:
	shl.b64 	%rd42, %rd66, 3;
	add.s64 	%rd43, %rd8, %rd42;
	ld.global.f64 	%fd1, [%rd43];
	neg.f64 	%fd16, %fd1;
	mov.f64 	%fd17, 0d4338000000000000;
	mov.f64 	%fd18, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd19, %fd16, %fd18, %fd17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd19;
	}
	mov.f64 	%fd20, 0dC338000000000000;
	add.rn.f64 	%fd21, %fd19, %fd20;
	mov.f64 	%fd22, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd16;
	mov.f64 	%fd24, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd25, %fd21, %fd24, %fd23;
	mov.f64 	%fd26, 0d3E928AF3FCA213EA;
	mov.f64 	%fd27, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F81111111122322;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	mov.f64 	%fd39, 0d3FA55555555502A1;
	fma.rn.f64 	%fd40, %fd38, %fd25, %fd39;
	mov.f64 	%fd41, 0d3FC5555555555511;
	fma.rn.f64 	%fd42, %fd40, %fd25, %fd41;
	mov.f64 	%fd43, 0d3FE000000000000B;
	fma.rn.f64 	%fd44, %fd42, %fd25, %fd43;
	mov.f64 	%fd45, 0d3FF0000000000000;
	fma.rn.f64 	%fd46, %fd44, %fd25, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd25, %fd45;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd47;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd47;
	}
	shl.b32 	%r33, %r7, 20;
	add.s32 	%r34, %r9, %r33;
	mov.b64 	%fd130, {%r8, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd16;
	}
	mov.b32 	%f4, %r35;
	abs.f32 	%f1, %f4;
	setp.lt.f32 	%p10, %f1, 0f4086232B;
	@%p10 bra 	$L__BB66_10;

	setp.gt.f64 	%p11, %fd1, 0d8000000000000000;
	mov.f64 	%fd48, 0d7FF0000000000000;
	sub.f64 	%fd49, %fd48, %fd1;
	selp.f64 	%fd130, 0d0000000000000000, %fd49, %p11;
	setp.geu.f32 	%p12, %f1, 0f40874800;
	@%p12 bra 	$L__BB66_10;

	shr.u32 	%r36, %r7, 31;
	add.s32 	%r37, %r7, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r9, %r39;
	mov.b64 	%fd50, {%r8, %r40};
	sub.s32 	%r41, %r7, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd51, {%r44, %r43};
	mul.f64 	%fd130, %fd50, %fd51;

$L__BB66_10:
	add.f64 	%fd52, %fd130, 0d3FF0000000000000;
	div.rn.f64 	%fd53, %fd1, %fd52;
	add.s64 	%rd45, %rd1, %rd42;
	st.global.f64 	[%rd45], %fd53;
	add.s32 	%r80, %r80, %r5;
	cvt.u64.u32 	%rd66, %r80;
	setp.lt.u64 	%p13, %rd66, %rd26;
	@%p13 bra 	$L__BB66_7;

$L__BB66_26:
	ret;

}
	// .globl	upowf_f32
.visible .entry upowf_f32(
	.param .u64 upowf_f32_param_0,
	.param .u64 upowf_f32_param_1,
	.param .u64 upowf_f32_param_2,
	.param .f32 upowf_f32_param_3,
	.param .u64 upowf_f32_param_4,
	.param .u64 upowf_f32_param_5
)
{
	.reg .pred 	%p<80>;
	.reg .f32 	%f<256>;
	.reg .b32 	%r<87>;
	.reg .b64 	%rd<74>;


	ld.param.u64 	%rd28, [upowf_f32_param_0];
	ld.param.u64 	%rd29, [upowf_f32_param_1];
	ld.param.u64 	%rd31, [upowf_f32_param_2];
	ld.param.f32 	%f27, [upowf_f32_param_3];
	ld.param.u64 	%rd30, [upowf_f32_param_4];
	ld.param.u64 	%rd32, [upowf_f32_param_5];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd30;
	cvta.to.global.u64 	%rd3, %rd31;
	setp.eq.s64 	%p3, %rd29, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p79, %p2;
	@%p3 bra 	$L__BB67_4;

	mov.u64 	%rd67, 1;
	mov.u32 	%r79, 0;

$L__BB67_2:
	not.b32 	%r22, %r79;
	cvt.u64.u32 	%rd34, %r22;
	add.s64 	%rd35, %rd34, %rd29;
	and.b64  	%rd5, %rd35, 4294967295;
	add.s64 	%rd36, %rd5, %rd29;
	shl.b64 	%rd37, %rd36, 3;
	add.s64 	%rd38, %rd3, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p5, %rd67, %rd39;
	mov.pred 	%p79, -1;
	@%p5 bra 	$L__BB67_4;

	shl.b64 	%rd40, %rd5, 3;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.u64 	%rd42, [%rd41];
	mul.lo.s64 	%rd67, %rd42, %rd67;
	add.s32 	%r79, %r79, 1;
	cvt.u64.u32 	%rd43, %r79;
	setp.lt.u64 	%p7, %rd43, %rd29;
	mov.pred 	%p79, %p2;
	@%p7 bra 	$L__BB67_2;

$L__BB67_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd7, %r4;
	@%p79 bra 	$L__BB67_19;
	bra.uni 	$L__BB67_5;

$L__BB67_19:
	setp.ge.u64 	%p31, %rd7, %rd28;
	@%p31 bra 	$L__BB67_46;

	mul.f32 	%f104, %f27, 0f3F000000;
	cvt.rzi.f32.f32 	%f105, %f104;
	add.f32 	%f106, %f105, %f105;
	sub.f32 	%f107, %f27, %f106;
	abs.f32 	%f10, %f107;
	mov.u32 	%r41, %nctaid.x;
	mul.lo.s32 	%r10, %r3, %r41;
	@%p3 bra 	$L__BB67_36;
	bra.uni 	$L__BB67_21;

$L__BB67_29:
	setp.eq.f32 	%p45, %f11, 0f00000000;
	setp.eq.f32 	%p46, %f12, 0f7F800000;
	or.pred  	%p47, %p45, %p46;
	@%p47 bra 	$L__BB67_33;
	bra.uni 	$L__BB67_30;

$L__BB67_33:
	setp.lt.f32 	%p54, %f27, 0f00000000;
	add.f32 	%f179, %f11, %f11;
	mov.b32 	%r60, %f179;
	xor.b32  	%r61, %r60, 2139095040;
	selp.b32 	%r62, %r61, %r60, %p54;
	and.b32  	%r63, %r62, 2147483647;
	setp.eq.f32 	%p55, %f10, 0f3F800000;
	selp.b32 	%r64, %r62, %r63, %p55;
	mov.b32 	%f254, %r64;
	bra.uni 	$L__BB67_35;

$L__BB67_30:
	setp.eq.f32 	%p48, %f11, 0fBF800000;
	setp.eq.f32 	%p49, %f14, 0f7F800000;
	and.pred  	%p50, %p48, %p49;
	@%p50 bra 	$L__BB67_35;

	setp.geu.f32 	%p51, %f11, 0f00000000;
	mov.f32 	%f254, %f13;
	@%p51 bra 	$L__BB67_35;

	setp.eq.f32 	%p52, %f10, 0f3F800000;
	neg.f32 	%f176, %f13;
	selp.f32 	%f177, %f176, %f13, %p52;
	cvt.rmi.f32.f32 	%f178, %f27;
	setp.neu.f32 	%p53, %f178, %f27;
	selp.f32 	%f254, 0f7FFFFFFF, %f177, %p53;
	bra.uni 	$L__BB67_35;

$L__BB67_21:
	mov.u32 	%r83, 0;
	mov.u32 	%r84, %r4;
	mov.u32 	%r85, %r83;

$L__BB67_22:
	not.b32 	%r44, %r83;
	cvt.u64.u32 	%rd50, %r44;
	add.s64 	%rd51, %rd50, %rd29;
	cvt.u64.u32 	%rd14, %r84;
	shl.b64 	%rd52, %rd51, 3;
	and.b64  	%rd53, %rd52, 34359738360;
	add.s64 	%rd15, %rd3, %rd53;
	ld.global.u64 	%rd16, [%rd15];
	and.b64  	%rd54, %rd16, -4294967296;
	setp.eq.s64 	%p33, %rd54, 0;
	@%p33 bra 	$L__BB67_24;

	div.u64 	%rd71, %rd14, %rd16;
	mul.lo.s64 	%rd55, %rd71, %rd16;
	sub.s64 	%rd72, %rd14, %rd55;
	bra.uni 	$L__BB67_25;

$L__BB67_24:
	cvt.u32.u64 	%r45, %rd16;
	cvt.u32.u64 	%r46, %rd14;
	div.u32 	%r47, %r46, %r45;
	mul.lo.s32 	%r48, %r47, %r45;
	sub.s32 	%r49, %r46, %r48;
	cvt.u64.u32 	%rd71, %r47;
	cvt.u64.u32 	%rd72, %r49;

$L__BB67_25:
	shl.b64 	%rd56, %rd29, 3;
	add.s64 	%rd57, %rd15, %rd56;
	ld.global.u64 	%rd58, [%rd57];
	mul.lo.s64 	%rd59, %rd58, %rd72;
	cvt.u32.u64 	%r50, %rd59;
	add.s32 	%r85, %r85, %r50;
	cvt.u32.u64 	%r84, %rd71;
	add.s32 	%r83, %r83, 1;
	cvt.u64.u32 	%rd60, %r83;
	setp.lt.u64 	%p34, %rd60, %rd29;
	mov.f32 	%f254, 0f3F800000;
	@%p34 bra 	$L__BB67_22;

	setp.eq.f32 	%p35, %f27, 0f00000000;
	mul.wide.u32 	%rd61, %r85, 4;
	add.s64 	%rd62, %rd2, %rd61;
	shl.b64 	%rd63, %rd7, 2;
	add.s64 	%rd23, %rd1, %rd63;
	setp.eq.s64 	%p36, %rd30, 0;
	selp.b64 	%rd64, %rd23, %rd62, %p36;
	ld.global.f32 	%f11, [%rd64];
	abs.f32 	%f12, %f11;
	setp.lt.f32 	%p37, %f12, 0f00800000;
	mul.f32 	%f109, %f12, 0f4B800000;
	selp.f32 	%f110, %f109, %f12, %p37;
	selp.f32 	%f111, 0fC1C00000, 0f00000000, %p37;
	mov.b32 	%r51, %f110;
	add.s32 	%r52, %r51, -1060439283;
	and.b32  	%r53, %r52, -8388608;
	sub.s32 	%r54, %r51, %r53;
	mov.b32 	%f112, %r54;
	cvt.rn.f32.s32 	%f113, %r53;
	mov.f32 	%f114, 0f34000000;
	fma.rn.f32 	%f115, %f113, %f114, %f111;
	add.f32 	%f116, %f112, 0fBF800000;
	add.f32 	%f117, %f112, 0f3F800000;
	mov.f32 	%f108, 0f3F800000;
	rcp.approx.ftz.f32 	%f118, %f117;
	add.f32 	%f119, %f116, %f116;
	mul.f32 	%f120, %f119, %f118;
	mul.f32 	%f121, %f120, %f120;
	sub.f32 	%f122, %f116, %f120;
	add.f32 	%f123, %f122, %f122;
	neg.f32 	%f124, %f120;
	fma.rn.f32 	%f125, %f124, %f116, %f123;
	mul.rn.f32 	%f126, %f118, %f125;
	mov.f32 	%f127, 0f3B52E7DB;
	mov.f32 	%f128, 0f3A2C32E4;
	fma.rn.f32 	%f129, %f128, %f121, %f127;
	mov.f32 	%f130, 0f3C93BB73;
	fma.rn.f32 	%f131, %f129, %f121, %f130;
	mov.f32 	%f132, 0f3DF6384F;
	fma.rn.f32 	%f133, %f131, %f121, %f132;
	mul.rn.f32 	%f134, %f133, %f121;
	mov.f32 	%f135, 0f3FB8AA3B;
	fma.rn.f32 	%f136, %f120, %f135, %f115;
	sub.f32 	%f137, %f115, %f136;
	fma.rn.f32 	%f138, %f120, %f135, %f137;
	fma.rn.f32 	%f139, %f126, %f135, %f138;
	mov.f32 	%f140, 0f32A55E34;
	fma.rn.f32 	%f141, %f120, %f140, %f139;
	mul.f32 	%f142, %f134, 0f40400000;
	fma.rn.f32 	%f143, %f142, %f126, %f141;
	fma.rn.f32 	%f144, %f134, %f120, %f143;
	add.rn.f32 	%f145, %f136, %f144;
	neg.f32 	%f146, %f136;
	add.rn.f32 	%f147, %f145, %f146;
	neg.f32 	%f148, %f147;
	add.rn.f32 	%f149, %f144, %f148;
	mul.rn.f32 	%f150, %f145, %f27;
	neg.f32 	%f151, %f150;
	fma.rn.f32 	%f152, %f145, %f27, %f151;
	fma.rn.f32 	%f153, %f149, %f27, %f152;
	cvt.rni.f32.f32 	%f154, %f150;
	sub.f32 	%f155, %f150, %f154;
	add.f32 	%f156, %f153, %f155;
	mov.f32 	%f157, 0f3AAF85ED;
	mov.f32 	%f158, 0f391FCB8E;
	fma.rn.f32 	%f159, %f158, %f156, %f157;
	mov.f32 	%f160, 0f3C1D9856;
	fma.rn.f32 	%f161, %f159, %f156, %f160;
	mov.f32 	%f162, 0f3D6357BB;
	fma.rn.f32 	%f163, %f161, %f156, %f162;
	mov.f32 	%f164, 0f3E75FDEC;
	fma.rn.f32 	%f165, %f163, %f156, %f164;
	mov.f32 	%f166, 0f3F317218;
	fma.rn.f32 	%f167, %f165, %f156, %f166;
	fma.rn.f32 	%f168, %f167, %f156, %f108;
	cvt.rzi.s32.f32 	%r55, %f154;
	setp.gt.f32 	%p38, %f154, 0f00000000;
	selp.b32 	%r56, 0, -2097152000, %p38;
	add.s32 	%r57, %r56, 2130706432;
	mov.b32 	%f169, %r57;
	mul.f32 	%f170, %f168, %f169;
	shl.b32 	%r58, %r55, 23;
	sub.s32 	%r59, %r58, %r56;
	mov.b32 	%f171, %r59;
	mul.f32 	%f172, %f170, %f171;
	abs.f32 	%f173, %f150;
	setp.gt.f32 	%p39, %f173, 0f43180000;
	setp.lt.f32 	%p40, %f150, 0f00000000;
	selp.f32 	%f174, 0f00000000, 0f7F800000, %p40;
	selp.f32 	%f13, %f174, %f172, %p39;
	setp.eq.f32 	%p41, %f11, 0f3F800000;
	or.pred  	%p42, %p35, %p41;
	@%p42 bra 	$L__BB67_35;

	setp.gtu.f32 	%p43, %f12, 0f7F800000;
	@%p43 bra 	$L__BB67_34;

	abs.f32 	%f14, %f27;
	setp.gtu.f32 	%p44, %f14, 0f7F800000;
	@%p44 bra 	$L__BB67_34;
	bra.uni 	$L__BB67_29;

$L__BB67_34:
	add.rn.f32 	%f254, %f11, %f27;

$L__BB67_35:
	st.global.f32 	[%rd23], %f254;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p56, %rd7, %rd28;
	@%p56 bra 	$L__BB67_21;
	bra.uni 	$L__BB67_46;

$L__BB67_5:
	setp.ge.u64 	%p8, %rd7, %rd28;
	@%p8 bra 	$L__BB67_46;

	mul.f32 	%f28, %f27, 0f3F000000;
	cvt.rzi.f32.f32 	%f29, %f28;
	add.f32 	%f30, %f29, %f29;
	sub.f32 	%f31, %f27, %f30;
	abs.f32 	%f1, %f31;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	setp.eq.f32 	%p9, %f27, 0f00000000;
	@%p9 bra 	$L__BB67_18;
	bra.uni 	$L__BB67_7;

$L__BB67_18:
	shl.b64 	%rd48, %rd7, 2;
	add.s64 	%rd49, %rd1, %rd48;
	mov.u32 	%r40, 1065353216;
	st.global.u32 	[%rd49], %r40;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p30, %rd7, %rd28;
	@%p30 bra 	$L__BB67_18;
	bra.uni 	$L__BB67_46;

$L__BB67_39:
	setp.eq.f32 	%p67, %f19, 0f00000000;
	setp.eq.f32 	%p68, %f20, 0f7F800000;
	or.pred  	%p69, %p67, %p68;
	@%p69 bra 	$L__BB67_43;
	bra.uni 	$L__BB67_40;

$L__BB67_43:
	setp.lt.f32 	%p76, %f27, 0f00000000;
	add.f32 	%f251, %f19, %f19;
	mov.b32 	%r74, %f251;
	xor.b32  	%r75, %r74, 2139095040;
	selp.b32 	%r76, %r75, %r74, %p76;
	and.b32  	%r77, %r76, 2147483647;
	setp.eq.f32 	%p77, %f10, 0f3F800000;
	selp.b32 	%r78, %r76, %r77, %p77;
	mov.b32 	%f255, %r78;
	bra.uni 	$L__BB67_45;

$L__BB67_40:
	setp.eq.f32 	%p70, %f19, 0fBF800000;
	setp.eq.f32 	%p71, %f22, 0f7F800000;
	and.pred  	%p72, %p70, %p71;
	@%p72 bra 	$L__BB67_45;

	setp.geu.f32 	%p73, %f19, 0f00000000;
	mov.f32 	%f255, %f21;
	@%p73 bra 	$L__BB67_45;

	setp.eq.f32 	%p74, %f10, 0f3F800000;
	neg.f32 	%f248, %f21;
	selp.f32 	%f249, %f248, %f21, %p74;
	cvt.rmi.f32.f32 	%f250, %f27;
	setp.neu.f32 	%p75, %f250, %f27;
	selp.f32 	%f255, 0f7FFFFFFF, %f249, %p75;
	bra.uni 	$L__BB67_45;

$L__BB67_36:
	shl.b64 	%rd65, %rd7, 2;
	add.s64 	%rd26, %rd1, %rd65;
	setp.eq.s64 	%p57, %rd30, 0;
	selp.b64 	%rd66, %rd26, %rd2, %p57;
	ld.global.f32 	%f19, [%rd66];
	abs.f32 	%f20, %f19;
	setp.lt.f32 	%p58, %f20, 0f00800000;
	mul.f32 	%f181, %f20, 0f4B800000;
	selp.f32 	%f182, %f181, %f20, %p58;
	selp.f32 	%f183, 0fC1C00000, 0f00000000, %p58;
	mov.b32 	%r65, %f182;
	add.s32 	%r66, %r65, -1060439283;
	and.b32  	%r67, %r66, -8388608;
	sub.s32 	%r68, %r65, %r67;
	mov.b32 	%f184, %r68;
	cvt.rn.f32.s32 	%f185, %r67;
	mov.f32 	%f186, 0f34000000;
	fma.rn.f32 	%f187, %f185, %f186, %f183;
	add.f32 	%f188, %f184, 0fBF800000;
	add.f32 	%f189, %f184, 0f3F800000;
	mov.f32 	%f255, 0f3F800000;
	rcp.approx.ftz.f32 	%f190, %f189;
	add.f32 	%f191, %f188, %f188;
	mul.f32 	%f192, %f191, %f190;
	mul.f32 	%f193, %f192, %f192;
	sub.f32 	%f194, %f188, %f192;
	add.f32 	%f195, %f194, %f194;
	neg.f32 	%f196, %f192;
	fma.rn.f32 	%f197, %f196, %f188, %f195;
	mul.rn.f32 	%f198, %f190, %f197;
	mov.f32 	%f199, 0f3B52E7DB;
	mov.f32 	%f200, 0f3A2C32E4;
	fma.rn.f32 	%f201, %f200, %f193, %f199;
	mov.f32 	%f202, 0f3C93BB73;
	fma.rn.f32 	%f203, %f201, %f193, %f202;
	mov.f32 	%f204, 0f3DF6384F;
	fma.rn.f32 	%f205, %f203, %f193, %f204;
	mul.rn.f32 	%f206, %f205, %f193;
	mov.f32 	%f207, 0f3FB8AA3B;
	fma.rn.f32 	%f208, %f192, %f207, %f187;
	sub.f32 	%f209, %f187, %f208;
	fma.rn.f32 	%f210, %f192, %f207, %f209;
	fma.rn.f32 	%f211, %f198, %f207, %f210;
	mov.f32 	%f212, 0f32A55E34;
	fma.rn.f32 	%f213, %f192, %f212, %f211;
	mul.f32 	%f214, %f206, 0f40400000;
	fma.rn.f32 	%f215, %f214, %f198, %f213;
	fma.rn.f32 	%f216, %f206, %f192, %f215;
	add.rn.f32 	%f217, %f208, %f216;
	neg.f32 	%f218, %f208;
	add.rn.f32 	%f219, %f217, %f218;
	neg.f32 	%f220, %f219;
	add.rn.f32 	%f221, %f216, %f220;
	mul.rn.f32 	%f222, %f217, %f27;
	neg.f32 	%f223, %f222;
	fma.rn.f32 	%f224, %f217, %f27, %f223;
	fma.rn.f32 	%f225, %f221, %f27, %f224;
	cvt.rni.f32.f32 	%f226, %f222;
	sub.f32 	%f227, %f222, %f226;
	add.f32 	%f228, %f225, %f227;
	mov.f32 	%f229, 0f3AAF85ED;
	mov.f32 	%f230, 0f391FCB8E;
	fma.rn.f32 	%f231, %f230, %f228, %f229;
	mov.f32 	%f232, 0f3C1D9856;
	fma.rn.f32 	%f233, %f231, %f228, %f232;
	mov.f32 	%f234, 0f3D6357BB;
	fma.rn.f32 	%f235, %f233, %f228, %f234;
	mov.f32 	%f236, 0f3E75FDEC;
	fma.rn.f32 	%f237, %f235, %f228, %f236;
	mov.f32 	%f238, 0f3F317218;
	fma.rn.f32 	%f239, %f237, %f228, %f238;
	fma.rn.f32 	%f240, %f239, %f228, %f255;
	cvt.rzi.s32.f32 	%r69, %f226;
	setp.gt.f32 	%p59, %f226, 0f00000000;
	selp.b32 	%r70, 0, -2097152000, %p59;
	add.s32 	%r71, %r70, 2130706432;
	mov.b32 	%f241, %r71;
	mul.f32 	%f242, %f240, %f241;
	shl.b32 	%r72, %r69, 23;
	sub.s32 	%r73, %r72, %r70;
	mov.b32 	%f243, %r73;
	mul.f32 	%f244, %f242, %f243;
	abs.f32 	%f245, %f222;
	setp.gt.f32 	%p60, %f245, 0f43180000;
	setp.lt.f32 	%p61, %f222, 0f00000000;
	selp.f32 	%f246, 0f00000000, 0f7F800000, %p61;
	selp.f32 	%f21, %f246, %f244, %p60;
	setp.eq.f32 	%p62, %f19, 0f3F800000;
	setp.eq.f32 	%p63, %f27, 0f00000000;
	or.pred  	%p64, %p63, %p62;
	@%p64 bra 	$L__BB67_45;

	setp.gtu.f32 	%p65, %f20, 0f7F800000;
	@%p65 bra 	$L__BB67_44;

	abs.f32 	%f22, %f27;
	setp.gtu.f32 	%p66, %f22, 0f7F800000;
	@%p66 bra 	$L__BB67_44;
	bra.uni 	$L__BB67_39;

$L__BB67_44:
	add.rn.f32 	%f255, %f19, %f27;

$L__BB67_45:
	st.global.f32 	[%rd26], %f255;
	add.s32 	%r4, %r4, %r10;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p78, %rd7, %rd28;
	@%p78 bra 	$L__BB67_36;
	bra.uni 	$L__BB67_46;

$L__BB67_7:
	setp.eq.s64 	%p10, %rd30, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p10;
	bra.uni 	$L__BB67_8;

$L__BB67_11:
	setp.eq.f32 	%p18, %f2, 0f00000000;
	setp.eq.f32 	%p19, %f3, 0f7F800000;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	$L__BB67_15;
	bra.uni 	$L__BB67_12;

$L__BB67_15:
	setp.lt.f32 	%p27, %f27, 0f00000000;
	add.f32 	%f103, %f2, %f2;
	mov.b32 	%r35, %f103;
	xor.b32  	%r36, %r35, 2139095040;
	selp.b32 	%r37, %r36, %r35, %p27;
	and.b32  	%r38, %r37, 2147483647;
	setp.eq.f32 	%p28, %f1, 0f3F800000;
	selp.b32 	%r39, %r37, %r38, %p28;
	mov.b32 	%f253, %r39;
	bra.uni 	$L__BB67_17;

$L__BB67_12:
	setp.eq.f32 	%p21, %f2, 0fBF800000;
	setp.eq.f32 	%p22, %f5, 0f7F800000;
	and.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB67_17;

	setp.geu.f32 	%p24, %f2, 0f00000000;
	mov.f32 	%f253, %f4;
	@%p24 bra 	$L__BB67_17;

	setp.eq.f32 	%p25, %f1, 0f3F800000;
	neg.f32 	%f100, %f4;
	selp.f32 	%f101, %f100, %f4, %p25;
	cvt.rmi.f32.f32 	%f102, %f27;
	setp.neu.f32 	%p26, %f102, %f27;
	selp.f32 	%f253, 0f7FFFFFFF, %f101, %p26;
	bra.uni 	$L__BB67_17;

$L__BB67_8:
	shl.b64 	%rd44, %rd7, 2;
	add.s64 	%rd45, %rd8, %rd44;
	ld.global.f32 	%f2, [%rd45];
	abs.f32 	%f3, %f2;
	setp.lt.f32 	%p11, %f3, 0f00800000;
	mul.f32 	%f33, %f3, 0f4B800000;
	selp.f32 	%f34, %f33, %f3, %p11;
	selp.f32 	%f35, 0fC1C00000, 0f00000000, %p11;
	mov.b32 	%r26, %f34;
	add.s32 	%r27, %r26, -1060439283;
	and.b32  	%r28, %r27, -8388608;
	sub.s32 	%r29, %r26, %r28;
	mov.b32 	%f36, %r29;
	cvt.rn.f32.s32 	%f37, %r28;
	mov.f32 	%f38, 0f34000000;
	fma.rn.f32 	%f39, %f37, %f38, %f35;
	add.f32 	%f40, %f36, 0fBF800000;
	add.f32 	%f41, %f36, 0f3F800000;
	mov.f32 	%f253, 0f3F800000;
	rcp.approx.ftz.f32 	%f42, %f41;
	add.f32 	%f43, %f40, %f40;
	mul.f32 	%f44, %f43, %f42;
	mul.f32 	%f45, %f44, %f44;
	sub.f32 	%f46, %f40, %f44;
	add.f32 	%f47, %f46, %f46;
	neg.f32 	%f48, %f44;
	fma.rn.f32 	%f49, %f48, %f40, %f47;
	mul.rn.f32 	%f50, %f42, %f49;
	mov.f32 	%f51, 0f3B52E7DB;
	mov.f32 	%f52, 0f3A2C32E4;
	fma.rn.f32 	%f53, %f52, %f45, %f51;
	mov.f32 	%f54, 0f3C93BB73;
	fma.rn.f32 	%f55, %f53, %f45, %f54;
	mov.f32 	%f56, 0f3DF6384F;
	fma.rn.f32 	%f57, %f55, %f45, %f56;
	mul.rn.f32 	%f58, %f57, %f45;
	mov.f32 	%f59, 0f3FB8AA3B;
	fma.rn.f32 	%f60, %f44, %f59, %f39;
	sub.f32 	%f61, %f39, %f60;
	fma.rn.f32 	%f62, %f44, %f59, %f61;
	fma.rn.f32 	%f63, %f50, %f59, %f62;
	mov.f32 	%f64, 0f32A55E34;
	fma.rn.f32 	%f65, %f44, %f64, %f63;
	mul.f32 	%f66, %f58, 0f40400000;
	fma.rn.f32 	%f67, %f66, %f50, %f65;
	fma.rn.f32 	%f68, %f58, %f44, %f67;
	add.rn.f32 	%f69, %f60, %f68;
	neg.f32 	%f70, %f60;
	add.rn.f32 	%f71, %f69, %f70;
	neg.f32 	%f72, %f71;
	add.rn.f32 	%f73, %f68, %f72;
	mul.rn.f32 	%f74, %f69, %f27;
	neg.f32 	%f75, %f74;
	fma.rn.f32 	%f76, %f69, %f27, %f75;
	fma.rn.f32 	%f77, %f73, %f27, %f76;
	cvt.rni.f32.f32 	%f78, %f74;
	sub.f32 	%f79, %f74, %f78;
	add.f32 	%f80, %f77, %f79;
	mov.f32 	%f81, 0f3AAF85ED;
	mov.f32 	%f82, 0f391FCB8E;
	fma.rn.f32 	%f83, %f82, %f80, %f81;
	mov.f32 	%f84, 0f3C1D9856;
	fma.rn.f32 	%f85, %f83, %f80, %f84;
	mov.f32 	%f86, 0f3D6357BB;
	fma.rn.f32 	%f87, %f85, %f80, %f86;
	mov.f32 	%f88, 0f3E75FDEC;
	fma.rn.f32 	%f89, %f87, %f80, %f88;
	mov.f32 	%f90, 0f3F317218;
	fma.rn.f32 	%f91, %f89, %f80, %f90;
	fma.rn.f32 	%f92, %f91, %f80, %f253;
	cvt.rzi.s32.f32 	%r30, %f78;
	setp.gt.f32 	%p12, %f78, 0f00000000;
	selp.b32 	%r31, 0, -2097152000, %p12;
	add.s32 	%r32, %r31, 2130706432;
	mov.b32 	%f93, %r32;
	mul.f32 	%f94, %f92, %f93;
	shl.b32 	%r33, %r30, 23;
	sub.s32 	%r34, %r33, %r31;
	mov.b32 	%f95, %r34;
	mul.f32 	%f96, %f94, %f95;
	abs.f32 	%f97, %f74;
	setp.gt.f32 	%p13, %f97, 0f43180000;
	setp.lt.f32 	%p14, %f74, 0f00000000;
	selp.f32 	%f98, 0f00000000, 0f7F800000, %p14;
	selp.f32 	%f4, %f98, %f96, %p13;
	setp.eq.f32 	%p15, %f2, 0f3F800000;
	@%p15 bra 	$L__BB67_17;

	setp.gtu.f32 	%p16, %f3, 0f7F800000;
	@%p16 bra 	$L__BB67_16;

	abs.f32 	%f5, %f27;
	setp.gtu.f32 	%p17, %f5, 0f7F800000;
	@%p17 bra 	$L__BB67_16;
	bra.uni 	$L__BB67_11;

$L__BB67_16:
	add.rn.f32 	%f253, %f2, %f27;

$L__BB67_17:
	add.s64 	%rd47, %rd1, %rd44;
	st.global.f32 	[%rd47], %f253;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p29, %rd7, %rd28;
	@%p29 bra 	$L__BB67_8;

$L__BB67_46:
	ret;

}
	// .globl	upowf_f64
.visible .entry upowf_f64(
	.param .u64 upowf_f64_param_0,
	.param .u64 upowf_f64_param_1,
	.param .u64 upowf_f64_param_2,
	.param .f64 upowf_f64_param_3,
	.param .u64 upowf_f64_param_4,
	.param .u64 upowf_f64_param_5
)
{
	.reg .pred 	%p<134>;
	.reg .b32 	%r<147>;
	.reg .f64 	%fd<78>;
	.reg .b64 	%rd<82>;


	ld.param.u64 	%rd29, [upowf_f64_param_0];
	ld.param.u64 	%rd30, [upowf_f64_param_1];
	ld.param.u64 	%rd33, [upowf_f64_param_2];
	ld.param.f64 	%fd43, [upowf_f64_param_3];
	ld.param.u64 	%rd31, [upowf_f64_param_4];
	ld.param.u64 	%rd32, [upowf_f64_param_5];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	cvta.to.global.u64 	%rd3, %rd33;
	setp.eq.s64 	%p3, %rd30, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p133, %p2;
	@%p3 bra 	$L__BB68_4;

	mov.u64 	%rd75, 1;
	mov.u32 	%r135, 0;

$L__BB68_2:
	not.b32 	%r44, %r135;
	cvt.u64.u32 	%rd35, %r44;
	add.s64 	%rd36, %rd35, %rd30;
	and.b64  	%rd5, %rd36, 4294967295;
	add.s64 	%rd37, %rd5, %rd30;
	shl.b64 	%rd38, %rd37, 3;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	setp.ne.s64 	%p5, %rd75, %rd40;
	mov.pred 	%p133, -1;
	@%p5 bra 	$L__BB68_4;

	shl.b64 	%rd41, %rd5, 3;
	add.s64 	%rd42, %rd3, %rd41;
	ld.global.u64 	%rd43, [%rd42];
	mul.lo.s64 	%rd75, %rd43, %rd75;
	add.s32 	%r135, %r135, 1;
	cvt.u64.u32 	%rd44, %r135;
	setp.lt.u64 	%p7, %rd44, %rd30;
	mov.pred 	%p133, %p2;
	@%p7 bra 	$L__BB68_2;

$L__BB68_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r45, %ctaid.x;
	mov.u32 	%r46, %tid.x;
	mad.lo.s32 	%r4, %r45, %r3, %r46;
	cvt.u64.u32 	%rd7, %r4;
	@%p133 bra 	$L__BB68_38;
	bra.uni 	$L__BB68_5;

$L__BB68_38:
	setp.ge.u64 	%p63, %rd7, %rd29;
	@%p63 bra 	$L__BB68_77;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd43;
	}
	and.b32  	%r84, %r21, 2146435072;
	shr.u32 	%r85, %r84, 20;
	add.s32 	%r86, %r85, -1012;
	mov.b64 	%rd55, %fd43;
	shl.b64 	%rd13, %rd55, %r86;
	setp.eq.s64 	%p65, %rd13, -9223372036854775808;
	selp.u32 	%r22, 1, 0, %p65;
	mov.u32 	%r87, %nctaid.x;
	mul.lo.s32 	%r23, %r3, %r87;
	and.b32  	%r24, %r21, 2147483647;
	setp.gt.s32 	%p66, %r21, -1;
	selp.b32 	%r25, 2146435072, 0, %p66;
	or.b32  	%r26, %r25, -2147483648;
	@%p3 bra 	$L__BB68_61;
	bra.uni 	$L__BB68_40;

$L__BB68_55:
	setp.eq.s32 	%p83, %r24, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r106, %temp}, %fd43;
	}
	setp.eq.s32 	%p84, %r106, 0;
	and.pred  	%p85, %p83, %p84;
	@%p85 bra 	$L__BB68_58;
	bra.uni 	$L__BB68_56;

$L__BB68_58:
	setp.lt.s32 	%p94, %r21, 0;
	mov.u32 	%r111, 0;
	setp.gt.f64 	%p95, %fd22, 0d3FF0000000000000;
	selp.b32 	%r112, 2146435072, 0, %p95;
	xor.b32  	%r113, %r112, 2146435072;
	selp.b32 	%r114, %r113, %r112, %p94;
	setp.eq.f64 	%p96, %fd21, 0dBFF0000000000000;
	selp.b32 	%r115, 1072693248, %r114, %p96;
	mov.b64 	%fd74, {%r111, %r115};
	bra.uni 	$L__BB68_60;

$L__BB68_56:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r107, %temp}, %fd21;
	}
	and.b32  	%r108, %r34, 2147483647;
	setp.ne.s32 	%p86, %r108, 2146435072;
	setp.ne.s32 	%p87, %r107, 0;
	or.pred  	%p88, %p86, %p87;
	@%p88 bra 	$L__BB68_60;

	setp.lt.s32 	%p89, %r34, 0;
	mov.u32 	%r109, 0;
	setp.ne.s32 	%p90, %r144, 0;
	and.pred  	%p91, %p89, %p90;
	setp.ne.s32 	%p92, %r24, 1071644672;
	and.pred  	%p93, %p92, %p91;
	selp.b32 	%r110, %r26, %r25, %p93;
	mov.b64 	%fd74, {%r109, %r110};
	bra.uni 	$L__BB68_60;

$L__BB68_40:
	mov.u32 	%r141, 0;
	mov.u32 	%r142, %r4;
	mov.u32 	%r143, %r141;

$L__BB68_41:
	not.b32 	%r90, %r141;
	cvt.u64.u32 	%rd56, %r90;
	add.s64 	%rd57, %rd56, %rd30;
	cvt.u64.u32 	%rd15, %r142;
	shl.b64 	%rd58, %rd57, 3;
	and.b64  	%rd59, %rd58, 34359738360;
	add.s64 	%rd16, %rd3, %rd59;
	ld.global.u64 	%rd17, [%rd16];
	and.b64  	%rd60, %rd17, -4294967296;
	setp.eq.s64 	%p67, %rd60, 0;
	@%p67 bra 	$L__BB68_43;

	div.u64 	%rd79, %rd15, %rd17;
	mul.lo.s64 	%rd61, %rd79, %rd17;
	sub.s64 	%rd80, %rd15, %rd61;
	bra.uni 	$L__BB68_44;

$L__BB68_43:
	cvt.u32.u64 	%r91, %rd17;
	cvt.u32.u64 	%r92, %rd15;
	div.u32 	%r93, %r92, %r91;
	mul.lo.s32 	%r94, %r93, %r91;
	sub.s32 	%r95, %r92, %r94;
	cvt.u64.u32 	%rd79, %r93;
	cvt.u64.u32 	%rd80, %r95;

$L__BB68_44:
	shl.b64 	%rd62, %rd30, 3;
	add.s64 	%rd63, %rd16, %rd62;
	ld.global.u64 	%rd64, [%rd63];
	mul.lo.s64 	%rd65, %rd64, %rd80;
	cvt.u32.u64 	%r96, %rd65;
	add.s32 	%r143, %r143, %r96;
	cvt.u32.u64 	%r142, %rd79;
	add.s32 	%r141, %r141, 1;
	cvt.u64.u32 	%rd66, %r141;
	setp.lt.u64 	%p68, %rd66, %rd30;
	@%p68 bra 	$L__BB68_41;

	setp.ne.s64 	%p69, %rd13, -9223372036854775808;
	mul.wide.u32 	%rd68, %r143, 8;
	add.s64 	%rd69, %rd2, %rd68;
	shl.b64 	%rd71, %rd7, 3;
	add.s64 	%rd24, %rd1, %rd71;
	setp.eq.s64 	%p70, %rd31, 0;
	selp.b64 	%rd72, %rd24, %rd69, %p70;
	ld.global.f64 	%fd21, [%rd72];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd21;
	}
	abs.f64 	%fd22, %fd21;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd22;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd74, [retval0+0];
	} // callseq 8
	setp.gt.s32 	%p71, %r34, -1;
	or.pred  	%p72, %p71, %p69;
	@%p72 bra 	$L__BB68_47;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r97}, %fd74;
	}
	xor.b32  	%r98, %r97, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r99, %temp}, %fd74;
	}
	mov.b64 	%fd74, {%r99, %r98};

$L__BB68_47:
	setp.eq.f64 	%p73, %fd21, 0d0000000000000000;
	@%p73 bra 	$L__BB68_51;
	bra.uni 	$L__BB68_48;

$L__BB68_51:
	setp.lt.s32 	%p76, %r21, 0;
	mov.u32 	%r100, 0;
	abs.f64 	%fd57, %fd43;
	setp.neu.f64 	%p77, %fd57, 0d3FE0000000000000;
	and.pred  	%p79, %p65, %p77;
	selp.u32 	%r144, 1, 0, %p79;
	selp.b32 	%r101, %r34, 0, %p79;
	or.b32  	%r102, %r101, 2146435072;
	selp.b32 	%r103, %r102, %r101, %p76;
	mov.b64 	%fd74, {%r100, %r103};
	bra.uni 	$L__BB68_52;

$L__BB68_48:
	mov.u32 	%r144, %r22;
	@%p71 bra 	$L__BB68_52;

	cvt.rzi.f64.f64 	%fd55, %fd43;
	setp.eq.f64 	%p75, %fd55, %fd43;
	mov.u32 	%r144, %r22;
	@%p75 bra 	$L__BB68_52;

	mov.f64 	%fd74, 0dFFF8000000000000;
	mov.u32 	%r144, %r22;

$L__BB68_52:
	add.f64 	%fd58, %fd21, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd58;
	}
	and.b32  	%r105, %r104, 2146435072;
	setp.ne.s32 	%p80, %r105, 2146435072;
	@%p80 bra 	$L__BB68_60;

	setp.gtu.f64 	%p81, %fd22, 0d7FF0000000000000;
	@%p81 bra 	$L__BB68_59;

	abs.f64 	%fd59, %fd43;
	setp.gtu.f64 	%p82, %fd59, 0d7FF0000000000000;
	@%p82 bra 	$L__BB68_59;
	bra.uni 	$L__BB68_55;

$L__BB68_59:
	add.rn.f64 	%fd74, %fd21, %fd43;

$L__BB68_60:
	setp.eq.f64 	%p97, %fd21, 0d3FF0000000000000;
	setp.eq.f64 	%p98, %fd43, 0d0000000000000000;
	or.pred  	%p99, %p98, %p97;
	selp.f64 	%fd60, 0d3FF0000000000000, %fd74, %p99;
	st.global.f64 	[%rd24], %fd60;
	add.s32 	%r4, %r4, %r23;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p100, %rd7, %rd29;
	@%p100 bra 	$L__BB68_40;
	bra.uni 	$L__BB68_77;

$L__BB68_5:
	setp.ge.u64 	%p8, %rd7, %rd29;
	@%p8 bra 	$L__BB68_77;

	setp.eq.s64 	%p9, %rd31, 0;
	selp.b64 	%rd8, %rd1, %rd2, %p9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd43;
	}
	and.b32  	%r47, %r5, 2146435072;
	shr.u32 	%r48, %r47, 20;
	add.s32 	%r49, %r48, -1012;
	mov.b64 	%rd45, %fd43;
	shl.b64 	%rd46, %rd45, %r49;
	setp.eq.s64 	%p10, %rd46, -9223372036854775808;
	selp.u32 	%r6, 1, 0, %p10;
	mov.u32 	%r50, %nctaid.x;
	mul.lo.s32 	%r7, %r3, %r50;
	and.b32  	%r8, %r5, 2147483647;
	setp.gt.s32 	%p11, %r5, -1;
	selp.b32 	%r9, 2146435072, 0, %p11;
	or.b32  	%r10, %r9, -2147483648;
	@%p10 bra 	$L__BB68_22;

	shr.s32 	%r51, %r5, 31;
	and.b32  	%r11, %r51, 2146435072;
	bra.uni 	$L__BB68_8;

$L__BB68_32:
	setp.eq.s32 	%p45, %r8, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %fd43;
	}
	setp.eq.s32 	%p46, %r74, 0;
	and.pred  	%p47, %p45, %p46;
	@%p47 bra 	$L__BB68_35;
	bra.uni 	$L__BB68_33;

$L__BB68_35:
	setp.lt.s32 	%p56, %r5, 0;
	mov.u32 	%r79, 0;
	setp.gt.f64 	%p57, %fd11, 0d3FF0000000000000;
	selp.b32 	%r80, 2146435072, 0, %p57;
	xor.b32  	%r81, %r80, 2146435072;
	selp.b32 	%r82, %r81, %r80, %p56;
	setp.eq.f64 	%p58, %fd10, 0dBFF0000000000000;
	selp.b32 	%r83, 1072693248, %r82, %p58;
	mov.b64 	%fd71, {%r79, %r83};
	bra.uni 	$L__BB68_37;

$L__BB68_33:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r75, %temp}, %fd10;
	}
	and.b32  	%r76, %r17, 2147483647;
	setp.ne.s32 	%p48, %r76, 2146435072;
	setp.ne.s32 	%p49, %r75, 0;
	or.pred  	%p50, %p48, %p49;
	@%p50 bra 	$L__BB68_37;

	setp.lt.s32 	%p51, %r17, 0;
	mov.u32 	%r77, 0;
	setp.ne.s32 	%p52, %r139, 0;
	and.pred  	%p53, %p51, %p52;
	setp.ne.s32 	%p54, %r8, 1071644672;
	and.pred  	%p55, %p54, %p53;
	selp.b32 	%r78, %r10, %r9, %p55;
	mov.b64 	%fd71, {%r77, %r78};
	bra.uni 	$L__BB68_37;

$L__BB68_22:
	shl.b64 	%rd51, %rd7, 3;
	add.s64 	%rd52, %rd8, %rd51;
	ld.global.f64 	%fd10, [%rd52];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd10;
	}
	abs.f64 	%fd11, %fd10;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd11;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd71, [retval0+0];
	} // callseq 7
	setp.gt.s32 	%p36, %r17, -1;
	@%p36 bra 	$L__BB68_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd71;
	}
	xor.b32  	%r66, %r65, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd71;
	}
	mov.b64 	%fd71, {%r67, %r66};

$L__BB68_24:
	setp.eq.f64 	%p37, %fd10, 0d0000000000000000;
	@%p37 bra 	$L__BB68_28;
	bra.uni 	$L__BB68_25;

$L__BB68_28:
	setp.lt.s32 	%p40, %r5, 0;
	mov.u32 	%r68, 0;
	abs.f64 	%fd51, %fd43;
	setp.neu.f64 	%p41, %fd51, 0d3FE0000000000000;
	selp.u32 	%r139, 1, 0, %p41;
	selp.b32 	%r69, %r17, 0, %p41;
	or.b32  	%r70, %r69, 2146435072;
	selp.b32 	%r71, %r70, %r69, %p40;
	mov.b64 	%fd71, {%r68, %r71};
	bra.uni 	$L__BB68_29;

$L__BB68_25:
	mov.u32 	%r139, %r6;
	@%p36 bra 	$L__BB68_29;

	cvt.rzi.f64.f64 	%fd49, %fd43;
	setp.eq.f64 	%p39, %fd49, %fd43;
	mov.u32 	%r139, %r6;
	@%p39 bra 	$L__BB68_29;

	mov.f64 	%fd71, 0dFFF8000000000000;
	mov.u32 	%r139, %r6;

$L__BB68_29:
	add.f64 	%fd52, %fd10, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r72}, %fd52;
	}
	and.b32  	%r73, %r72, 2146435072;
	setp.ne.s32 	%p42, %r73, 2146435072;
	@%p42 bra 	$L__BB68_37;

	setp.gtu.f64 	%p43, %fd11, 0d7FF0000000000000;
	@%p43 bra 	$L__BB68_36;

	abs.f64 	%fd53, %fd43;
	setp.gtu.f64 	%p44, %fd53, 0d7FF0000000000000;
	@%p44 bra 	$L__BB68_36;
	bra.uni 	$L__BB68_32;

$L__BB68_36:
	add.rn.f64 	%fd71, %fd10, %fd43;

$L__BB68_37:
	setp.eq.f64 	%p59, %fd10, 0d3FF0000000000000;
	setp.eq.f64 	%p60, %fd43, 0d0000000000000000;
	or.pred  	%p61, %p60, %p59;
	selp.f64 	%fd54, 0d3FF0000000000000, %fd71, %p61;
	add.s64 	%rd54, %rd1, %rd51;
	st.global.f64 	[%rd54], %fd54;
	add.s32 	%r4, %r4, %r7;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p62, %rd7, %rd29;
	@%p62 bra 	$L__BB68_22;
	bra.uni 	$L__BB68_77;

$L__BB68_16:
	setp.eq.s32 	%p18, %r8, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd43;
	}
	setp.eq.s32 	%p19, %r55, 0;
	and.pred  	%p20, %p18, %p19;
	@%p20 bra 	$L__BB68_19;
	bra.uni 	$L__BB68_17;

$L__BB68_19:
	setp.lt.s32 	%p29, %r5, 0;
	mov.u32 	%r60, 0;
	setp.gt.f64 	%p30, %fd2, 0d3FF0000000000000;
	selp.b32 	%r61, 2146435072, 0, %p30;
	xor.b32  	%r62, %r61, 2146435072;
	selp.b32 	%r63, %r62, %r61, %p29;
	setp.eq.f64 	%p31, %fd1, 0dBFF0000000000000;
	selp.b32 	%r64, 1072693248, %r63, %p31;
	mov.b64 	%fd68, {%r60, %r64};
	bra.uni 	$L__BB68_21;

$L__BB68_17:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd1;
	}
	and.b32  	%r57, %r13, 2147483647;
	setp.ne.s32 	%p21, %r57, 2146435072;
	setp.ne.s32 	%p22, %r56, 0;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB68_21;

	setp.lt.s32 	%p24, %r13, 0;
	mov.u32 	%r58, 0;
	setp.ne.s32 	%p25, %r137, 0;
	and.pred  	%p26, %p24, %p25;
	setp.ne.s32 	%p27, %r8, 1071644672;
	and.pred  	%p28, %p27, %p26;
	selp.b32 	%r59, %r10, %r9, %p28;
	mov.b64 	%fd68, {%r58, %r59};
	bra.uni 	$L__BB68_21;

$L__BB68_8:
	shl.b64 	%rd47, %rd7, 3;
	add.s64 	%rd48, %rd8, %rd47;
	ld.global.f64 	%fd1, [%rd48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd1;
	}
	abs.f64 	%fd2, %fd1;
	setp.eq.f64 	%p12, %fd1, 0d0000000000000000;
	@%p12 bra 	$L__BB68_12;
	bra.uni 	$L__BB68_9;

$L__BB68_12:
	mov.u32 	%r137, 0;
	mov.b64 	%fd68, {%r137, %r11};
	bra.uni 	$L__BB68_13;

$L__BB68_9:
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd2;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd68, [retval0+0];
	} // callseq 6
	setp.gt.s32 	%p13, %r13, -1;
	mov.u32 	%r137, %r6;
	@%p13 bra 	$L__BB68_13;

	cvt.rzi.f64.f64 	%fd44, %fd43;
	setp.eq.f64 	%p14, %fd44, %fd43;
	mov.u32 	%r137, %r6;
	@%p14 bra 	$L__BB68_13;

	mov.f64 	%fd68, 0dFFF8000000000000;
	mov.u32 	%r137, %r6;

$L__BB68_13:
	add.f64 	%fd46, %fd1, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd46;
	}
	and.b32  	%r54, %r53, 2146435072;
	setp.ne.s32 	%p15, %r54, 2146435072;
	@%p15 bra 	$L__BB68_21;

	setp.gtu.f64 	%p16, %fd2, 0d7FF0000000000000;
	@%p16 bra 	$L__BB68_20;

	abs.f64 	%fd47, %fd43;
	setp.gtu.f64 	%p17, %fd47, 0d7FF0000000000000;
	@%p17 bra 	$L__BB68_20;
	bra.uni 	$L__BB68_16;

$L__BB68_20:
	add.rn.f64 	%fd68, %fd1, %fd43;

$L__BB68_21:
	setp.eq.f64 	%p32, %fd1, 0d3FF0000000000000;
	setp.eq.f64 	%p33, %fd43, 0d0000000000000000;
	or.pred  	%p34, %p33, %p32;
	selp.f64 	%fd48, 0d3FF0000000000000, %fd68, %p34;
	add.s64 	%rd50, %rd1, %rd47;
	st.global.f64 	[%rd50], %fd48;
	add.s32 	%r4, %r4, %r7;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p35, %rd7, %rd29;
	@%p35 bra 	$L__BB68_8;
	bra.uni 	$L__BB68_77;

$L__BB68_71:
	setp.eq.s32 	%p115, %r24, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r125, %temp}, %fd43;
	}
	setp.eq.s32 	%p116, %r125, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	$L__BB68_74;
	bra.uni 	$L__BB68_72;

$L__BB68_74:
	setp.lt.s32 	%p126, %r21, 0;
	mov.u32 	%r130, 0;
	setp.gt.f64 	%p127, %fd33, 0d3FF0000000000000;
	selp.b32 	%r131, 2146435072, 0, %p127;
	xor.b32  	%r132, %r131, 2146435072;
	selp.b32 	%r133, %r132, %r131, %p126;
	setp.eq.f64 	%p128, %fd32, 0dBFF0000000000000;
	selp.b32 	%r134, 1072693248, %r133, %p128;
	mov.b64 	%fd77, {%r130, %r134};
	bra.uni 	$L__BB68_76;

$L__BB68_72:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r126, %temp}, %fd32;
	}
	and.b32  	%r127, %r39, 2147483647;
	setp.ne.s32 	%p118, %r127, 2146435072;
	setp.ne.s32 	%p119, %r126, 0;
	or.pred  	%p120, %p118, %p119;
	@%p120 bra 	$L__BB68_76;

	setp.lt.s32 	%p121, %r39, 0;
	mov.u32 	%r128, 0;
	setp.ne.s32 	%p122, %r146, 0;
	and.pred  	%p123, %p121, %p122;
	setp.ne.s32 	%p124, %r24, 1071644672;
	and.pred  	%p125, %p124, %p123;
	selp.b32 	%r129, %r26, %r25, %p125;
	mov.b64 	%fd77, {%r128, %r129};
	bra.uni 	$L__BB68_76;

$L__BB68_61:
	shl.b64 	%rd73, %rd7, 3;
	add.s64 	%rd27, %rd1, %rd73;
	setp.eq.s64 	%p101, %rd31, 0;
	selp.b64 	%rd74, %rd27, %rd2, %p101;
	ld.global.f64 	%fd32, [%rd74];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd32;
	}
	abs.f64 	%fd33, %fd32;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd33;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd77, [retval0+0];
	} // callseq 9
	setp.gt.s32 	%p102, %r39, -1;
	setp.ne.s64 	%p103, %rd13, -9223372036854775808;
	or.pred  	%p104, %p102, %p103;
	@%p104 bra 	$L__BB68_63;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r116}, %fd77;
	}
	xor.b32  	%r117, %r116, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r118, %temp}, %fd77;
	}
	mov.b64 	%fd77, {%r118, %r117};

$L__BB68_63:
	setp.eq.f64 	%p105, %fd32, 0d0000000000000000;
	@%p105 bra 	$L__BB68_67;
	bra.uni 	$L__BB68_64;

$L__BB68_67:
	setp.lt.s32 	%p108, %r21, 0;
	mov.u32 	%r119, 0;
	abs.f64 	%fd63, %fd43;
	setp.neu.f64 	%p109, %fd63, 0d3FE0000000000000;
	and.pred  	%p111, %p65, %p109;
	selp.u32 	%r146, 1, 0, %p111;
	selp.b32 	%r120, %r39, 0, %p111;
	or.b32  	%r121, %r120, 2146435072;
	selp.b32 	%r122, %r121, %r120, %p108;
	mov.b64 	%fd77, {%r119, %r122};
	bra.uni 	$L__BB68_68;

$L__BB68_64:
	mov.u32 	%r146, %r22;
	@%p102 bra 	$L__BB68_68;

	cvt.rzi.f64.f64 	%fd61, %fd43;
	setp.eq.f64 	%p107, %fd61, %fd43;
	mov.u32 	%r146, %r22;
	@%p107 bra 	$L__BB68_68;

	mov.f64 	%fd77, 0dFFF8000000000000;
	mov.u32 	%r146, %r22;

$L__BB68_68:
	add.f64 	%fd64, %fd32, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r123}, %fd64;
	}
	and.b32  	%r124, %r123, 2146435072;
	setp.ne.s32 	%p112, %r124, 2146435072;
	@%p112 bra 	$L__BB68_76;

	setp.gtu.f64 	%p113, %fd33, 0d7FF0000000000000;
	@%p113 bra 	$L__BB68_75;

	abs.f64 	%fd65, %fd43;
	setp.gtu.f64 	%p114, %fd65, 0d7FF0000000000000;
	@%p114 bra 	$L__BB68_75;
	bra.uni 	$L__BB68_71;

$L__BB68_75:
	add.rn.f64 	%fd77, %fd32, %fd43;

$L__BB68_76:
	setp.eq.f64 	%p129, %fd32, 0d3FF0000000000000;
	setp.eq.f64 	%p130, %fd43, 0d0000000000000000;
	or.pred  	%p131, %p130, %p129;
	selp.f64 	%fd66, 0d3FF0000000000000, %fd77, %p131;
	st.global.f64 	[%rd27], %fd66;
	add.s32 	%r4, %r4, %r23;
	cvt.u64.u32 	%rd7, %r4;
	setp.lt.u64 	%p132, %rd7, %rd29;
	@%p132 bra 	$L__BB68_61;

$L__BB68_77:
	ret;

}
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot69[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<34>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<80>;


	mov.u64 	%SPL, __local_depot69;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd18, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	shr.u32 	%r10, %r1, 20;
	and.b32  	%r2, %r10, 2047;
	setp.eq.s32 	%p1, %r2, 2047;
	@%p1 bra 	$L__BB69_7;

	add.s32 	%r3, %r2, -1024;
	shr.u32 	%r11, %r3, 6;
	mov.u32 	%r12, 16;
	sub.s32 	%r13, %r12, %r11;
	mov.u32 	%r14, 15;
	sub.s32 	%r4, %r14, %r11;
	mov.u32 	%r15, 19;
	sub.s32 	%r16, %r15, %r11;
	setp.gt.s32 	%p2, %r13, 14;
	selp.b32 	%r5, 18, %r16, %p2;
	setp.gt.s32 	%p3, %r13, %r5;
	mov.u64 	%rd77, 0;
	mov.u32 	%r33, %r4;
	@%p3 bra 	$L__BB69_4;

	add.s32 	%r17, %r4, -15;
	mul.wide.s32 	%rd22, %r17, 8;
	mov.u64 	%rd23, __cudart_i2opi_d;
	add.s64 	%rd24, %rd23, %rd22;
	add.s64 	%rd75, %rd24, 120;
	mov.b64 	%rd25, %fd4;
	shl.b64 	%rd26, %rd25, 11;
	or.b64  	%rd3, %rd26, -9223372036854775808;
	mov.u64 	%rd77, 0;
	mov.u64 	%rd74, %rd1;
	mov.u32 	%r33, %r4;

$L__BB69_3:
	.pragma "nounroll";
	ld.global.nc.u64 	%rd27, [%rd75];
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi, %clo, %chi;
	mov.b64 	{%alo,%ahi}, %rd27;
	mov.b64 	{%blo,%bhi}, %rd3;
	mov.b64 	{%clo,%chi}, %rd77;
	mad.lo.cc.u32 	%r0, %alo, %blo, %clo;
	madc.hi.cc.u32 	%r1, %alo, %blo, %chi;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd28, {%r0,%r1};
	mov.b64 	%rd77, {%r2,%r3};
	}
	st.local.u64 	[%rd74], %rd28;
	add.s64 	%rd75, %rd75, 8;
	add.s64 	%rd74, %rd74, 8;
	add.s32 	%r33, %r33, 1;
	setp.lt.s32 	%p4, %r33, %r5;
	@%p4 bra 	$L__BB69_3;

$L__BB69_4:
	sub.s32 	%r18, %r33, %r4;
	mul.wide.s32 	%rd29, %r18, 8;
	add.s64 	%rd30, %rd1, %rd29;
	st.local.u64 	[%rd30], %rd77;
	ld.local.u64 	%rd79, [%rd1+16];
	ld.local.u64 	%rd78, [%rd1+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32 	%p5, %r9, 0;
	@%p5 bra 	$L__BB69_6;

	mov.u32 	%r19, 64;
	sub.s32 	%r20, %r19, %r9;
	shl.b64 	%rd31, %rd78, %r9;
	shr.u64 	%rd32, %rd79, %r20;
	or.b64  	%rd78, %rd31, %rd32;
	shl.b64 	%rd33, %rd79, %r9;
	ld.local.u64 	%rd34, [%rd1+8];
	shr.u64 	%rd35, %rd34, %r20;
	or.b64  	%rd79, %rd35, %rd33;

$L__BB69_6:
	and.b32  	%r21, %r1, -2147483648;
	shr.u64 	%rd36, %rd78, 62;
	cvt.u32.u64 	%r22, %rd36;
	shr.u64 	%rd37, %rd79, 62;
	shl.b64 	%rd38, %rd78, 2;
	or.b64  	%rd39, %rd37, %rd38;
	shr.u64 	%rd40, %rd78, 61;
	cvt.u32.u64 	%r23, %rd40;
	and.b32  	%r24, %r23, 1;
	add.s32 	%r25, %r24, %r22;
	neg.s32 	%r26, %r25;
	setp.eq.s32 	%p6, %r21, 0;
	selp.b32 	%r27, %r25, %r26, %p6;
	cvta.to.local.u64 	%rd41, %rd18;
	mov.u64 	%rd42, 0;
	st.local.u32 	[%rd41], %r27;
	setp.eq.s32 	%p7, %r24, 0;
	shl.b64 	%rd43, %rd79, 2;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd42;
	mov.b64 	{%a2,%a3}, %rd42;
	mov.b64 	{%b0,%b1}, %rd43;
	mov.b64 	{%b2,%b3}, %rd39;
	sub.cc.u32 	%r0, %a0, %b0;
	subc.cc.u32 	%r1, %a1, %b1;
	subc.cc.u32 	%r2, %a2, %b2;
	subc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd44, {%r0,%r1};
	mov.b64 	%rd45, {%r2,%r3};
	}
	selp.b64 	%rd46, %rd39, %rd45, %p7;
	selp.b64 	%rd47, %rd43, %rd44, %p7;
	xor.b32  	%r28, %r21, -2147483648;
	selp.b32 	%r29, %r21, %r28, %p7;
	clz.b64 	%r30, %rd46;
	cvt.u64.u32 	%rd48, %r30;
	setp.eq.s64 	%p8, %rd48, 0;
	shl.b64 	%rd49, %rd46, %r30;
	mov.u64 	%rd50, 64;
	sub.s64 	%rd51, %rd50, %rd48;
	cvt.u32.u64 	%r31, %rd51;
	shr.u64 	%rd52, %rd47, %r31;
	or.b64  	%rd53, %rd52, %rd49;
	selp.b64 	%rd54, %rd46, %rd53, %p8;
	mov.u64 	%rd55, -3958705157555305931;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi;
	mov.b64 	{%alo,%ahi}, %rd54;
	mov.b64 	{%blo,%bhi}, %rd55;
	mul.lo.u32 	%r0, %alo, %blo;
	mul.hi.u32 	%r1, %alo, %blo;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd56, {%r0,%r1};
	mov.b64 	%rd57, {%r2,%r3};
	}
	setp.gt.s64 	%p9, %rd57, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd56;
	mov.b64 	{%a2,%a3}, %rd57;
	mov.b64 	{%b0,%b1}, %rd56;
	mov.b64 	{%b2,%b3}, %rd57;
	add.cc.u32 	%r0, %a0, %b0;
	addc.cc.u32 	%r1, %a1, %b1;
	addc.cc.u32 	%r2, %a2, %b2;
	addc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd58, {%r0,%r1};
	mov.b64 	%rd59, {%r2,%r3};
	}
	selp.b64 	%rd60, %rd59, %rd57, %p9;
	selp.u64 	%rd61, 1, 0, %p9;
	add.s64 	%rd62, %rd48, %rd61;
	cvt.u64.u32 	%rd63, %r29;
	shl.b64 	%rd64, %rd63, 32;
	shl.b64 	%rd65, %rd62, 52;
	mov.u64 	%rd66, 4602678819172646912;
	sub.s64 	%rd67, %rd66, %rd65;
	add.s64 	%rd68, %rd60, 1;
	shr.u64 	%rd69, %rd68, 10;
	add.s64 	%rd70, %rd69, 1;
	shr.u64 	%rd71, %rd70, 1;
	add.s64 	%rd72, %rd67, %rd71;
	or.b64  	%rd73, %rd72, %rd64;
	mov.b64 	%fd4, %rd73;

$L__BB69_7:
	st.param.f64 	[func_retval0+0], %fd4;
	ret;

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32 	%p1, %r51, 0;
	@%p1 bra 	$L__BB70_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

$L__BB70_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB70_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

$L__BB70_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	mov.f64 	%fd16, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd17, %fd15;
	neg.f64 	%fd18, %fd15;
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd17;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, -2147483648;
	mov.u32 	%r27, 1127219200;
	mov.b64 	%fd79, {%r25, %r27};
	mov.b64 	%fd80, {%r26, %r27};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	shl.b32 	%r29, %r28, 1;
	setp.gt.u32 	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32 	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd16;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	%f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB70_7;

	setp.lt.f64 	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64 	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB70_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

$L__BB70_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.eq.s32 	%p7, %r46, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32 	%p8, %r47, 0;
	and.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB70_9;

	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

$L__BB70_9:
	st.param.f64 	[func_retval0+0], %fd136;
	ret;

}

