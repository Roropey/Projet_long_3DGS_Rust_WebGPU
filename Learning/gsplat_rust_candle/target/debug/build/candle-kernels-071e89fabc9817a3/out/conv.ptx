//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_75
.address_size 64

	// .globl	conv1d_f16

.visible .entry conv1d_f16(
	.param .u64 conv1d_f16_param_0,
	.param .u64 conv1d_f16_param_1,
	.param .u64 conv1d_f16_param_2,
	.param .u64 conv1d_f16_param_3,
	.param .u64 conv1d_f16_param_4,
	.param .u64 conv1d_f16_param_5,
	.param .u64 conv1d_f16_param_6,
	.param .u64 conv1d_f16_param_7,
	.param .u64 conv1d_f16_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd57, [conv1d_f16_param_1];
	ld.param.u64 	%rd58, [conv1d_f16_param_2];
	ld.param.u64 	%rd59, [conv1d_f16_param_3];
	ld.param.u64 	%rd60, [conv1d_f16_param_4];
	ld.param.u64 	%rd62, [conv1d_f16_param_5];
	ld.param.u64 	%rd63, [conv1d_f16_param_6];
	ld.param.u64 	%rd64, [conv1d_f16_param_7];
	ld.param.u64 	%rd61, [conv1d_f16_param_8];
	cvta.to.global.u64 	%rd1, %rd64;
	cvta.to.global.u64 	%rd2, %rd63;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd62;
	ld.global.u64 	%rd5, [%rd4+64];
	ld.global.u64 	%rd6, [%rd4+8];
	ld.global.u64 	%rd7, [%rd4+48];
	mul.lo.s64 	%rd8, %rd7, %rd57;
	ld.global.u64 	%rd65, [%rd4];
	mul.lo.s64 	%rd66, %rd8, %rd65;
	setp.le.u64 	%p1, %rd66, %rd3;
	@%p1 bra 	$L__BB0_23;

	ld.global.u64 	%rd9, [%rd4+16];
	and.b64  	%rd67, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd67, 0;
	@%p2 bra 	$L__BB0_3;

	div.u64 	%rd118, %rd3, %rd8;
	bra.uni 	$L__BB0_4;

$L__BB0_3:
	cvt.u32.u64 	%r5, %rd8;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd118, %r7;

$L__BB0_4:
	and.b64  	%rd68, %rd57, -4294967296;
	setp.eq.s64 	%p3, %rd68, 0;
	@%p3 bra 	$L__BB0_6;

	div.u64 	%rd119, %rd3, %rd57;
	mul.lo.s64 	%rd69, %rd119, %rd57;
	sub.s64 	%rd120, %rd3, %rd69;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r8, %rd57;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd119, %r10;
	cvt.u64.u32 	%rd120, %r12;

$L__BB0_7:
	and.b64  	%rd70, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB0_9;

	rem.u64 	%rd121, %rd119, %rd7;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r13, %rd7;
	cvt.u32.u64 	%r14, %rd119;
	rem.u32 	%r15, %r14, %r13;
	cvt.u64.u32 	%rd121, %r15;

$L__BB0_10:
	ld.global.u64 	%rd71, [%rd4+24];
	mul.lo.s64 	%rd22, %rd71, %rd118;
	setp.eq.s64 	%p5, %rd5, 0;
	mov.f32 	%f38, 0f00000000;
	@%p5 bra 	$L__BB0_22;

	setp.eq.s64 	%p6, %rd6, 0;
	@%p6 bra 	$L__BB0_22;

	add.s64 	%rd23, %rd6, -1;
	and.b64  	%rd24, %rd6, 3;
	sub.s64 	%rd25, %rd6, %rd24;
	shl.b64 	%rd73, %rd22, 1;
	add.s64 	%rd26, %rd2, %rd73;
	mul.lo.s64 	%rd74, %rd120, %rd60;
	mul.lo.s64 	%rd75, %rd74, %rd58;
	shl.b64 	%rd76, %rd75, 1;
	shl.b64 	%rd77, %rd59, 1;
	sub.s64 	%rd27, %rd76, %rd77;
	shl.b64 	%rd28, %rd60, 1;
	add.s64 	%rd29, %rd9, %rd59;
	mul.lo.s64 	%rd30, %rd120, %rd58;
	mov.f32 	%f38, 0f00000000;
	mov.u64 	%rd122, 0;

$L__BB0_13:
	ld.param.u64 	%rd117, [conv1d_f16_param_4];
	add.s64 	%rd78, %rd122, %rd30;
	mul.lo.s64 	%rd32, %rd78, %rd117;
	setp.lt.u64 	%p7, %rd32, %rd59;
	setp.ge.u64 	%p8, %rd32, %rd29;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB0_21;

	setp.lt.u64 	%p10, %rd23, 3;
	ld.global.u64 	%rd33, [%rd4+32];
	mov.u64 	%rd127, 0;
	ld.global.u64 	%rd34, [%rd4+40];
	ld.global.u64 	%rd80, [%rd4+72];
	mul.lo.s64 	%rd35, %rd80, %rd121;
	ld.global.u64 	%rd36, [%rd4+80];
	ld.global.u64 	%rd37, [%rd4+88];
	@%p10 bra 	$L__BB0_17;

	mul.lo.s64 	%rd82, %rd28, %rd122;
	add.s64 	%rd83, %rd27, %rd82;
	mul.lo.s64 	%rd84, %rd34, %rd83;
	add.s64 	%rd125, %rd26, %rd84;
	shl.b64 	%rd85, %rd35, 1;
	add.s64 	%rd86, %rd1, %rd85;
	mul.lo.s64 	%rd87, %rd122, %rd37;
	shl.b64 	%rd88, %rd87, 1;
	add.s64 	%rd126, %rd86, %rd88;
	mov.u64 	%rd127, 0;
	mov.u64 	%rd124, %rd25;

$L__BB0_16:
	shl.b64 	%rd115, %rd36, 1;
	shl.b64 	%rd114, %rd33, 1;
	ld.global.u16 	%rs1, [%rd125];
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd126];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs2;}

	// end inline asm
	fma.rn.f32 	%f23, %f15, %f16, %f38;
	add.s64 	%rd89, %rd125, %rd114;
	ld.global.u16 	%rs3, [%rd89];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs3;}

	// end inline asm
	add.s64 	%rd90, %rd126, %rd115;
	ld.global.u16 	%rs4, [%rd90];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs4;}

	// end inline asm
	fma.rn.f32 	%f24, %f17, %f18, %f23;
	add.s64 	%rd91, %rd89, %rd114;
	ld.global.u16 	%rs5, [%rd91];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs5;}

	// end inline asm
	add.s64 	%rd92, %rd90, %rd115;
	ld.global.u16 	%rs6, [%rd92];
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs6;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f20, %f24;
	add.s64 	%rd93, %rd91, %rd114;
	add.s64 	%rd125, %rd93, %rd114;
	ld.global.u16 	%rs7, [%rd93];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs7;}

	// end inline asm
	add.s64 	%rd94, %rd92, %rd115;
	add.s64 	%rd126, %rd94, %rd115;
	ld.global.u16 	%rs8, [%rd94];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs8;}

	// end inline asm
	fma.rn.f32 	%f38, %f21, %f22, %f25;
	add.s64 	%rd127, %rd127, 4;
	add.s64 	%rd124, %rd124, -4;
	setp.ne.s64 	%p11, %rd124, 0;
	@%p11 bra 	$L__BB0_16;

$L__BB0_17:
	setp.eq.s64 	%p12, %rd24, 0;
	sub.s64 	%rd95, %rd32, %rd59;
	mul.lo.s64 	%rd51, %rd34, %rd95;
	@%p12 bra 	$L__BB0_21;

	setp.eq.s64 	%p13, %rd24, 1;
	mul.lo.s64 	%rd96, %rd33, %rd127;
	add.s64 	%rd97, %rd96, %rd22;
	add.s64 	%rd98, %rd97, %rd51;
	mul.lo.s64 	%rd99, %rd36, %rd127;
	add.s64 	%rd100, %rd99, %rd35;
	mul.lo.s64 	%rd101, %rd37, %rd122;
	add.s64 	%rd102, %rd100, %rd101;
	shl.b64 	%rd103, %rd98, 1;
	add.s64 	%rd52, %rd2, %rd103;
	ld.global.u16 	%rs9, [%rd52];
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs9;}

	// end inline asm
	shl.b64 	%rd104, %rd102, 1;
	add.s64 	%rd53, %rd1, %rd104;
	ld.global.u16 	%rs10, [%rd53];
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs10;}

	// end inline asm
	fma.rn.f32 	%f38, %f26, %f27, %f38;
	@%p13 bra 	$L__BB0_21;

	setp.eq.s64 	%p14, %rd24, 2;
	shl.b64 	%rd105, %rd33, 1;
	add.s64 	%rd54, %rd52, %rd105;
	ld.global.u16 	%rs11, [%rd54];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs11;}

	// end inline asm
	shl.b64 	%rd106, %rd36, 1;
	add.s64 	%rd55, %rd53, %rd106;
	ld.global.u16 	%rs12, [%rd55];
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs12;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f29, %f38;
	@%p14 bra 	$L__BB0_21;

	add.s64 	%rd108, %rd54, %rd105;
	ld.global.u16 	%rs13, [%rd108];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs13;}

	// end inline asm
	add.s64 	%rd110, %rd55, %rd106;
	ld.global.u16 	%rs14, [%rd110];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs14;}

	// end inline asm
	fma.rn.f32 	%f38, %f30, %f31, %f38;

$L__BB0_21:
	add.s64 	%rd122, %rd122, 1;
	setp.lt.u64 	%p15, %rd122, %rd5;
	@%p15 bra 	$L__BB0_13;

$L__BB0_22:
	mov.u32 	%r19, %tid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r16, %r17, %r18, %r19;
	cvt.u64.u32 	%rd116, %r16;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f38;}

	// end inline asm
	cvta.to.global.u64 	%rd111, %rd61;
	shl.b64 	%rd112, %rd116, 1;
	add.s64 	%rd113, %rd111, %rd112;
	st.global.u16 	[%rd113], %rs15;

$L__BB0_23:
	ret;

}
	// .globl	conv2d_f16
.visible .entry conv2d_f16(
	.param .u64 conv2d_f16_param_0,
	.param .u64 conv2d_f16_param_1,
	.param .u64 conv2d_f16_param_2,
	.param .u64 conv2d_f16_param_3,
	.param .u64 conv2d_f16_param_4,
	.param .u64 conv2d_f16_param_5,
	.param .u64 conv2d_f16_param_6,
	.param .u64 conv2d_f16_param_7,
	.param .u64 conv2d_f16_param_8,
	.param .u64 conv2d_f16_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<170>;


	ld.param.u64 	%rd75, [conv2d_f16_param_1];
	ld.param.u64 	%rd76, [conv2d_f16_param_2];
	ld.param.u64 	%rd77, [conv2d_f16_param_3];
	ld.param.u64 	%rd78, [conv2d_f16_param_4];
	ld.param.u64 	%rd79, [conv2d_f16_param_5];
	ld.param.u64 	%rd81, [conv2d_f16_param_6];
	ld.param.u64 	%rd82, [conv2d_f16_param_7];
	ld.param.u64 	%rd83, [conv2d_f16_param_8];
	cvta.to.global.u64 	%rd1, %rd83;
	cvta.to.global.u64 	%rd2, %rd82;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd81;
	ld.global.u64 	%rd5, [%rd4+80];
	ld.global.u64 	%rd6, [%rd4+88];
	ld.global.u64 	%rd7, [%rd4+8];
	mul.lo.s64 	%rd8, %rd76, %rd75;
	ld.global.u64 	%rd9, [%rd4+64];
	mul.lo.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd84, [%rd4];
	mul.lo.s64 	%rd85, %rd10, %rd84;
	setp.le.u64 	%p1, %rd85, %rd3;
	@%p1 bra 	$L__BB1_33;

	ld.global.u64 	%rd11, [%rd4+16];
	ld.global.u64 	%rd12, [%rd4+24];
	and.b64  	%rd86, %rd10, -4294967296;
	setp.eq.s64 	%p2, %rd86, 0;
	@%p2 bra 	$L__BB1_3;

	div.u64 	%rd158, %rd3, %rd10;
	bra.uni 	$L__BB1_4;

$L__BB1_3:
	cvt.u32.u64 	%r5, %rd10;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd158, %r7;

$L__BB1_4:
	and.b64  	%rd87, %rd8, -4294967296;
	setp.eq.s64 	%p3, %rd87, 0;
	@%p3 bra 	$L__BB1_6;

	div.u64 	%rd159, %rd3, %rd8;
	bra.uni 	$L__BB1_7;

$L__BB1_6:
	cvt.u32.u64 	%r8, %rd8;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd159, %r10;

$L__BB1_7:
	and.b64  	%rd88, %rd9, -4294967296;
	setp.eq.s64 	%p4, %rd88, 0;
	@%p4 bra 	$L__BB1_9;

	rem.u64 	%rd160, %rd159, %rd9;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r11, %rd9;
	cvt.u32.u64 	%r12, %rd159;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB1_10:
	and.b64  	%rd89, %rd75, -4294967296;
	setp.eq.s64 	%p5, %rd89, 0;
	@%p5 bra 	$L__BB1_12;

	div.u64 	%rd161, %rd3, %rd75;
	mul.lo.s64 	%rd90, %rd161, %rd75;
	sub.s64 	%rd162, %rd3, %rd90;
	bra.uni 	$L__BB1_13;

$L__BB1_12:
	cvt.u32.u64 	%r14, %rd75;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd161, %r16;
	cvt.u64.u32 	%rd162, %r18;

$L__BB1_13:
	and.b64  	%rd91, %rd76, -4294967296;
	setp.eq.s64 	%p6, %rd91, 0;
	@%p6 bra 	$L__BB1_15;

	rem.u64 	%rd163, %rd161, %rd76;
	bra.uni 	$L__BB1_16;

$L__BB1_15:
	cvt.u32.u64 	%r19, %rd76;
	cvt.u32.u64 	%r20, %rd161;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd163, %r21;

$L__BB1_16:
	ld.global.u64 	%rd92, [%rd4+32];
	mul.lo.s64 	%rd31, %rd92, %rd158;
	setp.eq.s64 	%p7, %rd6, 0;
	mov.f32 	%f41, 0f00000000;
	@%p7 bra 	$L__BB1_32;

	setp.eq.s64 	%p8, %rd5, 0;
	@%p8 bra 	$L__BB1_32;

	add.s64 	%rd32, %rd7, -1;
	and.b64  	%rd33, %rd7, 3;
	sub.s64 	%rd34, %rd33, %rd7;
	shl.b64 	%rd94, %rd31, 1;
	add.s64 	%rd35, %rd2, %rd94;
	mul.lo.s64 	%rd42, %rd162, %rd77;
	shl.b64 	%rd95, %rd42, 1;
	shl.b64 	%rd96, %rd78, 1;
	sub.s64 	%rd36, %rd95, %rd96;
	shl.b64 	%rd37, %rd79, 1;
	mul.lo.s64 	%rd40, %rd163, %rd77;
	shl.b64 	%rd97, %rd40, 1;
	sub.s64 	%rd38, %rd97, %rd96;
	add.s64 	%rd39, %rd11, %rd78;
	add.s64 	%rd41, %rd12, %rd78;
	mov.f32 	%f41, 0f00000000;
	mov.u64 	%rd164, 0;

$L__BB1_19:
	mul.lo.s64 	%rd98, %rd164, %rd79;
	add.s64 	%rd44, %rd98, %rd42;
	setp.lt.u64 	%p9, %rd44, %rd78;
	setp.ge.u64 	%p10, %rd44, %rd41;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB1_31;

	setp.eq.s64 	%p12, %rd7, 0;
	@%p12 bra 	$L__BB1_31;

	mov.u64 	%rd165, 0;

$L__BB1_22:
	mul.lo.s64 	%rd101, %rd165, %rd79;
	add.s64 	%rd49, %rd101, %rd40;
	setp.lt.u64 	%p13, %rd49, %rd78;
	setp.ge.u64 	%p14, %rd49, %rd39;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB1_30;

	setp.lt.u64 	%p16, %rd32, 3;
	ld.global.u64 	%rd50, [%rd4+40];
	mov.u64 	%rd169, 0;
	ld.global.u64 	%rd51, [%rd4+48];
	ld.global.u64 	%rd52, [%rd4+56];
	ld.global.u64 	%rd103, [%rd4+96];
	mul.lo.s64 	%rd53, %rd103, %rd160;
	ld.global.u64 	%rd54, [%rd4+104];
	ld.global.u64 	%rd55, [%rd4+112];
	ld.global.u64 	%rd56, [%rd4+120];
	@%p16 bra 	$L__BB1_26;

	mul.lo.s64 	%rd157, %rd37, %rd164;
	add.s64 	%rd156, %rd36, %rd157;
	shl.b64 	%rd150, %rd164, 1;
	mul.lo.s64 	%rd105, %rd156, %rd52;
	mul.lo.s64 	%rd106, %rd37, %rd165;
	add.s64 	%rd107, %rd38, %rd106;
	mul.lo.s64 	%rd108, %rd51, %rd107;
	add.s64 	%rd109, %rd105, %rd108;
	add.s64 	%rd167, %rd35, %rd109;
	shl.b64 	%rd110, %rd53, 1;
	add.s64 	%rd111, %rd1, %rd110;
	mul.lo.s64 	%rd112, %rd165, %rd55;
	shl.b64 	%rd113, %rd112, 1;
	mul.lo.s64 	%rd114, %rd150, %rd56;
	add.s64 	%rd115, %rd114, %rd113;
	add.s64 	%rd168, %rd111, %rd115;
	mov.u64 	%rd169, 0;

$L__BB1_25:
	shl.b64 	%rd147, %rd54, 1;
	shl.b64 	%rd146, %rd50, 1;
	ld.global.u16 	%rs1, [%rd167];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd168];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs2;}

	// end inline asm
	fma.rn.f32 	%f25, %f17, %f18, %f41;
	add.s64 	%rd116, %rd167, %rd146;
	ld.global.u16 	%rs3, [%rd116];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs3;}

	// end inline asm
	add.s64 	%rd117, %rd168, %rd147;
	ld.global.u16 	%rs4, [%rd117];
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs4;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f20, %f25;
	add.s64 	%rd118, %rd116, %rd146;
	ld.global.u16 	%rs5, [%rd118];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs5;}

	// end inline asm
	add.s64 	%rd119, %rd117, %rd147;
	ld.global.u16 	%rs6, [%rd119];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs6;}

	// end inline asm
	fma.rn.f32 	%f27, %f21, %f22, %f26;
	add.s64 	%rd120, %rd118, %rd146;
	add.s64 	%rd167, %rd120, %rd146;
	ld.global.u16 	%rs7, [%rd120];
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs7;}

	// end inline asm
	add.s64 	%rd121, %rd119, %rd147;
	add.s64 	%rd168, %rd121, %rd147;
	ld.global.u16 	%rs8, [%rd121];
	// begin inline asm
	{  cvt.f32.f16 %f24, %rs8;}

	// end inline asm
	fma.rn.f32 	%f41, %f23, %f24, %f27;
	add.s64 	%rd169, %rd169, 4;
	add.s64 	%rd122, %rd34, %rd169;
	setp.ne.s64 	%p17, %rd122, 0;
	@%p17 bra 	$L__BB1_25;

$L__BB1_26:
	mul.lo.s64 	%rd152, %rd165, %rd79;
	add.s64 	%rd151, %rd152, %rd40;
	setp.eq.s64 	%p18, %rd33, 0;
	sub.s64 	%rd123, %rd151, %rd78;
	mul.lo.s64 	%rd68, %rd51, %rd123;
	@%p18 bra 	$L__BB1_30;

	mul.lo.s64 	%rd155, %rd164, %rd79;
	add.s64 	%rd154, %rd155, %rd42;
	sub.s64 	%rd153, %rd154, %rd78;
	setp.eq.s64 	%p19, %rd33, 1;
	mul.lo.s64 	%rd124, %rd50, %rd169;
	add.s64 	%rd125, %rd124, %rd31;
	add.s64 	%rd126, %rd125, %rd68;
	mul.lo.s64 	%rd127, %rd52, %rd153;
	add.s64 	%rd128, %rd126, %rd127;
	mul.lo.s64 	%rd129, %rd54, %rd169;
	add.s64 	%rd130, %rd129, %rd53;
	mul.lo.s64 	%rd131, %rd55, %rd165;
	add.s64 	%rd132, %rd130, %rd131;
	mul.lo.s64 	%rd133, %rd56, %rd164;
	add.s64 	%rd134, %rd132, %rd133;
	shl.b64 	%rd135, %rd128, 1;
	add.s64 	%rd69, %rd2, %rd135;
	ld.global.u16 	%rs9, [%rd69];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs9;}

	// end inline asm
	shl.b64 	%rd136, %rd134, 1;
	add.s64 	%rd70, %rd1, %rd136;
	ld.global.u16 	%rs10, [%rd70];
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs10;}

	// end inline asm
	fma.rn.f32 	%f41, %f28, %f29, %f41;
	@%p19 bra 	$L__BB1_30;

	setp.eq.s64 	%p20, %rd33, 2;
	shl.b64 	%rd137, %rd50, 1;
	add.s64 	%rd71, %rd69, %rd137;
	ld.global.u16 	%rs11, [%rd71];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs11;}

	// end inline asm
	shl.b64 	%rd138, %rd54, 1;
	add.s64 	%rd72, %rd70, %rd138;
	ld.global.u16 	%rs12, [%rd72];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs12;}

	// end inline asm
	fma.rn.f32 	%f41, %f30, %f31, %f41;
	@%p20 bra 	$L__BB1_30;

	add.s64 	%rd140, %rd71, %rd137;
	ld.global.u16 	%rs13, [%rd140];
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs13;}

	// end inline asm
	add.s64 	%rd142, %rd72, %rd138;
	ld.global.u16 	%rs14, [%rd142];
	// begin inline asm
	{  cvt.f32.f16 %f33, %rs14;}

	// end inline asm
	fma.rn.f32 	%f41, %f32, %f33, %f41;

$L__BB1_30:
	add.s64 	%rd165, %rd165, 1;
	setp.lt.u64 	%p21, %rd165, %rd5;
	@%p21 bra 	$L__BB1_22;

$L__BB1_31:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p22, %rd164, %rd6;
	@%p22 bra 	$L__BB1_19;

$L__BB1_32:
	ld.param.u64 	%rd149, [conv2d_f16_param_9];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r22, %r23, %r24, %r25;
	cvt.u64.u32 	%rd148, %r22;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f41;}

	// end inline asm
	cvta.to.global.u64 	%rd143, %rd149;
	shl.b64 	%rd144, %rd148, 1;
	add.s64 	%rd145, %rd143, %rd144;
	st.global.u16 	[%rd145], %rs15;

$L__BB1_33:
	ret;

}
	// .globl	conv_transpose1d_f16
.visible .entry conv_transpose1d_f16(
	.param .u64 conv_transpose1d_f16_param_0,
	.param .u64 conv_transpose1d_f16_param_1,
	.param .u64 conv_transpose1d_f16_param_2,
	.param .u64 conv_transpose1d_f16_param_3,
	.param .u64 conv_transpose1d_f16_param_4,
	.param .u64 conv_transpose1d_f16_param_5,
	.param .u64 conv_transpose1d_f16_param_6,
	.param .u64 conv_transpose1d_f16_param_7,
	.param .u64 conv_transpose1d_f16_param_8,
	.param .u64 conv_transpose1d_f16_param_9
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<130>;


	ld.param.u64 	%rd62, [conv_transpose1d_f16_param_1];
	ld.param.u64 	%rd63, [conv_transpose1d_f16_param_2];
	ld.param.u64 	%rd64, [conv_transpose1d_f16_param_3];
	ld.param.u64 	%rd65, [conv_transpose1d_f16_param_5];
	ld.param.u64 	%rd67, [conv_transpose1d_f16_param_6];
	ld.param.u64 	%rd68, [conv_transpose1d_f16_param_7];
	ld.param.u64 	%rd69, [conv_transpose1d_f16_param_8];
	ld.param.u64 	%rd66, [conv_transpose1d_f16_param_9];
	cvta.to.global.u64 	%rd1, %rd69;
	cvta.to.global.u64 	%rd2, %rd68;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	cvt.u64.u32 	%rd3, %r7;
	cvta.to.global.u64 	%rd4, %rd67;
	ld.global.u64 	%rd5, [%rd4+8];
	ld.global.u64 	%rd6, [%rd4+56];
	mul.lo.s64 	%rd7, %rd6, %rd62;
	ld.global.u64 	%rd70, [%rd4];
	mul.lo.s64 	%rd71, %rd7, %rd70;
	setp.le.u64 	%p1, %rd71, %rd3;
	@%p1 bra 	$L__BB2_31;

	ld.global.u64 	%rd8, [%rd4+16];
	ld.global.u64 	%rd9, [%rd4+64];
	and.b64  	%rd72, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd72, 0;
	@%p2 bra 	$L__BB2_3;

	div.u64 	%rd118, %rd3, %rd7;
	bra.uni 	$L__BB2_4;

$L__BB2_3:
	cvt.u32.u64 	%r8, %rd7;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd118, %r10;

$L__BB2_4:
	and.b64  	%rd73, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd73, 0;
	@%p3 bra 	$L__BB2_6;

	div.u64 	%rd119, %rd3, %rd62;
	mul.lo.s64 	%rd74, %rd119, %rd62;
	sub.s64 	%rd120, %rd3, %rd74;
	bra.uni 	$L__BB2_7;

$L__BB2_6:
	cvt.u32.u64 	%r11, %rd62;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	mul.lo.s32 	%r14, %r13, %r11;
	sub.s32 	%r15, %r12, %r14;
	cvt.u64.u32 	%rd119, %r13;
	cvt.u64.u32 	%rd120, %r15;

$L__BB2_7:
	and.b64  	%rd75, %rd6, -4294967296;
	setp.eq.s64 	%p4, %rd75, 0;
	@%p4 bra 	$L__BB2_9;

	rem.u64 	%rd121, %rd119, %rd6;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r16, %rd6;
	cvt.u32.u64 	%r17, %rd119;
	rem.u32 	%r18, %r17, %r16;
	cvt.u64.u32 	%rd121, %r18;

$L__BB2_10:
	ld.global.u64 	%rd76, [%rd4+24];
	mul.lo.s64 	%rd22, %rd76, %rd118;
	cvt.u32.u64 	%r1, %rd9;
	setp.lt.s32 	%p5, %r1, 1;
	mov.f32 	%f38, 0f00000000;
	@%p5 bra 	$L__BB2_30;

	setp.eq.s64 	%p6, %rd5, 0;
	@%p6 bra 	$L__BB2_30;

	add.s64 	%rd23, %rd5, -1;
	and.b64  	%rd24, %rd5, 3;
	sub.s64 	%rd25, %rd5, %rd24;
	shl.b64 	%rd78, %rd22, 1;
	add.s64 	%rd26, %rd2, %rd78;
	add.s64 	%rd27, %rd120, %rd64;
	mov.f32 	%f38, 0f00000000;
	mov.u32 	%r28, 0;
	mov.u64 	%rd122, 0;
	cvt.u32.u64 	%r21, %rd63;

$L__BB2_13:
	cvt.s64.s32 	%rd29, %r28;
	mul.lo.s64 	%rd79, %rd29, %rd65;
	sub.s64 	%rd30, %rd27, %rd79;
	cvt.u32.u64 	%r20, %rd30;
	setp.lt.s32 	%p7, %r20, 0;
	@%p7 bra 	$L__BB2_29;

	cvt.s64.s32 	%rd31, %rd30;
	or.b64  	%rd80, %rd31, %rd63;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p8, %rd81, 0;
	@%p8 bra 	$L__BB2_16;

	rem.u64 	%rd123, %rd31, %rd63;
	bra.uni 	$L__BB2_17;

$L__BB2_16:
	cvt.u32.u64 	%r22, %rd31;
	rem.u32 	%r23, %r22, %r21;
	cvt.u64.u32 	%rd123, %r23;

$L__BB2_17:
	setp.ne.s64 	%p9, %rd123, 0;
	@%p9 bra 	$L__BB2_29;

	@%p8 bra 	$L__BB2_20;

	div.u64 	%rd124, %rd31, %rd63;
	bra.uni 	$L__BB2_21;

$L__BB2_20:
	cvt.u32.u64 	%r25, %rd31;
	div.u32 	%r26, %r25, %r21;
	cvt.u64.u32 	%rd124, %r26;

$L__BB2_21:
	cvt.s64.s32 	%rd38, %rd124;
	setp.ge.u64 	%p11, %rd38, %rd8;
	@%p11 bra 	$L__BB2_29;

	setp.lt.u64 	%p12, %rd23, 3;
	ld.global.u64 	%rd39, [%rd4+32];
	mov.u64 	%rd129, 0;
	ld.global.u64 	%rd85, [%rd4+40];
	mul.lo.s64 	%rd40, %rd85, %rd38;
	ld.global.u64 	%rd41, [%rd4+72];
	ld.global.u64 	%rd86, [%rd4+80];
	mul.lo.s64 	%rd42, %rd86, %rd121;
	ld.global.u64 	%rd43, [%rd4+88];
	@%p12 bra 	$L__BB2_25;

	shl.b64 	%rd88, %rd40, 1;
	add.s64 	%rd127, %rd26, %rd88;
	shl.b64 	%rd89, %rd42, 1;
	add.s64 	%rd90, %rd1, %rd89;
	mul.lo.s64 	%rd91, %rd43, %rd122;
	add.s64 	%rd128, %rd90, %rd91;
	mov.u64 	%rd129, 0;
	mov.u64 	%rd126, %rd25;

$L__BB2_24:
	shl.b64 	%rd117, %rd41, 1;
	shl.b64 	%rd116, %rd39, 1;
	ld.global.u16 	%rs1, [%rd127];
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd128];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs2;}

	// end inline asm
	fma.rn.f32 	%f23, %f15, %f16, %f38;
	add.s64 	%rd92, %rd127, %rd116;
	ld.global.u16 	%rs3, [%rd92];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs3;}

	// end inline asm
	add.s64 	%rd93, %rd128, %rd117;
	ld.global.u16 	%rs4, [%rd93];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs4;}

	// end inline asm
	fma.rn.f32 	%f24, %f17, %f18, %f23;
	add.s64 	%rd94, %rd92, %rd116;
	ld.global.u16 	%rs5, [%rd94];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs5;}

	// end inline asm
	add.s64 	%rd95, %rd93, %rd117;
	ld.global.u16 	%rs6, [%rd95];
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs6;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f20, %f24;
	add.s64 	%rd96, %rd94, %rd116;
	add.s64 	%rd127, %rd96, %rd116;
	ld.global.u16 	%rs7, [%rd96];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs7;}

	// end inline asm
	add.s64 	%rd97, %rd95, %rd117;
	add.s64 	%rd128, %rd97, %rd117;
	ld.global.u16 	%rs8, [%rd97];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs8;}

	// end inline asm
	fma.rn.f32 	%f38, %f21, %f22, %f25;
	add.s64 	%rd129, %rd129, 4;
	add.s64 	%rd126, %rd126, -4;
	setp.ne.s64 	%p13, %rd126, 0;
	@%p13 bra 	$L__BB2_24;

$L__BB2_25:
	setp.eq.s64 	%p14, %rd24, 0;
	@%p14 bra 	$L__BB2_29;

	setp.eq.s64 	%p15, %rd24, 1;
	mul.lo.s64 	%rd98, %rd39, %rd129;
	add.s64 	%rd99, %rd98, %rd22;
	add.s64 	%rd100, %rd99, %rd40;
	mul.lo.s64 	%rd101, %rd41, %rd129;
	add.s64 	%rd102, %rd42, %rd101;
	mul.lo.s64 	%rd103, %rd43, %rd29;
	add.s64 	%rd104, %rd102, %rd103;
	shl.b64 	%rd105, %rd100, 1;
	add.s64 	%rd57, %rd2, %rd105;
	ld.global.u16 	%rs9, [%rd57];
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs9;}

	// end inline asm
	shl.b64 	%rd106, %rd104, 1;
	add.s64 	%rd58, %rd1, %rd106;
	ld.global.u16 	%rs10, [%rd58];
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs10;}

	// end inline asm
	fma.rn.f32 	%f38, %f26, %f27, %f38;
	@%p15 bra 	$L__BB2_29;

	setp.eq.s64 	%p16, %rd24, 2;
	shl.b64 	%rd107, %rd39, 1;
	add.s64 	%rd59, %rd57, %rd107;
	ld.global.u16 	%rs11, [%rd59];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs11;}

	// end inline asm
	shl.b64 	%rd108, %rd41, 1;
	add.s64 	%rd60, %rd58, %rd108;
	ld.global.u16 	%rs12, [%rd60];
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs12;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f29, %f38;
	@%p16 bra 	$L__BB2_29;

	add.s64 	%rd110, %rd59, %rd107;
	ld.global.u16 	%rs13, [%rd110];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs13;}

	// end inline asm
	add.s64 	%rd112, %rd60, %rd108;
	ld.global.u16 	%rs14, [%rd112];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs14;}

	// end inline asm
	fma.rn.f32 	%f38, %f30, %f31, %f38;

$L__BB2_29:
	cvt.u32.u64 	%r27, %rd29;
	add.s32 	%r28, %r27, 1;
	setp.lt.s32 	%p17, %r28, %r1;
	add.s64 	%rd122, %rd122, 2;
	@%p17 bra 	$L__BB2_13;

$L__BB2_30:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f38;}

	// end inline asm
	cvta.to.global.u64 	%rd113, %rd66;
	shl.b64 	%rd114, %rd3, 1;
	add.s64 	%rd115, %rd113, %rd114;
	st.global.u16 	[%rd115], %rs15;

$L__BB2_31:
	ret;

}
	// .globl	conv_transpose2d_f16
.visible .entry conv_transpose2d_f16(
	.param .u64 conv_transpose2d_f16_param_0,
	.param .u64 conv_transpose2d_f16_param_1,
	.param .u64 conv_transpose2d_f16_param_2,
	.param .u64 conv_transpose2d_f16_param_3,
	.param .u64 conv_transpose2d_f16_param_4,
	.param .u64 conv_transpose2d_f16_param_5,
	.param .u64 conv_transpose2d_f16_param_6,
	.param .u64 conv_transpose2d_f16_param_7,
	.param .u64 conv_transpose2d_f16_param_8,
	.param .u64 conv_transpose2d_f16_param_9,
	.param .u64 conv_transpose2d_f16_param_10
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd84, [conv_transpose2d_f16_param_1];
	ld.param.u64 	%rd85, [conv_transpose2d_f16_param_2];
	ld.param.u64 	%rd86, [conv_transpose2d_f16_param_3];
	ld.param.u64 	%rd87, [conv_transpose2d_f16_param_4];
	ld.param.u64 	%rd88, [conv_transpose2d_f16_param_6];
	ld.param.u64 	%rd90, [conv_transpose2d_f16_param_7];
	ld.param.u64 	%rd91, [conv_transpose2d_f16_param_8];
	ld.param.u64 	%rd92, [conv_transpose2d_f16_param_9];
	cvta.to.global.u64 	%rd1, %rd92;
	cvta.to.global.u64 	%rd2, %rd91;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r10, %r8, %r7, %r9;
	cvt.u64.u32 	%rd3, %r10;
	cvta.to.global.u64 	%rd4, %rd90;
	ld.global.u64 	%rd5, [%rd4+8];
	mul.lo.s64 	%rd6, %rd85, %rd84;
	ld.global.u64 	%rd7, [%rd4+72];
	mul.lo.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd93, [%rd4];
	mul.lo.s64 	%rd94, %rd8, %rd93;
	setp.le.u64 	%p1, %rd94, %rd3;
	@%p1 bra 	$L__BB3_48;

	ld.global.u64 	%rd9, [%rd4+80];
	ld.global.u64 	%rd10, [%rd4+16];
	ld.global.u64 	%rd11, [%rd4+24];
	ld.global.u64 	%rd12, [%rd4+88];
	and.b64  	%rd95, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd95, 0;
	@%p2 bra 	$L__BB3_3;

	div.u64 	%rd160, %rd3, %rd8;
	bra.uni 	$L__BB3_4;

$L__BB3_3:
	cvt.u32.u64 	%r11, %rd8;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB3_4:
	and.b64  	%rd96, %rd6, -4294967296;
	setp.eq.s64 	%p3, %rd96, 0;
	@%p3 bra 	$L__BB3_6;

	div.u64 	%rd161, %rd3, %rd6;
	bra.uni 	$L__BB3_7;

$L__BB3_6:
	cvt.u32.u64 	%r14, %rd6;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd161, %r16;

$L__BB3_7:
	and.b64  	%rd97, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd97, 0;
	@%p4 bra 	$L__BB3_9;

	rem.u64 	%rd162, %rd161, %rd7;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r17, %rd7;
	cvt.u32.u64 	%r18, %rd161;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd162, %r19;

$L__BB3_10:
	and.b64  	%rd98, %rd84, -4294967296;
	setp.eq.s64 	%p5, %rd98, 0;
	@%p5 bra 	$L__BB3_12;

	div.u64 	%rd163, %rd3, %rd84;
	mul.lo.s64 	%rd99, %rd163, %rd84;
	sub.s64 	%rd164, %rd3, %rd99;
	bra.uni 	$L__BB3_13;

$L__BB3_12:
	cvt.u32.u64 	%r20, %rd84;
	cvt.u32.u64 	%r21, %rd3;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd163, %r22;
	cvt.u64.u32 	%rd164, %r24;

$L__BB3_13:
	and.b64  	%rd100, %rd85, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB3_15;

	rem.u64 	%rd165, %rd163, %rd85;
	bra.uni 	$L__BB3_16;

$L__BB3_15:
	cvt.u32.u64 	%r25, %rd85;
	cvt.u32.u64 	%r26, %rd163;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd165, %r27;

$L__BB3_16:
	ld.global.u64 	%rd101, [%rd4+32];
	mul.lo.s64 	%rd31, %rd101, %rd160;
	cvt.u32.u64 	%r1, %rd12;
	setp.lt.s32 	%p7, %r1, 1;
	mov.f32 	%f41, 0f00000000;
	@%p7 bra 	$L__BB3_47;

	cvt.u32.u64 	%r2, %rd9;
	setp.lt.s32 	%p8, %r2, 1;
	@%p8 bra 	$L__BB3_47;

	add.s64 	%rd32, %rd5, -1;
	and.b64  	%rd33, %rd5, 3;
	sub.s64 	%rd34, %rd33, %rd5;
	shl.b64 	%rd103, %rd31, 1;
	add.s64 	%rd35, %rd2, %rd103;
	add.s64 	%rd36, %rd165, %rd87;
	add.s64 	%rd37, %rd164, %rd87;
	mov.f32 	%f41, 0f00000000;
	mov.u32 	%r50, 0;
	mov.u64 	%rd166, 0;
	cvt.u32.u64 	%r30, %rd86;

$L__BB3_19:
	cvt.s64.s32 	%rd39, %r50;
	mul.lo.s64 	%rd104, %rd39, %rd88;
	sub.s64 	%rd40, %rd37, %rd104;
	cvt.u32.u64 	%r29, %rd40;
	setp.lt.s32 	%p9, %r29, 0;
	@%p9 bra 	$L__BB3_46;

	cvt.s64.s32 	%rd41, %rd40;
	or.b64  	%rd105, %rd41, %rd86;
	and.b64  	%rd106, %rd105, -4294967296;
	setp.eq.s64 	%p10, %rd106, 0;
	@%p10 bra 	$L__BB3_22;

	rem.u64 	%rd167, %rd41, %rd86;
	bra.uni 	$L__BB3_23;

$L__BB3_22:
	cvt.u32.u64 	%r31, %rd41;
	rem.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd167, %r32;

$L__BB3_23:
	setp.ne.s64 	%p11, %rd167, 0;
	@%p11 bra 	$L__BB3_46;

	@%p10 bra 	$L__BB3_26;

	div.u64 	%rd168, %rd41, %rd86;
	bra.uni 	$L__BB3_27;

$L__BB3_26:
	cvt.u32.u64 	%r34, %rd41;
	div.u32 	%r35, %r34, %r30;
	cvt.u64.u32 	%rd168, %r35;

$L__BB3_27:
	cvt.s64.s32 	%rd48, %rd168;
	setp.ge.u64 	%p13, %rd48, %rd11;
	setp.eq.s64 	%p14, %rd5, 0;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB3_46;

	mov.u32 	%r51, 0;
	mov.u64 	%rd169, 0;

$L__BB3_29:
	cvt.s64.s32 	%rd50, %r51;
	mul.lo.s64 	%rd110, %rd50, %rd88;
	sub.s64 	%rd51, %rd36, %rd110;
	cvt.u32.u64 	%r37, %rd51;
	setp.lt.s32 	%p16, %r37, 0;
	@%p16 bra 	$L__BB3_45;

	cvt.s64.s32 	%rd52, %rd51;
	or.b64  	%rd111, %rd52, %rd86;
	and.b64  	%rd112, %rd111, -4294967296;
	setp.eq.s64 	%p17, %rd112, 0;
	@%p17 bra 	$L__BB3_32;

	rem.u64 	%rd170, %rd52, %rd86;
	bra.uni 	$L__BB3_33;

$L__BB3_32:
	cvt.u32.u64 	%r39, %rd52;
	rem.u32 	%r40, %r39, %r30;
	cvt.u64.u32 	%rd170, %r40;

$L__BB3_33:
	setp.ne.s64 	%p18, %rd170, 0;
	@%p18 bra 	$L__BB3_45;

	@%p17 bra 	$L__BB3_36;

	div.u64 	%rd171, %rd52, %rd86;
	bra.uni 	$L__BB3_37;

$L__BB3_36:
	cvt.u32.u64 	%r42, %rd52;
	div.u32 	%r43, %r42, %r30;
	cvt.u64.u32 	%rd171, %r43;

$L__BB3_37:
	cvt.s64.s32 	%rd59, %rd171;
	setp.ge.u64 	%p20, %rd59, %rd10;
	@%p20 bra 	$L__BB3_45;

	setp.lt.u64 	%p21, %rd32, 3;
	ld.global.u64 	%rd60, [%rd4+40];
	mov.u64 	%rd175, 0;
	ld.global.u64 	%rd116, [%rd4+48];
	mul.lo.s64 	%rd61, %rd116, %rd59;
	ld.global.u64 	%rd117, [%rd4+56];
	mul.lo.s64 	%rd62, %rd117, %rd48;
	ld.global.u64 	%rd63, [%rd4+96];
	ld.global.u64 	%rd118, [%rd4+104];
	mul.lo.s64 	%rd64, %rd118, %rd162;
	ld.global.u64 	%rd65, [%rd4+112];
	ld.global.u64 	%rd66, [%rd4+120];
	@%p21 bra 	$L__BB3_41;

	add.s64 	%rd120, %rd62, %rd61;
	shl.b64 	%rd121, %rd120, 1;
	add.s64 	%rd173, %rd35, %rd121;
	shl.b64 	%rd122, %rd64, 1;
	add.s64 	%rd123, %rd1, %rd122;
	mul.lo.s64 	%rd124, %rd169, %rd65;
	shl.b64 	%rd125, %rd124, 1;
	mul.lo.s64 	%rd126, %rd166, %rd66;
	add.s64 	%rd127, %rd126, %rd125;
	add.s64 	%rd174, %rd123, %rd127;
	mov.u64 	%rd175, 0;

$L__BB3_40:
	shl.b64 	%rd157, %rd63, 1;
	shl.b64 	%rd156, %rd60, 1;
	ld.global.u16 	%rs1, [%rd173];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd174];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs2;}

	// end inline asm
	fma.rn.f32 	%f25, %f17, %f18, %f41;
	add.s64 	%rd128, %rd173, %rd156;
	ld.global.u16 	%rs3, [%rd128];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs3;}

	// end inline asm
	add.s64 	%rd129, %rd174, %rd157;
	ld.global.u16 	%rs4, [%rd129];
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs4;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f20, %f25;
	add.s64 	%rd130, %rd128, %rd156;
	ld.global.u16 	%rs5, [%rd130];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs5;}

	// end inline asm
	add.s64 	%rd131, %rd129, %rd157;
	ld.global.u16 	%rs6, [%rd131];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs6;}

	// end inline asm
	fma.rn.f32 	%f27, %f21, %f22, %f26;
	add.s64 	%rd132, %rd130, %rd156;
	add.s64 	%rd173, %rd132, %rd156;
	ld.global.u16 	%rs7, [%rd132];
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs7;}

	// end inline asm
	add.s64 	%rd133, %rd131, %rd157;
	add.s64 	%rd174, %rd133, %rd157;
	ld.global.u16 	%rs8, [%rd133];
	// begin inline asm
	{  cvt.f32.f16 %f24, %rs8;}

	// end inline asm
	fma.rn.f32 	%f41, %f23, %f24, %f27;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd134, %rd34, %rd175;
	setp.ne.s64 	%p22, %rd134, 0;
	@%p22 bra 	$L__BB3_40;

$L__BB3_41:
	setp.eq.s64 	%p23, %rd33, 0;
	@%p23 bra 	$L__BB3_45;

	setp.eq.s64 	%p24, %rd33, 1;
	mul.lo.s64 	%rd135, %rd60, %rd175;
	add.s64 	%rd136, %rd135, %rd31;
	add.s64 	%rd137, %rd136, %rd61;
	add.s64 	%rd138, %rd137, %rd62;
	mul.lo.s64 	%rd139, %rd63, %rd175;
	add.s64 	%rd140, %rd64, %rd139;
	mul.lo.s64 	%rd141, %rd65, %rd50;
	add.s64 	%rd142, %rd140, %rd141;
	mul.lo.s64 	%rd143, %rd66, %rd39;
	add.s64 	%rd144, %rd142, %rd143;
	shl.b64 	%rd145, %rd138, 1;
	add.s64 	%rd78, %rd2, %rd145;
	ld.global.u16 	%rs9, [%rd78];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs9;}

	// end inline asm
	shl.b64 	%rd146, %rd144, 1;
	add.s64 	%rd79, %rd1, %rd146;
	ld.global.u16 	%rs10, [%rd79];
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs10;}

	// end inline asm
	fma.rn.f32 	%f41, %f28, %f29, %f41;
	@%p24 bra 	$L__BB3_45;

	setp.eq.s64 	%p25, %rd33, 2;
	shl.b64 	%rd147, %rd60, 1;
	add.s64 	%rd80, %rd78, %rd147;
	ld.global.u16 	%rs11, [%rd80];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs11;}

	// end inline asm
	shl.b64 	%rd148, %rd63, 1;
	add.s64 	%rd81, %rd79, %rd148;
	ld.global.u16 	%rs12, [%rd81];
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs12;}

	// end inline asm
	fma.rn.f32 	%f41, %f30, %f31, %f41;
	@%p25 bra 	$L__BB3_45;

	add.s64 	%rd150, %rd80, %rd147;
	ld.global.u16 	%rs13, [%rd150];
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs13;}

	// end inline asm
	add.s64 	%rd152, %rd81, %rd148;
	ld.global.u16 	%rs14, [%rd152];
	// begin inline asm
	{  cvt.f32.f16 %f33, %rs14;}

	// end inline asm
	fma.rn.f32 	%f41, %f32, %f33, %f41;

$L__BB3_45:
	cvt.u32.u64 	%r44, %rd50;
	add.s32 	%r51, %r44, 1;
	setp.lt.s32 	%p26, %r51, %r2;
	add.s64 	%rd169, %rd169, 1;
	@%p26 bra 	$L__BB3_29;

$L__BB3_46:
	cvt.u32.u64 	%r45, %rd39;
	add.s32 	%r50, %r45, 1;
	setp.lt.s32 	%p27, %r50, %r1;
	add.s64 	%rd166, %rd166, 2;
	@%p27 bra 	$L__BB3_19;

$L__BB3_47:
	ld.param.u64 	%rd159, [conv_transpose2d_f16_param_10];
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r47, %ctaid.x;
	mad.lo.s32 	%r46, %r47, %r48, %r49;
	cvt.u64.u32 	%rd158, %r46;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f41;}

	// end inline asm
	cvta.to.global.u64 	%rd153, %rd159;
	shl.b64 	%rd154, %rd158, 1;
	add.s64 	%rd155, %rd153, %rd154;
	st.global.u16 	[%rd155], %rs15;

$L__BB3_48:
	ret;

}
	// .globl	avg_pool2d_f16
.visible .entry avg_pool2d_f16(
	.param .u64 avg_pool2d_f16_param_0,
	.param .u64 avg_pool2d_f16_param_1,
	.param .u64 avg_pool2d_f16_param_2,
	.param .u64 avg_pool2d_f16_param_3,
	.param .u64 avg_pool2d_f16_param_4,
	.param .u64 avg_pool2d_f16_param_5,
	.param .u64 avg_pool2d_f16_param_6,
	.param .u64 avg_pool2d_f16_param_7
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<46>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd58, [avg_pool2d_f16_param_1];
	ld.param.u64 	%rd59, [avg_pool2d_f16_param_2];
	ld.param.u64 	%rd60, [avg_pool2d_f16_param_3];
	ld.param.u64 	%rd61, [avg_pool2d_f16_param_4];
	ld.param.u64 	%rd63, [avg_pool2d_f16_param_5];
	ld.param.u64 	%rd64, [avg_pool2d_f16_param_6];
	ld.param.u64 	%rd62, [avg_pool2d_f16_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB4_2;

	div.u64 	%rd164, %rd7, %rd60;
	bra.uni 	$L__BB4_3;

$L__BB4_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd164, %r7;

$L__BB4_3:
	add.s64 	%rd11, %rd164, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB4_5;

	div.u64 	%rd165, %rd12, %rd61;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd165, %r10;

$L__BB4_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd165, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB4_48;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB4_9;

	div.u64 	%rd166, %rd2, %rd18;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd166, %r13;

$L__BB4_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB4_12;

	div.u64 	%rd167, %rd2, %rd17;
	bra.uni 	$L__BB4_13;

$L__BB4_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd167, %r16;

$L__BB4_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB4_15;

	rem.u64 	%rd168, %rd167, %rd4;
	bra.uni 	$L__BB4_16;

$L__BB4_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd167;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd168, %r19;

$L__BB4_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB4_18;

	div.u64 	%rd169, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd169, %rd16;
	sub.s64 	%rd170, %rd2, %rd77;
	bra.uni 	$L__BB4_19;

$L__BB4_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd169, %r22;
	cvt.u64.u32 	%rd170, %r24;

$L__BB4_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB4_21;

	rem.u64 	%rd171, %rd169, %rd11;
	bra.uni 	$L__BB4_22;

$L__BB4_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd169;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd171, %r27;

$L__BB4_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd166;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.f32 	%f36, 0f00000000;
	@%p9 bra 	$L__BB4_47;

	mul.lo.s64 	%rd38, %rd170, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB4_47;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd171, %rd60;
	mov.f32 	%f36, 0f00000000;
	mov.u64 	%rd172, 0;

$L__BB4_25:
	add.s64 	%rd44, %rd172, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB4_46;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd175, 0;
	@%p12 bra 	$L__BB4_37;

	mov.u64 	%rd175, 0;
	mov.u64 	%rd174, %rd41;

$L__BB4_28:
	add.s64 	%rd47, %rd175, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	@%p13 bra 	$L__BB4_30;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd168;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 1;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.u16 	%rs1, [%rd93];
	// begin inline asm
	{  cvt.f32.f16 %f24, %rs1;}

	// end inline asm
	add.f32 	%f36, %f36, %f24;

$L__BB4_30:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p14, %rd48, %rd5;
	@%p14 bra 	$L__BB4_32;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd168;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 1;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.u16 	%rs2, [%rd104];
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs2;}

	// end inline asm
	add.f32 	%f36, %f36, %f25;

$L__BB4_32:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p15, %rd49, %rd5;
	@%p15 bra 	$L__BB4_34;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd168;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 1;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.u16 	%rs3, [%rd115];
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs3;}

	// end inline asm
	add.f32 	%f36, %f36, %f26;

$L__BB4_34:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p16, %rd50, %rd5;
	@%p16 bra 	$L__BB4_36;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd168;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 1;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.u16 	%rs4, [%rd126];
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs4;}

	// end inline asm
	add.f32 	%f36, %f36, %f27;

$L__BB4_36:
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, -4;
	setp.ne.s64 	%p17, %rd174, 0;
	@%p17 bra 	$L__BB4_28;

$L__BB4_37:
	setp.eq.s64 	%p18, %rd40, 0;
	@%p18 bra 	$L__BB4_46;

	add.s64 	%rd54, %rd175, %rd38;
	setp.ge.u64 	%p19, %rd54, %rd5;
	@%p19 bra 	$L__BB4_40;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd168;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 1;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.u16 	%rs5, [%rd137];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs5;}

	// end inline asm
	add.f32 	%f36, %f36, %f28;

$L__BB4_40:
	setp.eq.s64 	%p20, %rd40, 1;
	@%p20 bra 	$L__BB4_46;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p21, %rd55, %rd5;
	@%p21 bra 	$L__BB4_43;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd168;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 1;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.u16 	%rs6, [%rd148];
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs6;}

	// end inline asm
	add.f32 	%f36, %f36, %f29;

$L__BB4_43:
	setp.eq.s64 	%p22, %rd40, 2;
	@%p22 bra 	$L__BB4_46;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p23, %rd56, %rd5;
	@%p23 bra 	$L__BB4_46;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd168;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 1;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.u16 	%rs7, [%rd159];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs7;}

	// end inline asm
	add.f32 	%f36, %f36, %f30;

$L__BB4_46:
	add.s64 	%rd172, %rd172, 1;
	setp.lt.u64 	%p24, %rd172, %rd58;
	@%p24 bra 	$L__BB4_25;

$L__BB4_47:
	mul.lo.s64 	%rd160, %rd59, %rd58;
	cvt.rn.f64.u64 	%fd1, %rd160;
	rcp.rn.f64 	%fd2, %fd1;
	cvt.rn.f32.f64 	%f32, %fd2;
	mul.f32 	%f31, %f36, %f32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f31;}

	// end inline asm
	cvta.to.global.u64 	%rd161, %rd62;
	shl.b64 	%rd162, %rd2, 1;
	add.s64 	%rd163, %rd161, %rd162;
	st.global.u16 	[%rd163], %rs8;

$L__BB4_48:
	ret;

}
	// .globl	max_pool2d_f16
.visible .entry max_pool2d_f16(
	.param .u64 max_pool2d_f16_param_0,
	.param .u64 max_pool2d_f16_param_1,
	.param .u64 max_pool2d_f16_param_2,
	.param .u64 max_pool2d_f16_param_3,
	.param .u64 max_pool2d_f16_param_4,
	.param .u64 max_pool2d_f16_param_5,
	.param .u64 max_pool2d_f16_param_6,
	.param .u64 max_pool2d_f16_param_7
)
{
	.reg .pred 	%p<46>;
	.reg .b16 	%rs<180>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<179>;


	ld.param.u64 	%rd58, [max_pool2d_f16_param_1];
	ld.param.u64 	%rd59, [max_pool2d_f16_param_2];
	ld.param.u64 	%rd60, [max_pool2d_f16_param_3];
	ld.param.u64 	%rd61, [max_pool2d_f16_param_4];
	ld.param.u64 	%rd63, [max_pool2d_f16_param_5];
	ld.param.u64 	%rd64, [max_pool2d_f16_param_6];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB5_2;

	div.u64 	%rd167, %rd7, %rd60;
	bra.uni 	$L__BB5_3;

$L__BB5_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd167, %r7;

$L__BB5_3:
	add.s64 	%rd11, %rd167, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB5_5;

	div.u64 	%rd168, %rd12, %rd61;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd168, %r10;

$L__BB5_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd168, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB5_76;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB5_9;

	div.u64 	%rd169, %rd2, %rd18;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd169, %r13;

$L__BB5_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB5_12;

	div.u64 	%rd170, %rd2, %rd17;
	bra.uni 	$L__BB5_13;

$L__BB5_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd170, %r16;

$L__BB5_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB5_15;

	rem.u64 	%rd171, %rd170, %rd4;
	bra.uni 	$L__BB5_16;

$L__BB5_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd170;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd171, %r19;

$L__BB5_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB5_18;

	div.u64 	%rd172, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd172, %rd16;
	sub.s64 	%rd173, %rd2, %rd77;
	bra.uni 	$L__BB5_19;

$L__BB5_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd172, %r22;
	cvt.u64.u32 	%rd173, %r24;

$L__BB5_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB5_21;

	rem.u64 	%rd174, %rd172, %rd11;
	bra.uni 	$L__BB5_22;

$L__BB5_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd172;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd174, %r27;

$L__BB5_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd169;
	mov.u32 	%r28, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs177, %r28;
	// end inline asm
	setp.eq.s64 	%p9, %rd58, 0;
	@%p9 bra 	$L__BB5_75;

	mul.lo.s64 	%rd38, %rd173, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB5_75;

	ld.param.u64 	%rd166, [max_pool2d_f16_param_3];
	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd174, %rd166;
	mov.u16 	%rs178, 0;
	mov.u64 	%rd175, 0;

$L__BB5_25:
	add.s64 	%rd44, %rd175, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB5_74;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd178, 0;
	mov.u16 	%rs165, %rs178;
	mov.u16 	%rs164, %rs177;
	@%p12 bra 	$L__BB5_53;

	mov.u64 	%rd178, 0;
	mov.u16 	%rs165, %rs178;
	mov.u16 	%rs164, %rs177;
	mov.u64 	%rd177, %rd41;

$L__BB5_28:
	add.s64 	%rd47, %rd178, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	mov.u16 	%rs155, %rs164;
	mov.u16 	%rs156, %rs165;
	@%p13 bra 	$L__BB5_34;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd171;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 1;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.u16 	%rs6, [%rd93];
	and.b16  	%rs60, %rs165, 255;
	setp.eq.s16 	%p14, %rs60, 0;
	mov.u16 	%rs156, 1;
	mov.u16 	%rs155, %rs6;
	@%p14 bra 	$L__BB5_34;

	// begin inline asm
	{set.nan.f16.f16 %rs61,%rs164,%rs164;
}
	// end inline asm
	setp.ne.s16 	%p15, %rs61, 0;
	mov.u16 	%rs155, 32767;
	@%p15 bra 	$L__BB5_33;

	// begin inline asm
	{set.nan.f16.f16 %rs65,%rs6,%rs6;
}
	// end inline asm
	setp.ne.s16 	%p16, %rs65, 0;
	@%p16 bra 	$L__BB5_33;

	// begin inline asm
	{  cvt.f32.f16 %f1, %rs164;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs6;}

	// end inline asm
	// begin inline asm
	{max.f32 %f3,%f1,%f2;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs155, %f3;}

	// end inline asm

$L__BB5_33:
	mov.u16 	%rs156, %rs165;

$L__BB5_34:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p17, %rd48, %rd5;
	mov.u16 	%rs158, %rs155;
	mov.u16 	%rs159, %rs156;
	@%p17 bra 	$L__BB5_40;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd171;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 1;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.u16 	%rs12, [%rd104];
	and.b16  	%rs73, %rs156, 255;
	setp.eq.s16 	%p18, %rs73, 0;
	mov.u16 	%rs159, 1;
	mov.u16 	%rs158, %rs12;
	@%p18 bra 	$L__BB5_40;

	// begin inline asm
	{set.nan.f16.f16 %rs74,%rs155,%rs155;
}
	// end inline asm
	setp.ne.s16 	%p19, %rs74, 0;
	mov.u16 	%rs158, 32767;
	@%p19 bra 	$L__BB5_39;

	// begin inline asm
	{set.nan.f16.f16 %rs78,%rs12,%rs12;
}
	// end inline asm
	setp.ne.s16 	%p20, %rs78, 0;
	@%p20 bra 	$L__BB5_39;

	// begin inline asm
	{  cvt.f32.f16 %f7, %rs155;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs12;}

	// end inline asm
	// begin inline asm
	{max.f32 %f9,%f7,%f8;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs158, %f9;}

	// end inline asm

$L__BB5_39:
	mov.u16 	%rs159, %rs156;

$L__BB5_40:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p21, %rd49, %rd5;
	mov.u16 	%rs161, %rs158;
	mov.u16 	%rs162, %rs159;
	@%p21 bra 	$L__BB5_46;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd171;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 1;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.u16 	%rs18, [%rd115];
	and.b16  	%rs86, %rs159, 255;
	setp.eq.s16 	%p22, %rs86, 0;
	mov.u16 	%rs162, 1;
	mov.u16 	%rs161, %rs18;
	@%p22 bra 	$L__BB5_46;

	// begin inline asm
	{set.nan.f16.f16 %rs87,%rs158,%rs158;
}
	// end inline asm
	setp.ne.s16 	%p23, %rs87, 0;
	mov.u16 	%rs161, 32767;
	@%p23 bra 	$L__BB5_45;

	// begin inline asm
	{set.nan.f16.f16 %rs91,%rs18,%rs18;
}
	// end inline asm
	setp.ne.s16 	%p24, %rs91, 0;
	@%p24 bra 	$L__BB5_45;

	// begin inline asm
	{  cvt.f32.f16 %f13, %rs158;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs18;}

	// end inline asm
	// begin inline asm
	{max.f32 %f15,%f13,%f14;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs161, %f15;}

	// end inline asm

$L__BB5_45:
	mov.u16 	%rs162, %rs159;

$L__BB5_46:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p25, %rd50, %rd5;
	mov.u16 	%rs164, %rs161;
	mov.u16 	%rs165, %rs162;
	@%p25 bra 	$L__BB5_52;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd171;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 1;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.u16 	%rs24, [%rd126];
	and.b16  	%rs99, %rs162, 255;
	setp.eq.s16 	%p26, %rs99, 0;
	mov.u16 	%rs165, 1;
	mov.u16 	%rs164, %rs24;
	@%p26 bra 	$L__BB5_52;

	// begin inline asm
	{set.nan.f16.f16 %rs100,%rs161,%rs161;
}
	// end inline asm
	setp.ne.s16 	%p27, %rs100, 0;
	mov.u16 	%rs164, 32767;
	@%p27 bra 	$L__BB5_51;

	// begin inline asm
	{set.nan.f16.f16 %rs104,%rs24,%rs24;
}
	// end inline asm
	setp.ne.s16 	%p28, %rs104, 0;
	@%p28 bra 	$L__BB5_51;

	// begin inline asm
	{  cvt.f32.f16 %f19, %rs161;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs24;}

	// end inline asm
	// begin inline asm
	{max.f32 %f21,%f19,%f20;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs164, %f21;}

	// end inline asm

$L__BB5_51:
	mov.u16 	%rs165, %rs162;

$L__BB5_52:
	add.s64 	%rd178, %rd178, 4;
	add.s64 	%rd177, %rd177, -4;
	setp.ne.s64 	%p29, %rd177, 0;
	@%p29 bra 	$L__BB5_28;

$L__BB5_53:
	setp.eq.s64 	%p30, %rd40, 0;
	mov.u16 	%rs177, %rs164;
	mov.u16 	%rs178, %rs165;
	@%p30 bra 	$L__BB5_74;

	add.s64 	%rd54, %rd178, %rd38;
	setp.ge.u64 	%p31, %rd54, %rd5;
	mov.u16 	%rs177, %rs164;
	mov.u16 	%rs178, %rs165;
	@%p31 bra 	$L__BB5_60;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd171;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 1;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.u16 	%rs34, [%rd137];
	and.b16  	%rs112, %rs165, 255;
	setp.eq.s16 	%p32, %rs112, 0;
	mov.u16 	%rs178, 1;
	mov.u16 	%rs177, %rs34;
	@%p32 bra 	$L__BB5_60;

	// begin inline asm
	{set.nan.f16.f16 %rs113,%rs164,%rs164;
}
	// end inline asm
	setp.ne.s16 	%p33, %rs113, 0;
	mov.u16 	%rs177, 32767;
	@%p33 bra 	$L__BB5_59;

	// begin inline asm
	{set.nan.f16.f16 %rs117,%rs34,%rs34;
}
	// end inline asm
	setp.ne.s16 	%p34, %rs117, 0;
	@%p34 bra 	$L__BB5_59;

	// begin inline asm
	{  cvt.f32.f16 %f25, %rs164;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs34;}

	// end inline asm
	// begin inline asm
	{max.f32 %f27,%f25,%f26;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f27;}

	// end inline asm

$L__BB5_59:
	mov.u16 	%rs178, %rs165;

$L__BB5_60:
	setp.eq.s64 	%p35, %rd40, 1;
	@%p35 bra 	$L__BB5_74;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p36, %rd55, %rd5;
	mov.u16 	%rs174, %rs177;
	mov.u16 	%rs175, %rs178;
	@%p36 bra 	$L__BB5_67;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd171;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 1;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.u16 	%rs40, [%rd148];
	and.b16  	%rs125, %rs178, 255;
	setp.eq.s16 	%p37, %rs125, 0;
	mov.u16 	%rs175, 1;
	mov.u16 	%rs174, %rs40;
	@%p37 bra 	$L__BB5_67;

	// begin inline asm
	{set.nan.f16.f16 %rs126,%rs177,%rs177;
}
	// end inline asm
	setp.ne.s16 	%p38, %rs126, 0;
	mov.u16 	%rs174, 32767;
	@%p38 bra 	$L__BB5_66;

	// begin inline asm
	{set.nan.f16.f16 %rs130,%rs40,%rs40;
}
	// end inline asm
	setp.ne.s16 	%p39, %rs130, 0;
	@%p39 bra 	$L__BB5_66;

	// begin inline asm
	{  cvt.f32.f16 %f31, %rs177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs40;}

	// end inline asm
	// begin inline asm
	{max.f32 %f33,%f31,%f32;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs174, %f33;}

	// end inline asm

$L__BB5_66:
	mov.u16 	%rs175, %rs178;

$L__BB5_67:
	setp.eq.s64 	%p40, %rd40, 2;
	mov.u16 	%rs177, %rs174;
	mov.u16 	%rs178, %rs175;
	@%p40 bra 	$L__BB5_74;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p41, %rd56, %rd5;
	mov.u16 	%rs177, %rs174;
	mov.u16 	%rs178, %rs175;
	@%p41 bra 	$L__BB5_74;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd171;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 1;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.u16 	%rs46, [%rd159];
	and.b16  	%rs138, %rs175, 255;
	setp.eq.s16 	%p42, %rs138, 0;
	mov.u16 	%rs178, 1;
	mov.u16 	%rs177, %rs46;
	@%p42 bra 	$L__BB5_74;

	// begin inline asm
	{set.nan.f16.f16 %rs139,%rs174,%rs174;
}
	// end inline asm
	setp.ne.s16 	%p43, %rs139, 0;
	mov.u16 	%rs177, 32767;
	@%p43 bra 	$L__BB5_73;

	// begin inline asm
	{set.nan.f16.f16 %rs143,%rs46,%rs46;
}
	// end inline asm
	setp.ne.s16 	%p44, %rs143, 0;
	@%p44 bra 	$L__BB5_73;

	// begin inline asm
	{  cvt.f32.f16 %f37, %rs174;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f38, %rs46;}

	// end inline asm
	// begin inline asm
	{max.f32 %f39,%f37,%f38;
}
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f39;}

	// end inline asm

$L__BB5_73:
	mov.u16 	%rs178, %rs175;

$L__BB5_74:
	ld.param.u64 	%rd163, [max_pool2d_f16_param_1];
	add.s64 	%rd175, %rd175, 1;
	setp.lt.u64 	%p45, %rd175, %rd163;
	@%p45 bra 	$L__BB5_25;

$L__BB5_75:
	ld.param.u64 	%rd165, [max_pool2d_f16_param_7];
	mov.u32 	%r32, %tid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r30, %ctaid.x;
	mad.lo.s32 	%r29, %r30, %r31, %r32;
	cvt.u64.u32 	%rd164, %r29;
	cvta.to.global.u64 	%rd160, %rd165;
	shl.b64 	%rd161, %rd164, 1;
	add.s64 	%rd162, %rd160, %rd161;
	st.global.u16 	[%rd162], %rs177;

$L__BB5_76:
	ret;

}
	// .globl	upsample_nearest2d_f16
.visible .entry upsample_nearest2d_f16(
	.param .u64 upsample_nearest2d_f16_param_0,
	.param .u64 upsample_nearest2d_f16_param_1,
	.param .f64 upsample_nearest2d_f16_param_2,
	.param .f64 upsample_nearest2d_f16_param_3,
	.param .u64 upsample_nearest2d_f16_param_4,
	.param .u64 upsample_nearest2d_f16_param_5,
	.param .u64 upsample_nearest2d_f16_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd26, [upsample_nearest2d_f16_param_0];
	ld.param.u64 	%rd27, [upsample_nearest2d_f16_param_1];
	ld.param.f64 	%fd1, [upsample_nearest2d_f16_param_2];
	ld.param.f64 	%fd2, [upsample_nearest2d_f16_param_3];
	ld.param.u64 	%rd30, [upsample_nearest2d_f16_param_4];
	ld.param.u64 	%rd28, [upsample_nearest2d_f16_param_5];
	ld.param.u64 	%rd29, [upsample_nearest2d_f16_param_6];
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	mul.lo.s64 	%rd3, %rd27, %rd26;
	ld.global.u64 	%rd4, [%rd2+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd31, [%rd2];
	mul.lo.s64 	%rd32, %rd5, %rd31;
	setp.le.u64 	%p1, %rd32, %rd1;
	@%p1 bra 	$L__BB6_17;

	ld.global.u64 	%rd6, [%rd2+16];
	ld.global.u64 	%rd7, [%rd2+24];
	and.b64  	%rd33, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB6_3;

	div.u64 	%rd62, %rd1, %rd5;
	bra.uni 	$L__BB6_4;

$L__BB6_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd62, %r7;

$L__BB6_4:
	and.b64  	%rd34, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB6_6;

	div.u64 	%rd63, %rd1, %rd3;
	bra.uni 	$L__BB6_7;

$L__BB6_6:
	cvt.u32.u64 	%r8, %rd3;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd63, %r10;

$L__BB6_7:
	and.b64  	%rd35, %rd4, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB6_9;

	rem.u64 	%rd64, %rd63, %rd4;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd63;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd64, %r13;

$L__BB6_10:
	and.b64  	%rd36, %rd27, -4294967296;
	setp.eq.s64 	%p5, %rd36, 0;
	@%p5 bra 	$L__BB6_12;

	div.u64 	%rd65, %rd1, %rd27;
	mul.lo.s64 	%rd37, %rd65, %rd27;
	sub.s64 	%rd66, %rd1, %rd37;
	bra.uni 	$L__BB6_13;

$L__BB6_12:
	cvt.u32.u64 	%r14, %rd27;
	cvt.u32.u64 	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd65, %r16;
	cvt.u64.u32 	%rd66, %r18;

$L__BB6_13:
	and.b64  	%rd38, %rd26, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB6_15;

	rem.u64 	%rd67, %rd65, %rd26;
	bra.uni 	$L__BB6_16;

$L__BB6_15:
	cvt.u32.u64 	%r19, %rd26;
	cvt.u32.u64 	%r20, %rd65;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd67, %r21;

$L__BB6_16:
	cvt.rn.f64.u64 	%fd3, %rd67;
	mul.f64 	%fd4, %fd3, %fd1;
	cvt.rzi.u64.f64 	%rd39, %fd4;
	cvt.rn.f64.u64 	%fd5, %rd66;
	mul.f64 	%fd6, %fd5, %fd2;
	cvt.rzi.u64.f64 	%rd40, %fd6;
	setp.lt.u64 	%p7, %rd39, %rd6;
	add.s64 	%rd41, %rd6, -1;
	selp.b64 	%rd42, %rd39, %rd41, %p7;
	setp.lt.u64 	%p8, %rd40, %rd7;
	add.s64 	%rd43, %rd7, -1;
	selp.b64 	%rd44, %rd40, %rd43, %p8;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd62;
	ld.global.u64 	%rd47, [%rd2+40];
	mul.lo.s64 	%rd48, %rd47, %rd64;
	add.s64 	%rd49, %rd48, %rd46;
	ld.global.u64 	%rd50, [%rd2+48];
	mul.lo.s64 	%rd51, %rd50, %rd42;
	add.s64 	%rd52, %rd49, %rd51;
	ld.global.u64 	%rd53, [%rd2+56];
	mul.lo.s64 	%rd54, %rd53, %rd44;
	add.s64 	%rd55, %rd52, %rd54;
	cvta.to.global.u64 	%rd56, %rd28;
	shl.b64 	%rd57, %rd55, 1;
	add.s64 	%rd58, %rd56, %rd57;
	ld.global.u16 	%rs1, [%rd58];
	cvta.to.global.u64 	%rd59, %rd29;
	shl.b64 	%rd60, %rd1, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB6_17:
	ret;

}
	// .globl	im2col_f16
.visible .entry im2col_f16(
	.param .u64 im2col_f16_param_0,
	.param .u64 im2col_f16_param_1,
	.param .u64 im2col_f16_param_2,
	.param .u64 im2col_f16_param_3,
	.param .u64 im2col_f16_param_4,
	.param .u64 im2col_f16_param_5,
	.param .u64 im2col_f16_param_6,
	.param .u64 im2col_f16_param_7,
	.param .u64 im2col_f16_param_8,
	.param .u64 im2col_f16_param_9,
	.param .u64 im2col_f16_param_10
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<87>;


	ld.param.u64 	%rd40, [im2col_f16_param_0];
	ld.param.u64 	%rd30, [im2col_f16_param_1];
	ld.param.u64 	%rd31, [im2col_f16_param_2];
	ld.param.u64 	%rd32, [im2col_f16_param_3];
	ld.param.u64 	%rd33, [im2col_f16_param_4];
	ld.param.u64 	%rd34, [im2col_f16_param_5];
	ld.param.u64 	%rd35, [im2col_f16_param_6];
	ld.param.u64 	%rd36, [im2col_f16_param_7];
	ld.param.u64 	%rd37, [im2col_f16_param_8];
	ld.param.u64 	%rd38, [im2col_f16_param_9];
	ld.param.u64 	%rd39, [im2col_f16_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd40;
	@%p1 bra 	$L__BB7_21;

	cvta.to.global.u64 	%rd2, %rd37;
	ld.global.u64 	%rd3, [%rd2+16];
	mul.lo.s64 	%rd4, %rd33, %rd32;
	ld.global.u64 	%rd41, [%rd2+8];
	mul.lo.s64 	%rd5, %rd41, %rd4;
	mul.lo.s64 	%rd6, %rd5, %rd31;
	mul.lo.s64 	%rd7, %rd6, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd42, 0;
	@%p2 bra 	$L__BB7_3;

	div.u64 	%rd82, %rd1, %rd7;
	bra.uni 	$L__BB7_4;

$L__BB7_3:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd82, %r7;

$L__BB7_4:
	mul.lo.s64 	%rd43, %rd7, %rd82;
	sub.s64 	%rd11, %rd1, %rd43;
	or.b64  	%rd44, %rd11, %rd6;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB7_6;

	div.u64 	%rd83, %rd11, %rd6;
	bra.uni 	$L__BB7_7;

$L__BB7_6:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd11;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd83, %r10;

$L__BB7_7:
	mul.lo.s64 	%rd46, %rd83, %rd6;
	sub.s64 	%rd15, %rd11, %rd46;
	or.b64  	%rd47, %rd15, %rd5;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB7_9;

	div.u64 	%rd84, %rd15, %rd5;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd15;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd84, %r13;

$L__BB7_10:
	mul.lo.s64 	%rd49, %rd84, %rd5;
	sub.s64 	%rd19, %rd15, %rd49;
	or.b64  	%rd50, %rd19, %rd4;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB7_12;

	div.u64 	%rd85, %rd19, %rd4;
	bra.uni 	$L__BB7_13;

$L__BB7_12:
	cvt.u32.u64 	%r14, %rd4;
	cvt.u32.u64 	%r15, %rd19;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd85, %r16;

$L__BB7_13:
	mul.lo.s64 	%rd52, %rd85, %rd4;
	sub.s64 	%rd23, %rd19, %rd52;
	or.b64  	%rd53, %rd23, %rd33;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB7_15;

	div.u64 	%rd86, %rd23, %rd33;
	bra.uni 	$L__BB7_16;

$L__BB7_15:
	cvt.u32.u64 	%r17, %rd33;
	cvt.u32.u64 	%r18, %rd23;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd86, %r19;

$L__BB7_16:
	mul.lo.s64 	%rd55, %rd86, %rd33;
	sub.s64 	%rd56, %rd23, %rd55;
	mul.lo.s64 	%rd57, %rd86, %rd36;
	mul.lo.s64 	%rd58, %rd83, %rd34;
	add.s64 	%rd27, %rd57, %rd58;
	mul.lo.s64 	%rd59, %rd56, %rd36;
	mul.lo.s64 	%rd60, %rd84, %rd34;
	add.s64 	%rd28, %rd59, %rd60;
	setp.lt.u64 	%p7, %rd27, %rd35;
	add.s64 	%rd61, %rd3, %rd35;
	setp.ge.u64 	%p8, %rd27, %rd61;
	or.pred  	%p9, %p7, %p8;
	cvta.to.global.u64 	%rd62, %rd39;
	shl.b64 	%rd63, %rd1, 1;
	add.s64 	%rd29, %rd62, %rd63;
	@%p9 bra 	$L__BB7_20;
	bra.uni 	$L__BB7_17;

$L__BB7_20:
	mov.u32 	%r21, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs3, %r21;
	// end inline asm
	st.global.u16 	[%rd29], %rs3;
	bra.uni 	$L__BB7_21;

$L__BB7_17:
	ld.global.u64 	%rd64, [%rd2+24];
	add.s64 	%rd65, %rd64, %rd35;
	setp.ge.u64 	%p10, %rd28, %rd65;
	setp.lt.u64 	%p11, %rd28, %rd35;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB7_19;
	bra.uni 	$L__BB7_18;

$L__BB7_19:
	mov.u32 	%r20, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs2, %r20;
	// end inline asm
	st.global.u16 	[%rd29], %rs2;
	bra.uni 	$L__BB7_21;

$L__BB7_18:
	sub.s64 	%rd66, %rd27, %rd35;
	ld.global.u64 	%rd67, [%rd2+32];
	mul.lo.s64 	%rd68, %rd67, %rd82;
	ld.global.u64 	%rd69, [%rd2+40];
	mul.lo.s64 	%rd70, %rd69, %rd85;
	add.s64 	%rd71, %rd70, %rd68;
	ld.global.u64 	%rd72, [%rd2+48];
	mul.lo.s64 	%rd73, %rd72, %rd66;
	add.s64 	%rd74, %rd71, %rd73;
	ld.global.u64 	%rd75, [%rd2+56];
	sub.s64 	%rd76, %rd28, %rd35;
	mul.lo.s64 	%rd77, %rd75, %rd76;
	add.s64 	%rd78, %rd74, %rd77;
	cvta.to.global.u64 	%rd79, %rd38;
	shl.b64 	%rd80, %rd78, 1;
	add.s64 	%rd81, %rd79, %rd80;
	ld.global.u16 	%rs1, [%rd81];
	st.global.u16 	[%rd29], %rs1;

$L__BB7_21:
	ret;

}
	// .globl	im2col1d_f16
.visible .entry im2col1d_f16(
	.param .u64 im2col1d_f16_param_0,
	.param .u64 im2col1d_f16_param_1,
	.param .u64 im2col1d_f16_param_2,
	.param .u64 im2col1d_f16_param_3,
	.param .u64 im2col1d_f16_param_4,
	.param .u64 im2col1d_f16_param_5,
	.param .u64 im2col1d_f16_param_6,
	.param .u64 im2col1d_f16_param_7,
	.param .u64 im2col1d_f16_param_8
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<58>;


	ld.param.u64 	%rd27, [im2col1d_f16_param_0];
	ld.param.u64 	%rd19, [im2col1d_f16_param_1];
	ld.param.u64 	%rd20, [im2col1d_f16_param_2];
	ld.param.u64 	%rd21, [im2col1d_f16_param_3];
	ld.param.u64 	%rd22, [im2col1d_f16_param_4];
	ld.param.u64 	%rd23, [im2col1d_f16_param_5];
	ld.param.u64 	%rd24, [im2col1d_f16_param_6];
	ld.param.u64 	%rd25, [im2col1d_f16_param_7];
	ld.param.u64 	%rd26, [im2col1d_f16_param_8];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd27;
	@%p1 bra 	$L__BB8_13;

	cvta.to.global.u64 	%rd2, %rd24;
	ld.global.u64 	%rd3, [%rd2+16];
	ld.global.u64 	%rd28, [%rd2+8];
	mul.lo.s64 	%rd4, %rd28, %rd20;
	mul.lo.s64 	%rd5, %rd4, %rd19;
	and.b64  	%rd29, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd29, 0;
	@%p2 bra 	$L__BB8_3;

	div.u64 	%rd55, %rd1, %rd5;
	bra.uni 	$L__BB8_4;

$L__BB8_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd55, %r7;

$L__BB8_4:
	mul.lo.s64 	%rd30, %rd55, %rd5;
	sub.s64 	%rd9, %rd1, %rd30;
	or.b64  	%rd31, %rd9, %rd4;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB8_6;

	div.u64 	%rd56, %rd9, %rd4;
	bra.uni 	$L__BB8_7;

$L__BB8_6:
	cvt.u32.u64 	%r8, %rd4;
	cvt.u32.u64 	%r9, %rd9;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd56, %r10;

$L__BB8_7:
	mul.lo.s64 	%rd33, %rd56, %rd4;
	sub.s64 	%rd13, %rd9, %rd33;
	or.b64  	%rd34, %rd13, %rd20;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB8_9;

	div.u64 	%rd57, %rd13, %rd20;
	bra.uni 	$L__BB8_10;

$L__BB8_9:
	cvt.u32.u64 	%r11, %rd20;
	cvt.u32.u64 	%r12, %rd13;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd57, %r13;

$L__BB8_10:
	mul.lo.s64 	%rd36, %rd57, %rd20;
	sub.s64 	%rd37, %rd13, %rd36;
	mul.lo.s64 	%rd38, %rd37, %rd23;
	mul.lo.s64 	%rd39, %rd56, %rd21;
	add.s64 	%rd17, %rd38, %rd39;
	setp.lt.u64 	%p5, %rd17, %rd22;
	add.s64 	%rd40, %rd3, %rd22;
	setp.ge.u64 	%p6, %rd17, %rd40;
	or.pred  	%p7, %p5, %p6;
	cvta.to.global.u64 	%rd41, %rd26;
	shl.b64 	%rd42, %rd1, 1;
	add.s64 	%rd18, %rd41, %rd42;
	@%p7 bra 	$L__BB8_12;
	bra.uni 	$L__BB8_11;

$L__BB8_12:
	mov.u32 	%r14, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs2, %r14;
	// end inline asm
	st.global.u16 	[%rd18], %rs2;
	bra.uni 	$L__BB8_13;

$L__BB8_11:
	sub.s64 	%rd43, %rd17, %rd22;
	ld.global.u64 	%rd44, [%rd2+24];
	mul.lo.s64 	%rd45, %rd44, %rd55;
	ld.global.u64 	%rd46, [%rd2+32];
	mul.lo.s64 	%rd47, %rd46, %rd57;
	add.s64 	%rd48, %rd47, %rd45;
	ld.global.u64 	%rd49, [%rd2+40];
	mul.lo.s64 	%rd50, %rd49, %rd43;
	add.s64 	%rd51, %rd48, %rd50;
	cvta.to.global.u64 	%rd52, %rd25;
	shl.b64 	%rd53, %rd51, 1;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.u16 	%rs1, [%rd54];
	st.global.u16 	[%rd18], %rs1;

$L__BB8_13:
	ret;

}
	// .globl	conv1d_f32
.visible .entry conv1d_f32(
	.param .u64 conv1d_f32_param_0,
	.param .u64 conv1d_f32_param_1,
	.param .u64 conv1d_f32_param_2,
	.param .u64 conv1d_f32_param_3,
	.param .u64 conv1d_f32_param_4,
	.param .u64 conv1d_f32_param_5,
	.param .u64 conv1d_f32_param_6,
	.param .u64 conv1d_f32_param_7,
	.param .u64 conv1d_f32_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd57, [conv1d_f32_param_1];
	ld.param.u64 	%rd58, [conv1d_f32_param_2];
	ld.param.u64 	%rd59, [conv1d_f32_param_3];
	ld.param.u64 	%rd60, [conv1d_f32_param_4];
	ld.param.u64 	%rd62, [conv1d_f32_param_5];
	ld.param.u64 	%rd63, [conv1d_f32_param_6];
	ld.param.u64 	%rd64, [conv1d_f32_param_7];
	ld.param.u64 	%rd61, [conv1d_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd64;
	cvta.to.global.u64 	%rd2, %rd63;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd62;
	ld.global.u64 	%rd5, [%rd4+64];
	ld.global.u64 	%rd6, [%rd4+8];
	ld.global.u64 	%rd7, [%rd4+48];
	mul.lo.s64 	%rd8, %rd7, %rd57;
	ld.global.u64 	%rd65, [%rd4];
	mul.lo.s64 	%rd66, %rd8, %rd65;
	setp.le.u64 	%p1, %rd66, %rd3;
	@%p1 bra 	$L__BB9_23;

	ld.global.u64 	%rd9, [%rd4+16];
	and.b64  	%rd67, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd67, 0;
	@%p2 bra 	$L__BB9_3;

	div.u64 	%rd118, %rd3, %rd8;
	bra.uni 	$L__BB9_4;

$L__BB9_3:
	cvt.u32.u64 	%r5, %rd8;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd118, %r7;

$L__BB9_4:
	and.b64  	%rd68, %rd57, -4294967296;
	setp.eq.s64 	%p3, %rd68, 0;
	@%p3 bra 	$L__BB9_6;

	div.u64 	%rd119, %rd3, %rd57;
	mul.lo.s64 	%rd69, %rd119, %rd57;
	sub.s64 	%rd120, %rd3, %rd69;
	bra.uni 	$L__BB9_7;

$L__BB9_6:
	cvt.u32.u64 	%r8, %rd57;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd119, %r10;
	cvt.u64.u32 	%rd120, %r12;

$L__BB9_7:
	and.b64  	%rd70, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB9_9;

	rem.u64 	%rd121, %rd119, %rd7;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r13, %rd7;
	cvt.u32.u64 	%r14, %rd119;
	rem.u32 	%r15, %r14, %r13;
	cvt.u64.u32 	%rd121, %r15;

$L__BB9_10:
	ld.global.u64 	%rd71, [%rd4+24];
	mul.lo.s64 	%rd22, %rd71, %rd118;
	setp.eq.s64 	%p5, %rd5, 0;
	mov.f32 	%f37, 0f00000000;
	@%p5 bra 	$L__BB9_22;

	setp.eq.s64 	%p6, %rd6, 0;
	@%p6 bra 	$L__BB9_22;

	add.s64 	%rd23, %rd6, -1;
	and.b64  	%rd24, %rd6, 3;
	sub.s64 	%rd25, %rd6, %rd24;
	shl.b64 	%rd73, %rd22, 2;
	add.s64 	%rd26, %rd2, %rd73;
	mul.lo.s64 	%rd74, %rd120, %rd60;
	mul.lo.s64 	%rd75, %rd74, %rd58;
	shl.b64 	%rd76, %rd75, 2;
	shl.b64 	%rd77, %rd59, 2;
	sub.s64 	%rd27, %rd76, %rd77;
	shl.b64 	%rd28, %rd60, 2;
	add.s64 	%rd29, %rd9, %rd59;
	mul.lo.s64 	%rd30, %rd120, %rd58;
	mov.f32 	%f37, 0f00000000;
	mov.u64 	%rd122, 0;

$L__BB9_13:
	ld.param.u64 	%rd117, [conv1d_f32_param_4];
	add.s64 	%rd78, %rd122, %rd30;
	mul.lo.s64 	%rd32, %rd78, %rd117;
	setp.lt.u64 	%p7, %rd32, %rd59;
	setp.ge.u64 	%p8, %rd32, %rd29;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB9_21;

	setp.lt.u64 	%p10, %rd23, 3;
	ld.global.u64 	%rd33, [%rd4+32];
	mov.u64 	%rd127, 0;
	ld.global.u64 	%rd34, [%rd4+40];
	ld.global.u64 	%rd80, [%rd4+72];
	mul.lo.s64 	%rd35, %rd80, %rd121;
	ld.global.u64 	%rd36, [%rd4+80];
	ld.global.u64 	%rd37, [%rd4+88];
	@%p10 bra 	$L__BB9_17;

	mul.lo.s64 	%rd82, %rd28, %rd122;
	add.s64 	%rd83, %rd27, %rd82;
	mul.lo.s64 	%rd84, %rd34, %rd83;
	add.s64 	%rd125, %rd26, %rd84;
	shl.b64 	%rd85, %rd35, 2;
	add.s64 	%rd86, %rd1, %rd85;
	mul.lo.s64 	%rd87, %rd122, %rd37;
	shl.b64 	%rd88, %rd87, 2;
	add.s64 	%rd126, %rd86, %rd88;
	mov.u64 	%rd127, 0;
	mov.u64 	%rd124, %rd25;

$L__BB9_16:
	shl.b64 	%rd115, %rd36, 2;
	shl.b64 	%rd114, %rd33, 2;
	ld.global.f32 	%f15, [%rd126];
	ld.global.f32 	%f16, [%rd125];
	fma.rn.f32 	%f17, %f16, %f15, %f37;
	add.s64 	%rd89, %rd125, %rd114;
	add.s64 	%rd90, %rd126, %rd115;
	ld.global.f32 	%f18, [%rd90];
	ld.global.f32 	%f19, [%rd89];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	add.s64 	%rd91, %rd89, %rd114;
	add.s64 	%rd92, %rd90, %rd115;
	ld.global.f32 	%f21, [%rd92];
	ld.global.f32 	%f22, [%rd91];
	fma.rn.f32 	%f23, %f22, %f21, %f20;
	add.s64 	%rd93, %rd91, %rd114;
	add.s64 	%rd125, %rd93, %rd114;
	add.s64 	%rd94, %rd92, %rd115;
	add.s64 	%rd126, %rd94, %rd115;
	ld.global.f32 	%f24, [%rd94];
	ld.global.f32 	%f25, [%rd93];
	fma.rn.f32 	%f37, %f25, %f24, %f23;
	add.s64 	%rd127, %rd127, 4;
	add.s64 	%rd124, %rd124, -4;
	setp.ne.s64 	%p11, %rd124, 0;
	@%p11 bra 	$L__BB9_16;

$L__BB9_17:
	setp.eq.s64 	%p12, %rd24, 0;
	sub.s64 	%rd95, %rd32, %rd59;
	mul.lo.s64 	%rd51, %rd34, %rd95;
	@%p12 bra 	$L__BB9_21;

	setp.eq.s64 	%p13, %rd24, 1;
	mul.lo.s64 	%rd96, %rd33, %rd127;
	add.s64 	%rd97, %rd96, %rd22;
	add.s64 	%rd98, %rd97, %rd51;
	mul.lo.s64 	%rd99, %rd36, %rd127;
	add.s64 	%rd100, %rd99, %rd35;
	mul.lo.s64 	%rd101, %rd37, %rd122;
	add.s64 	%rd102, %rd100, %rd101;
	shl.b64 	%rd103, %rd98, 2;
	add.s64 	%rd52, %rd2, %rd103;
	shl.b64 	%rd104, %rd102, 2;
	add.s64 	%rd53, %rd1, %rd104;
	ld.global.f32 	%f26, [%rd53];
	ld.global.f32 	%f27, [%rd52];
	fma.rn.f32 	%f37, %f27, %f26, %f37;
	@%p13 bra 	$L__BB9_21;

	setp.eq.s64 	%p14, %rd24, 2;
	shl.b64 	%rd105, %rd36, 2;
	add.s64 	%rd54, %rd53, %rd105;
	ld.global.f32 	%f28, [%rd54];
	shl.b64 	%rd106, %rd33, 2;
	add.s64 	%rd55, %rd52, %rd106;
	ld.global.f32 	%f29, [%rd55];
	fma.rn.f32 	%f37, %f29, %f28, %f37;
	@%p14 bra 	$L__BB9_21;

	add.s64 	%rd108, %rd55, %rd106;
	add.s64 	%rd110, %rd54, %rd105;
	ld.global.f32 	%f30, [%rd110];
	ld.global.f32 	%f31, [%rd108];
	fma.rn.f32 	%f37, %f31, %f30, %f37;

$L__BB9_21:
	add.s64 	%rd122, %rd122, 1;
	setp.lt.u64 	%p15, %rd122, %rd5;
	@%p15 bra 	$L__BB9_13;

$L__BB9_22:
	mov.u32 	%r19, %tid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r16, %r17, %r18, %r19;
	cvt.u64.u32 	%rd116, %r16;
	cvta.to.global.u64 	%rd111, %rd61;
	shl.b64 	%rd112, %rd116, 2;
	add.s64 	%rd113, %rd111, %rd112;
	st.global.f32 	[%rd113], %f37;

$L__BB9_23:
	ret;

}
	// .globl	conv1d_f64
.visible .entry conv1d_f64(
	.param .u64 conv1d_f64_param_0,
	.param .u64 conv1d_f64_param_1,
	.param .u64 conv1d_f64_param_2,
	.param .u64 conv1d_f64_param_3,
	.param .u64 conv1d_f64_param_4,
	.param .u64 conv1d_f64_param_5,
	.param .u64 conv1d_f64_param_6,
	.param .u64 conv1d_f64_param_7,
	.param .u64 conv1d_f64_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<39>;
	.reg .b64 	%rd<129>;


	ld.param.u64 	%rd57, [conv1d_f64_param_1];
	ld.param.u64 	%rd58, [conv1d_f64_param_2];
	ld.param.u64 	%rd59, [conv1d_f64_param_3];
	ld.param.u64 	%rd60, [conv1d_f64_param_4];
	ld.param.u64 	%rd62, [conv1d_f64_param_5];
	ld.param.u64 	%rd63, [conv1d_f64_param_6];
	ld.param.u64 	%rd64, [conv1d_f64_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	cvta.to.global.u64 	%rd2, %rd63;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd62;
	ld.global.u64 	%rd5, [%rd4+64];
	ld.global.u64 	%rd6, [%rd4+8];
	ld.global.u64 	%rd7, [%rd4+48];
	mul.lo.s64 	%rd8, %rd7, %rd57;
	ld.global.u64 	%rd65, [%rd4];
	mul.lo.s64 	%rd66, %rd8, %rd65;
	setp.le.u64 	%p1, %rd66, %rd3;
	@%p1 bra 	$L__BB10_23;

	ld.global.u64 	%rd9, [%rd4+16];
	and.b64  	%rd67, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd67, 0;
	@%p2 bra 	$L__BB10_3;

	div.u64 	%rd119, %rd3, %rd8;
	bra.uni 	$L__BB10_4;

$L__BB10_3:
	cvt.u32.u64 	%r5, %rd8;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd119, %r7;

$L__BB10_4:
	and.b64  	%rd68, %rd57, -4294967296;
	setp.eq.s64 	%p3, %rd68, 0;
	@%p3 bra 	$L__BB10_6;

	div.u64 	%rd120, %rd3, %rd57;
	mul.lo.s64 	%rd69, %rd120, %rd57;
	sub.s64 	%rd121, %rd3, %rd69;
	bra.uni 	$L__BB10_7;

$L__BB10_6:
	cvt.u32.u64 	%r8, %rd57;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd120, %r10;
	cvt.u64.u32 	%rd121, %r12;

$L__BB10_7:
	and.b64  	%rd70, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB10_9;

	rem.u64 	%rd122, %rd120, %rd7;
	bra.uni 	$L__BB10_10;

$L__BB10_9:
	cvt.u32.u64 	%r13, %rd7;
	cvt.u32.u64 	%r14, %rd120;
	rem.u32 	%r15, %r14, %r13;
	cvt.u64.u32 	%rd122, %r15;

$L__BB10_10:
	ld.global.u64 	%rd71, [%rd4+24];
	mul.lo.s64 	%rd22, %rd71, %rd119;
	setp.eq.s64 	%p5, %rd5, 0;
	mov.f64 	%fd37, 0d0000000000000000;
	@%p5 bra 	$L__BB10_22;

	setp.eq.s64 	%p6, %rd6, 0;
	@%p6 bra 	$L__BB10_22;

	add.s64 	%rd23, %rd6, -1;
	and.b64  	%rd24, %rd6, 3;
	sub.s64 	%rd25, %rd6, %rd24;
	shl.b64 	%rd73, %rd22, 3;
	add.s64 	%rd26, %rd2, %rd73;
	mul.lo.s64 	%rd74, %rd121, %rd60;
	mul.lo.s64 	%rd75, %rd74, %rd58;
	shl.b64 	%rd76, %rd75, 3;
	shl.b64 	%rd77, %rd59, 3;
	sub.s64 	%rd27, %rd76, %rd77;
	shl.b64 	%rd28, %rd60, 3;
	add.s64 	%rd29, %rd9, %rd59;
	mul.lo.s64 	%rd30, %rd121, %rd58;
	mov.f64 	%fd37, 0d0000000000000000;
	mov.u64 	%rd123, 0;

$L__BB10_13:
	ld.param.u64 	%rd117, [conv1d_f64_param_4];
	add.s64 	%rd78, %rd123, %rd30;
	mul.lo.s64 	%rd32, %rd78, %rd117;
	setp.lt.u64 	%p7, %rd32, %rd59;
	setp.ge.u64 	%p8, %rd32, %rd29;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB10_21;

	setp.lt.u64 	%p10, %rd23, 3;
	ld.global.u64 	%rd33, [%rd4+32];
	mov.u64 	%rd128, 0;
	ld.global.u64 	%rd34, [%rd4+40];
	ld.global.u64 	%rd80, [%rd4+72];
	mul.lo.s64 	%rd35, %rd80, %rd122;
	ld.global.u64 	%rd36, [%rd4+80];
	ld.global.u64 	%rd37, [%rd4+88];
	@%p10 bra 	$L__BB10_17;

	mul.lo.s64 	%rd82, %rd28, %rd123;
	add.s64 	%rd83, %rd27, %rd82;
	mul.lo.s64 	%rd84, %rd34, %rd83;
	add.s64 	%rd126, %rd26, %rd84;
	shl.b64 	%rd85, %rd35, 3;
	add.s64 	%rd86, %rd1, %rd85;
	mul.lo.s64 	%rd87, %rd123, %rd37;
	shl.b64 	%rd88, %rd87, 3;
	add.s64 	%rd127, %rd86, %rd88;
	mov.u64 	%rd128, 0;
	mov.u64 	%rd125, %rd25;

$L__BB10_16:
	shl.b64 	%rd115, %rd36, 3;
	shl.b64 	%rd114, %rd33, 3;
	ld.global.f64 	%fd15, [%rd127];
	ld.global.f64 	%fd16, [%rd126];
	fma.rn.f64 	%fd17, %fd16, %fd15, %fd37;
	add.s64 	%rd89, %rd126, %rd114;
	add.s64 	%rd90, %rd127, %rd115;
	ld.global.f64 	%fd18, [%rd90];
	ld.global.f64 	%fd19, [%rd89];
	fma.rn.f64 	%fd20, %fd19, %fd18, %fd17;
	add.s64 	%rd91, %rd89, %rd114;
	add.s64 	%rd92, %rd90, %rd115;
	ld.global.f64 	%fd21, [%rd92];
	ld.global.f64 	%fd22, [%rd91];
	fma.rn.f64 	%fd23, %fd22, %fd21, %fd20;
	add.s64 	%rd93, %rd91, %rd114;
	add.s64 	%rd126, %rd93, %rd114;
	add.s64 	%rd94, %rd92, %rd115;
	add.s64 	%rd127, %rd94, %rd115;
	ld.global.f64 	%fd24, [%rd94];
	ld.global.f64 	%fd25, [%rd93];
	fma.rn.f64 	%fd37, %fd25, %fd24, %fd23;
	add.s64 	%rd128, %rd128, 4;
	add.s64 	%rd125, %rd125, -4;
	setp.ne.s64 	%p11, %rd125, 0;
	@%p11 bra 	$L__BB10_16;

$L__BB10_17:
	setp.eq.s64 	%p12, %rd24, 0;
	sub.s64 	%rd95, %rd32, %rd59;
	mul.lo.s64 	%rd51, %rd34, %rd95;
	@%p12 bra 	$L__BB10_21;

	setp.eq.s64 	%p13, %rd24, 1;
	mul.lo.s64 	%rd96, %rd33, %rd128;
	add.s64 	%rd97, %rd96, %rd22;
	add.s64 	%rd98, %rd97, %rd51;
	mul.lo.s64 	%rd99, %rd36, %rd128;
	add.s64 	%rd100, %rd99, %rd35;
	mul.lo.s64 	%rd101, %rd37, %rd123;
	add.s64 	%rd102, %rd100, %rd101;
	shl.b64 	%rd103, %rd98, 3;
	add.s64 	%rd52, %rd2, %rd103;
	shl.b64 	%rd104, %rd102, 3;
	add.s64 	%rd53, %rd1, %rd104;
	ld.global.f64 	%fd26, [%rd53];
	ld.global.f64 	%fd27, [%rd52];
	fma.rn.f64 	%fd37, %fd27, %fd26, %fd37;
	@%p13 bra 	$L__BB10_21;

	setp.eq.s64 	%p14, %rd24, 2;
	shl.b64 	%rd105, %rd36, 3;
	add.s64 	%rd54, %rd53, %rd105;
	ld.global.f64 	%fd28, [%rd54];
	shl.b64 	%rd106, %rd33, 3;
	add.s64 	%rd55, %rd52, %rd106;
	ld.global.f64 	%fd29, [%rd55];
	fma.rn.f64 	%fd37, %fd29, %fd28, %fd37;
	@%p14 bra 	$L__BB10_21;

	add.s64 	%rd108, %rd55, %rd106;
	add.s64 	%rd110, %rd54, %rd105;
	ld.global.f64 	%fd30, [%rd110];
	ld.global.f64 	%fd31, [%rd108];
	fma.rn.f64 	%fd37, %fd31, %fd30, %fd37;

$L__BB10_21:
	add.s64 	%rd123, %rd123, 1;
	setp.lt.u64 	%p15, %rd123, %rd5;
	@%p15 bra 	$L__BB10_13;

$L__BB10_22:
	ld.param.u64 	%rd118, [conv1d_f64_param_8];
	mov.u32 	%r19, %tid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r16, %r17, %r18, %r19;
	cvt.u64.u32 	%rd116, %r16;
	cvta.to.global.u64 	%rd111, %rd118;
	shl.b64 	%rd112, %rd116, 3;
	add.s64 	%rd113, %rd111, %rd112;
	st.global.f64 	[%rd113], %fd37;

$L__BB10_23:
	ret;

}
	// .globl	conv1d_u8
.visible .entry conv1d_u8(
	.param .u64 conv1d_u8_param_0,
	.param .u64 conv1d_u8_param_1,
	.param .u64 conv1d_u8_param_2,
	.param .u64 conv1d_u8_param_3,
	.param .u64 conv1d_u8_param_4,
	.param .u64 conv1d_u8_param_5,
	.param .u64 conv1d_u8_param_6,
	.param .u64 conv1d_u8_param_7,
	.param .u64 conv1d_u8_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<102>;


	ld.param.u64 	%rd52, [conv1d_u8_param_1];
	ld.param.u64 	%rd53, [conv1d_u8_param_2];
	ld.param.u64 	%rd54, [conv1d_u8_param_3];
	ld.param.u64 	%rd55, [conv1d_u8_param_4];
	ld.param.u64 	%rd57, [conv1d_u8_param_5];
	ld.param.u64 	%rd58, [conv1d_u8_param_6];
	ld.param.u64 	%rd59, [conv1d_u8_param_7];
	ld.param.u64 	%rd56, [conv1d_u8_param_8];
	cvta.to.global.u64 	%rd1, %rd59;
	cvta.to.global.u64 	%rd2, %rd58;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd57;
	ld.global.u64 	%rd5, [%rd4+64];
	ld.global.u64 	%rd6, [%rd4+8];
	ld.global.u64 	%rd7, [%rd4+48];
	mul.lo.s64 	%rd8, %rd7, %rd52;
	ld.global.u64 	%rd60, [%rd4];
	mul.lo.s64 	%rd61, %rd8, %rd60;
	setp.le.u64 	%p1, %rd61, %rd3;
	@%p1 bra 	$L__BB11_23;

	ld.global.u64 	%rd9, [%rd4+16];
	and.b64  	%rd62, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd62, 0;
	@%p2 bra 	$L__BB11_3;

	div.u64 	%rd92, %rd3, %rd8;
	bra.uni 	$L__BB11_4;

$L__BB11_3:
	cvt.u32.u64 	%r5, %rd8;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd92, %r7;

$L__BB11_4:
	and.b64  	%rd63, %rd52, -4294967296;
	setp.eq.s64 	%p3, %rd63, 0;
	@%p3 bra 	$L__BB11_6;

	div.u64 	%rd93, %rd3, %rd52;
	mul.lo.s64 	%rd64, %rd93, %rd52;
	sub.s64 	%rd94, %rd3, %rd64;
	bra.uni 	$L__BB11_7;

$L__BB11_6:
	cvt.u32.u64 	%r8, %rd52;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd93, %r10;
	cvt.u64.u32 	%rd94, %r12;

$L__BB11_7:
	and.b64  	%rd65, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd65, 0;
	@%p4 bra 	$L__BB11_9;

	rem.u64 	%rd95, %rd93, %rd7;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r13, %rd7;
	cvt.u32.u64 	%r14, %rd93;
	rem.u32 	%r15, %r14, %r13;
	cvt.u64.u32 	%rd95, %r15;

$L__BB11_10:
	ld.global.u64 	%rd66, [%rd4+24];
	mul.lo.s64 	%rd22, %rd66, %rd92;
	setp.eq.s64 	%p5, %rd5, 0;
	mov.u16 	%rs43, 0;
	@%p5 bra 	$L__BB11_22;

	setp.eq.s64 	%p6, %rd6, 0;
	@%p6 bra 	$L__BB11_22;

	add.s64 	%rd23, %rd6, -1;
	and.b64  	%rd24, %rd6, 3;
	sub.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd2, %rd22;
	add.s64 	%rd27, %rd9, %rd54;
	mul.lo.s64 	%rd28, %rd94, %rd53;
	mov.u16 	%rs43, 0;
	mov.u64 	%rd96, 0;

$L__BB11_13:
	add.s64 	%rd68, %rd96, %rd28;
	mul.lo.s64 	%rd30, %rd68, %rd55;
	setp.lt.u64 	%p7, %rd30, %rd54;
	setp.ge.u64 	%p8, %rd30, %rd27;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB11_21;

	setp.lt.u64 	%p10, %rd23, 3;
	ld.global.u64 	%rd31, [%rd4+32];
	mov.u64 	%rd101, 0;
	ld.global.u64 	%rd70, [%rd4+40];
	sub.s64 	%rd71, %rd30, %rd54;
	mul.lo.s64 	%rd32, %rd70, %rd71;
	ld.global.u64 	%rd72, [%rd4+72];
	mul.lo.s64 	%rd33, %rd72, %rd95;
	ld.global.u64 	%rd34, [%rd4+80];
	ld.global.u64 	%rd73, [%rd4+88];
	mul.lo.s64 	%rd35, %rd73, %rd96;
	@%p10 bra 	$L__BB11_17;

	add.s64 	%rd99, %rd26, %rd32;
	add.s64 	%rd75, %rd33, %rd35;
	add.s64 	%rd100, %rd1, %rd75;
	mov.u64 	%rd101, 0;
	mov.u64 	%rd98, %rd25;

$L__BB11_16:
	ld.global.u8 	%rs15, [%rd100];
	ld.global.u8 	%rs16, [%rd99];
	mul.lo.s16 	%rs17, %rs15, %rs16;
	add.s16 	%rs18, %rs17, %rs43;
	add.s64 	%rd76, %rd100, %rd34;
	ld.global.u8 	%rs19, [%rd76];
	add.s64 	%rd77, %rd99, %rd31;
	ld.global.u8 	%rs20, [%rd77];
	mul.lo.s16 	%rs21, %rs19, %rs20;
	add.s16 	%rs22, %rs21, %rs18;
	add.s64 	%rd78, %rd77, %rd31;
	add.s64 	%rd79, %rd76, %rd34;
	ld.global.u8 	%rs23, [%rd79];
	ld.global.u8 	%rs24, [%rd78];
	mul.lo.s16 	%rs25, %rs23, %rs24;
	add.s16 	%rs26, %rs25, %rs22;
	add.s64 	%rd80, %rd78, %rd31;
	add.s64 	%rd99, %rd80, %rd31;
	add.s64 	%rd81, %rd79, %rd34;
	add.s64 	%rd100, %rd81, %rd34;
	ld.global.u8 	%rs27, [%rd81];
	ld.global.u8 	%rs28, [%rd80];
	mul.lo.s16 	%rs29, %rs27, %rs28;
	add.s16 	%rs43, %rs29, %rs26;
	add.s64 	%rd101, %rd101, 4;
	add.s64 	%rd98, %rd98, -4;
	setp.ne.s64 	%p11, %rd98, 0;
	@%p11 bra 	$L__BB11_16;

$L__BB11_17:
	setp.eq.s64 	%p12, %rd24, 0;
	@%p12 bra 	$L__BB11_21;

	setp.eq.s64 	%p13, %rd24, 1;
	mul.lo.s64 	%rd82, %rd31, %rd101;
	add.s64 	%rd83, %rd82, %rd22;
	add.s64 	%rd84, %rd83, %rd32;
	mul.lo.s64 	%rd85, %rd34, %rd101;
	add.s64 	%rd86, %rd85, %rd33;
	add.s64 	%rd87, %rd86, %rd35;
	add.s64 	%rd47, %rd2, %rd84;
	add.s64 	%rd48, %rd1, %rd87;
	ld.global.u8 	%rs30, [%rd48];
	ld.global.u8 	%rs31, [%rd47];
	mul.lo.s16 	%rs32, %rs30, %rs31;
	add.s16 	%rs43, %rs32, %rs43;
	@%p13 bra 	$L__BB11_21;

	setp.eq.s64 	%p14, %rd24, 2;
	add.s64 	%rd49, %rd48, %rd34;
	ld.global.u8 	%rs33, [%rd49];
	add.s64 	%rd50, %rd47, %rd31;
	ld.global.u8 	%rs34, [%rd50];
	mul.lo.s16 	%rs35, %rs33, %rs34;
	add.s16 	%rs43, %rs35, %rs43;
	@%p14 bra 	$L__BB11_21;

	add.s64 	%rd88, %rd50, %rd31;
	add.s64 	%rd89, %rd49, %rd34;
	ld.global.u8 	%rs36, [%rd89];
	ld.global.u8 	%rs37, [%rd88];
	mul.lo.s16 	%rs38, %rs36, %rs37;
	add.s16 	%rs43, %rs38, %rs43;

$L__BB11_21:
	add.s64 	%rd96, %rd96, 1;
	setp.lt.u64 	%p15, %rd96, %rd5;
	@%p15 bra 	$L__BB11_13;

$L__BB11_22:
	cvta.to.global.u64 	%rd90, %rd56;
	add.s64 	%rd91, %rd90, %rd3;
	st.global.u8 	[%rd91], %rs43;

$L__BB11_23:
	ret;

}
	// .globl	conv1d_u32
.visible .entry conv1d_u32(
	.param .u64 conv1d_u32_param_0,
	.param .u64 conv1d_u32_param_1,
	.param .u64 conv1d_u32_param_2,
	.param .u64 conv1d_u32_param_3,
	.param .u64 conv1d_u32_param_4,
	.param .u64 conv1d_u32_param_5,
	.param .u64 conv1d_u32_param_6,
	.param .u64 conv1d_u32_param_7,
	.param .u64 conv1d_u32_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<57>;
	.reg .b64 	%rd<128>;


	ld.param.u64 	%rd57, [conv1d_u32_param_1];
	ld.param.u64 	%rd58, [conv1d_u32_param_2];
	ld.param.u64 	%rd59, [conv1d_u32_param_3];
	ld.param.u64 	%rd60, [conv1d_u32_param_4];
	ld.param.u64 	%rd62, [conv1d_u32_param_5];
	ld.param.u64 	%rd63, [conv1d_u32_param_6];
	ld.param.u64 	%rd64, [conv1d_u32_param_7];
	ld.param.u64 	%rd61, [conv1d_u32_param_8];
	cvta.to.global.u64 	%rd1, %rd64;
	cvta.to.global.u64 	%rd2, %rd63;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r14, %r12, %r11, %r13;
	cvt.u64.u32 	%rd3, %r14;
	cvta.to.global.u64 	%rd4, %rd62;
	ld.global.u64 	%rd5, [%rd4+64];
	ld.global.u64 	%rd6, [%rd4+8];
	ld.global.u64 	%rd7, [%rd4+48];
	mul.lo.s64 	%rd8, %rd7, %rd57;
	ld.global.u64 	%rd65, [%rd4];
	mul.lo.s64 	%rd66, %rd8, %rd65;
	setp.le.u64 	%p1, %rd66, %rd3;
	@%p1 bra 	$L__BB12_23;

	ld.global.u64 	%rd9, [%rd4+16];
	and.b64  	%rd67, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd67, 0;
	@%p2 bra 	$L__BB12_3;

	div.u64 	%rd118, %rd3, %rd8;
	bra.uni 	$L__BB12_4;

$L__BB12_3:
	cvt.u32.u64 	%r15, %rd8;
	cvt.u32.u64 	%r16, %rd3;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd118, %r17;

$L__BB12_4:
	and.b64  	%rd68, %rd57, -4294967296;
	setp.eq.s64 	%p3, %rd68, 0;
	@%p3 bra 	$L__BB12_6;

	div.u64 	%rd119, %rd3, %rd57;
	mul.lo.s64 	%rd69, %rd119, %rd57;
	sub.s64 	%rd120, %rd3, %rd69;
	bra.uni 	$L__BB12_7;

$L__BB12_6:
	cvt.u32.u64 	%r18, %rd57;
	cvt.u32.u64 	%r19, %rd3;
	div.u32 	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, %r18;
	sub.s32 	%r22, %r19, %r21;
	cvt.u64.u32 	%rd119, %r20;
	cvt.u64.u32 	%rd120, %r22;

$L__BB12_7:
	and.b64  	%rd70, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB12_9;

	rem.u64 	%rd121, %rd119, %rd7;
	bra.uni 	$L__BB12_10;

$L__BB12_9:
	cvt.u32.u64 	%r23, %rd7;
	cvt.u32.u64 	%r24, %rd119;
	rem.u32 	%r25, %r24, %r23;
	cvt.u64.u32 	%rd121, %r25;

$L__BB12_10:
	ld.global.u64 	%rd71, [%rd4+24];
	mul.lo.s64 	%rd22, %rd71, %rd118;
	setp.eq.s64 	%p5, %rd5, 0;
	mov.u32 	%r55, 0;
	@%p5 bra 	$L__BB12_22;

	setp.eq.s64 	%p6, %rd6, 0;
	@%p6 bra 	$L__BB12_22;

	add.s64 	%rd23, %rd6, -1;
	and.b64  	%rd24, %rd6, 3;
	sub.s64 	%rd25, %rd6, %rd24;
	shl.b64 	%rd73, %rd22, 2;
	add.s64 	%rd26, %rd2, %rd73;
	mul.lo.s64 	%rd74, %rd120, %rd60;
	mul.lo.s64 	%rd75, %rd74, %rd58;
	shl.b64 	%rd76, %rd75, 2;
	shl.b64 	%rd77, %rd59, 2;
	sub.s64 	%rd27, %rd76, %rd77;
	shl.b64 	%rd28, %rd60, 2;
	add.s64 	%rd29, %rd9, %rd59;
	mul.lo.s64 	%rd30, %rd120, %rd58;
	mov.u32 	%r55, 0;
	mov.u64 	%rd122, 0;

$L__BB12_13:
	ld.param.u64 	%rd117, [conv1d_u32_param_4];
	add.s64 	%rd78, %rd122, %rd30;
	mul.lo.s64 	%rd32, %rd78, %rd117;
	setp.lt.u64 	%p7, %rd32, %rd59;
	setp.ge.u64 	%p8, %rd32, %rd29;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB12_21;

	setp.lt.u64 	%p10, %rd23, 3;
	ld.global.u64 	%rd33, [%rd4+32];
	mov.u64 	%rd127, 0;
	ld.global.u64 	%rd34, [%rd4+40];
	ld.global.u64 	%rd80, [%rd4+72];
	mul.lo.s64 	%rd35, %rd80, %rd121;
	ld.global.u64 	%rd36, [%rd4+80];
	ld.global.u64 	%rd37, [%rd4+88];
	@%p10 bra 	$L__BB12_17;

	mul.lo.s64 	%rd82, %rd28, %rd122;
	add.s64 	%rd83, %rd27, %rd82;
	mul.lo.s64 	%rd84, %rd34, %rd83;
	add.s64 	%rd125, %rd26, %rd84;
	shl.b64 	%rd85, %rd35, 2;
	add.s64 	%rd86, %rd1, %rd85;
	mul.lo.s64 	%rd87, %rd122, %rd37;
	shl.b64 	%rd88, %rd87, 2;
	add.s64 	%rd126, %rd86, %rd88;
	mov.u64 	%rd127, 0;
	mov.u64 	%rd124, %rd25;

$L__BB12_16:
	shl.b64 	%rd115, %rd36, 2;
	shl.b64 	%rd114, %rd33, 2;
	ld.global.u32 	%r30, [%rd126];
	ld.global.u32 	%r31, [%rd125];
	mad.lo.s32 	%r32, %r30, %r31, %r55;
	add.s64 	%rd89, %rd125, %rd114;
	add.s64 	%rd90, %rd126, %rd115;
	ld.global.u32 	%r33, [%rd90];
	ld.global.u32 	%r34, [%rd89];
	mad.lo.s32 	%r35, %r33, %r34, %r32;
	add.s64 	%rd91, %rd89, %rd114;
	add.s64 	%rd92, %rd90, %rd115;
	ld.global.u32 	%r36, [%rd92];
	ld.global.u32 	%r37, [%rd91];
	mad.lo.s32 	%r38, %r36, %r37, %r35;
	add.s64 	%rd93, %rd91, %rd114;
	add.s64 	%rd125, %rd93, %rd114;
	add.s64 	%rd94, %rd92, %rd115;
	add.s64 	%rd126, %rd94, %rd115;
	ld.global.u32 	%r39, [%rd94];
	ld.global.u32 	%r40, [%rd93];
	mad.lo.s32 	%r55, %r39, %r40, %r38;
	add.s64 	%rd127, %rd127, 4;
	add.s64 	%rd124, %rd124, -4;
	setp.ne.s64 	%p11, %rd124, 0;
	@%p11 bra 	$L__BB12_16;

$L__BB12_17:
	setp.eq.s64 	%p12, %rd24, 0;
	sub.s64 	%rd95, %rd32, %rd59;
	mul.lo.s64 	%rd51, %rd34, %rd95;
	@%p12 bra 	$L__BB12_21;

	setp.eq.s64 	%p13, %rd24, 1;
	mul.lo.s64 	%rd96, %rd33, %rd127;
	add.s64 	%rd97, %rd96, %rd22;
	add.s64 	%rd98, %rd97, %rd51;
	mul.lo.s64 	%rd99, %rd36, %rd127;
	add.s64 	%rd100, %rd99, %rd35;
	mul.lo.s64 	%rd101, %rd37, %rd122;
	add.s64 	%rd102, %rd100, %rd101;
	shl.b64 	%rd103, %rd98, 2;
	add.s64 	%rd52, %rd2, %rd103;
	shl.b64 	%rd104, %rd102, 2;
	add.s64 	%rd53, %rd1, %rd104;
	ld.global.u32 	%r41, [%rd53];
	ld.global.u32 	%r42, [%rd52];
	mad.lo.s32 	%r55, %r41, %r42, %r55;
	@%p13 bra 	$L__BB12_21;

	setp.eq.s64 	%p14, %rd24, 2;
	shl.b64 	%rd105, %rd36, 2;
	add.s64 	%rd54, %rd53, %rd105;
	ld.global.u32 	%r43, [%rd54];
	shl.b64 	%rd106, %rd33, 2;
	add.s64 	%rd55, %rd52, %rd106;
	ld.global.u32 	%r44, [%rd55];
	mad.lo.s32 	%r55, %r43, %r44, %r55;
	@%p14 bra 	$L__BB12_21;

	add.s64 	%rd108, %rd55, %rd106;
	add.s64 	%rd110, %rd54, %rd105;
	ld.global.u32 	%r45, [%rd110];
	ld.global.u32 	%r46, [%rd108];
	mad.lo.s32 	%r55, %r45, %r46, %r55;

$L__BB12_21:
	add.s64 	%rd122, %rd122, 1;
	setp.lt.u64 	%p15, %rd122, %rd5;
	@%p15 bra 	$L__BB12_13;

$L__BB12_22:
	mov.u32 	%r50, %tid.x;
	mov.u32 	%r49, %ntid.x;
	mov.u32 	%r48, %ctaid.x;
	mad.lo.s32 	%r47, %r48, %r49, %r50;
	cvt.u64.u32 	%rd116, %r47;
	cvta.to.global.u64 	%rd111, %rd61;
	shl.b64 	%rd112, %rd116, 2;
	add.s64 	%rd113, %rd111, %rd112;
	st.global.u32 	[%rd113], %r55;

$L__BB12_23:
	ret;

}
	// .globl	conv2d_f32
.visible .entry conv2d_f32(
	.param .u64 conv2d_f32_param_0,
	.param .u64 conv2d_f32_param_1,
	.param .u64 conv2d_f32_param_2,
	.param .u64 conv2d_f32_param_3,
	.param .u64 conv2d_f32_param_4,
	.param .u64 conv2d_f32_param_5,
	.param .u64 conv2d_f32_param_6,
	.param .u64 conv2d_f32_param_7,
	.param .u64 conv2d_f32_param_8,
	.param .u64 conv2d_f32_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<170>;


	ld.param.u64 	%rd75, [conv2d_f32_param_1];
	ld.param.u64 	%rd76, [conv2d_f32_param_2];
	ld.param.u64 	%rd77, [conv2d_f32_param_3];
	ld.param.u64 	%rd78, [conv2d_f32_param_4];
	ld.param.u64 	%rd79, [conv2d_f32_param_5];
	ld.param.u64 	%rd81, [conv2d_f32_param_6];
	ld.param.u64 	%rd82, [conv2d_f32_param_7];
	ld.param.u64 	%rd83, [conv2d_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd83;
	cvta.to.global.u64 	%rd2, %rd82;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd81;
	ld.global.u64 	%rd5, [%rd4+80];
	ld.global.u64 	%rd6, [%rd4+88];
	ld.global.u64 	%rd7, [%rd4+8];
	mul.lo.s64 	%rd8, %rd76, %rd75;
	ld.global.u64 	%rd9, [%rd4+64];
	mul.lo.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd84, [%rd4];
	mul.lo.s64 	%rd85, %rd10, %rd84;
	setp.le.u64 	%p1, %rd85, %rd3;
	@%p1 bra 	$L__BB13_33;

	ld.global.u64 	%rd11, [%rd4+16];
	ld.global.u64 	%rd12, [%rd4+24];
	and.b64  	%rd86, %rd10, -4294967296;
	setp.eq.s64 	%p2, %rd86, 0;
	@%p2 bra 	$L__BB13_3;

	div.u64 	%rd158, %rd3, %rd10;
	bra.uni 	$L__BB13_4;

$L__BB13_3:
	cvt.u32.u64 	%r5, %rd10;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd158, %r7;

$L__BB13_4:
	and.b64  	%rd87, %rd8, -4294967296;
	setp.eq.s64 	%p3, %rd87, 0;
	@%p3 bra 	$L__BB13_6;

	div.u64 	%rd159, %rd3, %rd8;
	bra.uni 	$L__BB13_7;

$L__BB13_6:
	cvt.u32.u64 	%r8, %rd8;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd159, %r10;

$L__BB13_7:
	and.b64  	%rd88, %rd9, -4294967296;
	setp.eq.s64 	%p4, %rd88, 0;
	@%p4 bra 	$L__BB13_9;

	rem.u64 	%rd160, %rd159, %rd9;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r11, %rd9;
	cvt.u32.u64 	%r12, %rd159;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB13_10:
	and.b64  	%rd89, %rd75, -4294967296;
	setp.eq.s64 	%p5, %rd89, 0;
	@%p5 bra 	$L__BB13_12;

	div.u64 	%rd161, %rd3, %rd75;
	mul.lo.s64 	%rd90, %rd161, %rd75;
	sub.s64 	%rd162, %rd3, %rd90;
	bra.uni 	$L__BB13_13;

$L__BB13_12:
	cvt.u32.u64 	%r14, %rd75;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd161, %r16;
	cvt.u64.u32 	%rd162, %r18;

$L__BB13_13:
	and.b64  	%rd91, %rd76, -4294967296;
	setp.eq.s64 	%p6, %rd91, 0;
	@%p6 bra 	$L__BB13_15;

	rem.u64 	%rd163, %rd161, %rd76;
	bra.uni 	$L__BB13_16;

$L__BB13_15:
	cvt.u32.u64 	%r19, %rd76;
	cvt.u32.u64 	%r20, %rd161;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd163, %r21;

$L__BB13_16:
	ld.global.u64 	%rd92, [%rd4+32];
	mul.lo.s64 	%rd31, %rd92, %rd158;
	setp.eq.s64 	%p7, %rd6, 0;
	mov.f32 	%f40, 0f00000000;
	@%p7 bra 	$L__BB13_32;

	setp.eq.s64 	%p8, %rd5, 0;
	@%p8 bra 	$L__BB13_32;

	add.s64 	%rd32, %rd7, -1;
	and.b64  	%rd33, %rd7, 3;
	sub.s64 	%rd34, %rd33, %rd7;
	shl.b64 	%rd94, %rd31, 2;
	add.s64 	%rd35, %rd2, %rd94;
	mul.lo.s64 	%rd42, %rd162, %rd77;
	shl.b64 	%rd95, %rd42, 2;
	shl.b64 	%rd96, %rd78, 2;
	sub.s64 	%rd36, %rd95, %rd96;
	shl.b64 	%rd37, %rd79, 2;
	mul.lo.s64 	%rd40, %rd163, %rd77;
	shl.b64 	%rd97, %rd40, 2;
	sub.s64 	%rd38, %rd97, %rd96;
	add.s64 	%rd39, %rd11, %rd78;
	add.s64 	%rd41, %rd12, %rd78;
	mov.f32 	%f40, 0f00000000;
	mov.u64 	%rd164, 0;

$L__BB13_19:
	mul.lo.s64 	%rd98, %rd164, %rd79;
	add.s64 	%rd44, %rd98, %rd42;
	setp.lt.u64 	%p9, %rd44, %rd78;
	setp.ge.u64 	%p10, %rd44, %rd41;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB13_31;

	setp.eq.s64 	%p12, %rd7, 0;
	@%p12 bra 	$L__BB13_31;

	mov.u64 	%rd165, 0;

$L__BB13_22:
	mul.lo.s64 	%rd101, %rd165, %rd79;
	add.s64 	%rd49, %rd101, %rd40;
	setp.lt.u64 	%p13, %rd49, %rd78;
	setp.ge.u64 	%p14, %rd49, %rd39;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB13_30;

	setp.lt.u64 	%p16, %rd32, 3;
	ld.global.u64 	%rd50, [%rd4+40];
	mov.u64 	%rd169, 0;
	ld.global.u64 	%rd51, [%rd4+48];
	ld.global.u64 	%rd52, [%rd4+56];
	ld.global.u64 	%rd103, [%rd4+96];
	mul.lo.s64 	%rd53, %rd103, %rd160;
	ld.global.u64 	%rd54, [%rd4+104];
	ld.global.u64 	%rd55, [%rd4+112];
	ld.global.u64 	%rd56, [%rd4+120];
	@%p16 bra 	$L__BB13_26;

	mul.lo.s64 	%rd157, %rd37, %rd164;
	add.s64 	%rd156, %rd36, %rd157;
	shl.b64 	%rd150, %rd164, 2;
	mul.lo.s64 	%rd105, %rd156, %rd52;
	mul.lo.s64 	%rd106, %rd37, %rd165;
	add.s64 	%rd107, %rd38, %rd106;
	mul.lo.s64 	%rd108, %rd51, %rd107;
	add.s64 	%rd109, %rd105, %rd108;
	add.s64 	%rd167, %rd35, %rd109;
	shl.b64 	%rd110, %rd53, 2;
	add.s64 	%rd111, %rd1, %rd110;
	mul.lo.s64 	%rd112, %rd165, %rd55;
	shl.b64 	%rd113, %rd112, 2;
	mul.lo.s64 	%rd114, %rd150, %rd56;
	add.s64 	%rd115, %rd114, %rd113;
	add.s64 	%rd168, %rd111, %rd115;
	mov.u64 	%rd169, 0;

$L__BB13_25:
	shl.b64 	%rd147, %rd54, 2;
	shl.b64 	%rd146, %rd50, 2;
	ld.global.f32 	%f17, [%rd168];
	ld.global.f32 	%f18, [%rd167];
	fma.rn.f32 	%f19, %f18, %f17, %f40;
	add.s64 	%rd116, %rd167, %rd146;
	add.s64 	%rd117, %rd168, %rd147;
	ld.global.f32 	%f20, [%rd117];
	ld.global.f32 	%f21, [%rd116];
	fma.rn.f32 	%f22, %f21, %f20, %f19;
	add.s64 	%rd118, %rd116, %rd146;
	add.s64 	%rd119, %rd117, %rd147;
	ld.global.f32 	%f23, [%rd119];
	ld.global.f32 	%f24, [%rd118];
	fma.rn.f32 	%f25, %f24, %f23, %f22;
	add.s64 	%rd120, %rd118, %rd146;
	add.s64 	%rd167, %rd120, %rd146;
	add.s64 	%rd121, %rd119, %rd147;
	add.s64 	%rd168, %rd121, %rd147;
	ld.global.f32 	%f26, [%rd121];
	ld.global.f32 	%f27, [%rd120];
	fma.rn.f32 	%f40, %f27, %f26, %f25;
	add.s64 	%rd169, %rd169, 4;
	add.s64 	%rd122, %rd34, %rd169;
	setp.ne.s64 	%p17, %rd122, 0;
	@%p17 bra 	$L__BB13_25;

$L__BB13_26:
	mul.lo.s64 	%rd152, %rd165, %rd79;
	add.s64 	%rd151, %rd152, %rd40;
	setp.eq.s64 	%p18, %rd33, 0;
	sub.s64 	%rd123, %rd151, %rd78;
	mul.lo.s64 	%rd68, %rd51, %rd123;
	@%p18 bra 	$L__BB13_30;

	mul.lo.s64 	%rd155, %rd164, %rd79;
	add.s64 	%rd154, %rd155, %rd42;
	sub.s64 	%rd153, %rd154, %rd78;
	setp.eq.s64 	%p19, %rd33, 1;
	mul.lo.s64 	%rd124, %rd50, %rd169;
	add.s64 	%rd125, %rd124, %rd31;
	add.s64 	%rd126, %rd125, %rd68;
	mul.lo.s64 	%rd127, %rd52, %rd153;
	add.s64 	%rd128, %rd126, %rd127;
	mul.lo.s64 	%rd129, %rd54, %rd169;
	add.s64 	%rd130, %rd129, %rd53;
	mul.lo.s64 	%rd131, %rd55, %rd165;
	add.s64 	%rd132, %rd130, %rd131;
	mul.lo.s64 	%rd133, %rd56, %rd164;
	add.s64 	%rd134, %rd132, %rd133;
	shl.b64 	%rd135, %rd128, 2;
	add.s64 	%rd69, %rd2, %rd135;
	shl.b64 	%rd136, %rd134, 2;
	add.s64 	%rd70, %rd1, %rd136;
	ld.global.f32 	%f28, [%rd70];
	ld.global.f32 	%f29, [%rd69];
	fma.rn.f32 	%f40, %f29, %f28, %f40;
	@%p19 bra 	$L__BB13_30;

	setp.eq.s64 	%p20, %rd33, 2;
	shl.b64 	%rd137, %rd54, 2;
	add.s64 	%rd71, %rd70, %rd137;
	ld.global.f32 	%f30, [%rd71];
	shl.b64 	%rd138, %rd50, 2;
	add.s64 	%rd72, %rd69, %rd138;
	ld.global.f32 	%f31, [%rd72];
	fma.rn.f32 	%f40, %f31, %f30, %f40;
	@%p20 bra 	$L__BB13_30;

	add.s64 	%rd140, %rd72, %rd138;
	add.s64 	%rd142, %rd71, %rd137;
	ld.global.f32 	%f32, [%rd142];
	ld.global.f32 	%f33, [%rd140];
	fma.rn.f32 	%f40, %f33, %f32, %f40;

$L__BB13_30:
	add.s64 	%rd165, %rd165, 1;
	setp.lt.u64 	%p21, %rd165, %rd5;
	@%p21 bra 	$L__BB13_22;

$L__BB13_31:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p22, %rd164, %rd6;
	@%p22 bra 	$L__BB13_19;

$L__BB13_32:
	ld.param.u64 	%rd149, [conv2d_f32_param_9];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r22, %r23, %r24, %r25;
	cvt.u64.u32 	%rd148, %r22;
	cvta.to.global.u64 	%rd143, %rd149;
	shl.b64 	%rd144, %rd148, 2;
	add.s64 	%rd145, %rd143, %rd144;
	st.global.f32 	[%rd145], %f40;

$L__BB13_33:
	ret;

}
	// .globl	conv2d_f64
.visible .entry conv2d_f64(
	.param .u64 conv2d_f64_param_0,
	.param .u64 conv2d_f64_param_1,
	.param .u64 conv2d_f64_param_2,
	.param .u64 conv2d_f64_param_3,
	.param .u64 conv2d_f64_param_4,
	.param .u64 conv2d_f64_param_5,
	.param .u64 conv2d_f64_param_6,
	.param .u64 conv2d_f64_param_7,
	.param .u64 conv2d_f64_param_8,
	.param .u64 conv2d_f64_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<26>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<170>;


	ld.param.u64 	%rd75, [conv2d_f64_param_1];
	ld.param.u64 	%rd76, [conv2d_f64_param_2];
	ld.param.u64 	%rd77, [conv2d_f64_param_3];
	ld.param.u64 	%rd78, [conv2d_f64_param_4];
	ld.param.u64 	%rd79, [conv2d_f64_param_5];
	ld.param.u64 	%rd81, [conv2d_f64_param_6];
	ld.param.u64 	%rd82, [conv2d_f64_param_7];
	ld.param.u64 	%rd83, [conv2d_f64_param_8];
	cvta.to.global.u64 	%rd1, %rd83;
	cvta.to.global.u64 	%rd2, %rd82;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd81;
	ld.global.u64 	%rd5, [%rd4+80];
	ld.global.u64 	%rd6, [%rd4+88];
	ld.global.u64 	%rd7, [%rd4+8];
	mul.lo.s64 	%rd8, %rd76, %rd75;
	ld.global.u64 	%rd9, [%rd4+64];
	mul.lo.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd84, [%rd4];
	mul.lo.s64 	%rd85, %rd10, %rd84;
	setp.le.u64 	%p1, %rd85, %rd3;
	@%p1 bra 	$L__BB14_33;

	ld.global.u64 	%rd11, [%rd4+16];
	ld.global.u64 	%rd12, [%rd4+24];
	and.b64  	%rd86, %rd10, -4294967296;
	setp.eq.s64 	%p2, %rd86, 0;
	@%p2 bra 	$L__BB14_3;

	div.u64 	%rd158, %rd3, %rd10;
	bra.uni 	$L__BB14_4;

$L__BB14_3:
	cvt.u32.u64 	%r5, %rd10;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd158, %r7;

$L__BB14_4:
	and.b64  	%rd87, %rd8, -4294967296;
	setp.eq.s64 	%p3, %rd87, 0;
	@%p3 bra 	$L__BB14_6;

	div.u64 	%rd159, %rd3, %rd8;
	bra.uni 	$L__BB14_7;

$L__BB14_6:
	cvt.u32.u64 	%r8, %rd8;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd159, %r10;

$L__BB14_7:
	and.b64  	%rd88, %rd9, -4294967296;
	setp.eq.s64 	%p4, %rd88, 0;
	@%p4 bra 	$L__BB14_9;

	rem.u64 	%rd160, %rd159, %rd9;
	bra.uni 	$L__BB14_10;

$L__BB14_9:
	cvt.u32.u64 	%r11, %rd9;
	cvt.u32.u64 	%r12, %rd159;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB14_10:
	and.b64  	%rd89, %rd75, -4294967296;
	setp.eq.s64 	%p5, %rd89, 0;
	@%p5 bra 	$L__BB14_12;

	div.u64 	%rd161, %rd3, %rd75;
	mul.lo.s64 	%rd90, %rd161, %rd75;
	sub.s64 	%rd162, %rd3, %rd90;
	bra.uni 	$L__BB14_13;

$L__BB14_12:
	cvt.u32.u64 	%r14, %rd75;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd161, %r16;
	cvt.u64.u32 	%rd162, %r18;

$L__BB14_13:
	and.b64  	%rd91, %rd76, -4294967296;
	setp.eq.s64 	%p6, %rd91, 0;
	@%p6 bra 	$L__BB14_15;

	rem.u64 	%rd163, %rd161, %rd76;
	bra.uni 	$L__BB14_16;

$L__BB14_15:
	cvt.u32.u64 	%r19, %rd76;
	cvt.u32.u64 	%r20, %rd161;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd163, %r21;

$L__BB14_16:
	ld.global.u64 	%rd92, [%rd4+32];
	mul.lo.s64 	%rd31, %rd92, %rd158;
	setp.eq.s64 	%p7, %rd6, 0;
	mov.f64 	%fd40, 0d0000000000000000;
	@%p7 bra 	$L__BB14_32;

	setp.eq.s64 	%p8, %rd5, 0;
	@%p8 bra 	$L__BB14_32;

	add.s64 	%rd32, %rd7, -1;
	and.b64  	%rd33, %rd7, 3;
	sub.s64 	%rd34, %rd33, %rd7;
	shl.b64 	%rd94, %rd31, 3;
	add.s64 	%rd35, %rd2, %rd94;
	mul.lo.s64 	%rd42, %rd162, %rd77;
	shl.b64 	%rd95, %rd42, 3;
	shl.b64 	%rd96, %rd78, 3;
	sub.s64 	%rd36, %rd95, %rd96;
	shl.b64 	%rd37, %rd79, 3;
	mul.lo.s64 	%rd40, %rd163, %rd77;
	shl.b64 	%rd97, %rd40, 3;
	sub.s64 	%rd38, %rd97, %rd96;
	add.s64 	%rd39, %rd11, %rd78;
	add.s64 	%rd41, %rd12, %rd78;
	mov.f64 	%fd40, 0d0000000000000000;
	mov.u64 	%rd164, 0;

$L__BB14_19:
	mul.lo.s64 	%rd98, %rd164, %rd79;
	add.s64 	%rd44, %rd98, %rd42;
	setp.lt.u64 	%p9, %rd44, %rd78;
	setp.ge.u64 	%p10, %rd44, %rd41;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB14_31;

	setp.eq.s64 	%p12, %rd7, 0;
	@%p12 bra 	$L__BB14_31;

	mov.u64 	%rd165, 0;

$L__BB14_22:
	mul.lo.s64 	%rd101, %rd165, %rd79;
	add.s64 	%rd49, %rd101, %rd40;
	setp.lt.u64 	%p13, %rd49, %rd78;
	setp.ge.u64 	%p14, %rd49, %rd39;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB14_30;

	setp.lt.u64 	%p16, %rd32, 3;
	ld.global.u64 	%rd50, [%rd4+40];
	mov.u64 	%rd169, 0;
	ld.global.u64 	%rd51, [%rd4+48];
	ld.global.u64 	%rd52, [%rd4+56];
	ld.global.u64 	%rd103, [%rd4+96];
	mul.lo.s64 	%rd53, %rd103, %rd160;
	ld.global.u64 	%rd54, [%rd4+104];
	ld.global.u64 	%rd55, [%rd4+112];
	ld.global.u64 	%rd56, [%rd4+120];
	@%p16 bra 	$L__BB14_26;

	mul.lo.s64 	%rd157, %rd37, %rd164;
	add.s64 	%rd156, %rd36, %rd157;
	shl.b64 	%rd150, %rd164, 3;
	mul.lo.s64 	%rd105, %rd156, %rd52;
	mul.lo.s64 	%rd106, %rd37, %rd165;
	add.s64 	%rd107, %rd38, %rd106;
	mul.lo.s64 	%rd108, %rd51, %rd107;
	add.s64 	%rd109, %rd105, %rd108;
	add.s64 	%rd167, %rd35, %rd109;
	shl.b64 	%rd110, %rd53, 3;
	add.s64 	%rd111, %rd1, %rd110;
	mul.lo.s64 	%rd112, %rd165, %rd55;
	shl.b64 	%rd113, %rd112, 3;
	mul.lo.s64 	%rd114, %rd150, %rd56;
	add.s64 	%rd115, %rd114, %rd113;
	add.s64 	%rd168, %rd111, %rd115;
	mov.u64 	%rd169, 0;

$L__BB14_25:
	shl.b64 	%rd147, %rd54, 3;
	shl.b64 	%rd146, %rd50, 3;
	ld.global.f64 	%fd17, [%rd168];
	ld.global.f64 	%fd18, [%rd167];
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd40;
	add.s64 	%rd116, %rd167, %rd146;
	add.s64 	%rd117, %rd168, %rd147;
	ld.global.f64 	%fd20, [%rd117];
	ld.global.f64 	%fd21, [%rd116];
	fma.rn.f64 	%fd22, %fd21, %fd20, %fd19;
	add.s64 	%rd118, %rd116, %rd146;
	add.s64 	%rd119, %rd117, %rd147;
	ld.global.f64 	%fd23, [%rd119];
	ld.global.f64 	%fd24, [%rd118];
	fma.rn.f64 	%fd25, %fd24, %fd23, %fd22;
	add.s64 	%rd120, %rd118, %rd146;
	add.s64 	%rd167, %rd120, %rd146;
	add.s64 	%rd121, %rd119, %rd147;
	add.s64 	%rd168, %rd121, %rd147;
	ld.global.f64 	%fd26, [%rd121];
	ld.global.f64 	%fd27, [%rd120];
	fma.rn.f64 	%fd40, %fd27, %fd26, %fd25;
	add.s64 	%rd169, %rd169, 4;
	add.s64 	%rd122, %rd34, %rd169;
	setp.ne.s64 	%p17, %rd122, 0;
	@%p17 bra 	$L__BB14_25;

$L__BB14_26:
	mul.lo.s64 	%rd152, %rd165, %rd79;
	add.s64 	%rd151, %rd152, %rd40;
	setp.eq.s64 	%p18, %rd33, 0;
	sub.s64 	%rd123, %rd151, %rd78;
	mul.lo.s64 	%rd68, %rd51, %rd123;
	@%p18 bra 	$L__BB14_30;

	mul.lo.s64 	%rd155, %rd164, %rd79;
	add.s64 	%rd154, %rd155, %rd42;
	sub.s64 	%rd153, %rd154, %rd78;
	setp.eq.s64 	%p19, %rd33, 1;
	mul.lo.s64 	%rd124, %rd50, %rd169;
	add.s64 	%rd125, %rd124, %rd31;
	add.s64 	%rd126, %rd125, %rd68;
	mul.lo.s64 	%rd127, %rd52, %rd153;
	add.s64 	%rd128, %rd126, %rd127;
	mul.lo.s64 	%rd129, %rd54, %rd169;
	add.s64 	%rd130, %rd129, %rd53;
	mul.lo.s64 	%rd131, %rd55, %rd165;
	add.s64 	%rd132, %rd130, %rd131;
	mul.lo.s64 	%rd133, %rd56, %rd164;
	add.s64 	%rd134, %rd132, %rd133;
	shl.b64 	%rd135, %rd128, 3;
	add.s64 	%rd69, %rd2, %rd135;
	shl.b64 	%rd136, %rd134, 3;
	add.s64 	%rd70, %rd1, %rd136;
	ld.global.f64 	%fd28, [%rd70];
	ld.global.f64 	%fd29, [%rd69];
	fma.rn.f64 	%fd40, %fd29, %fd28, %fd40;
	@%p19 bra 	$L__BB14_30;

	setp.eq.s64 	%p20, %rd33, 2;
	shl.b64 	%rd137, %rd54, 3;
	add.s64 	%rd71, %rd70, %rd137;
	ld.global.f64 	%fd30, [%rd71];
	shl.b64 	%rd138, %rd50, 3;
	add.s64 	%rd72, %rd69, %rd138;
	ld.global.f64 	%fd31, [%rd72];
	fma.rn.f64 	%fd40, %fd31, %fd30, %fd40;
	@%p20 bra 	$L__BB14_30;

	add.s64 	%rd140, %rd72, %rd138;
	add.s64 	%rd142, %rd71, %rd137;
	ld.global.f64 	%fd32, [%rd142];
	ld.global.f64 	%fd33, [%rd140];
	fma.rn.f64 	%fd40, %fd33, %fd32, %fd40;

$L__BB14_30:
	add.s64 	%rd165, %rd165, 1;
	setp.lt.u64 	%p21, %rd165, %rd5;
	@%p21 bra 	$L__BB14_22;

$L__BB14_31:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p22, %rd164, %rd6;
	@%p22 bra 	$L__BB14_19;

$L__BB14_32:
	ld.param.u64 	%rd149, [conv2d_f64_param_9];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r22, %r23, %r24, %r25;
	cvt.u64.u32 	%rd148, %r22;
	cvta.to.global.u64 	%rd143, %rd149;
	shl.b64 	%rd144, %rd148, 3;
	add.s64 	%rd145, %rd143, %rd144;
	st.global.f64 	[%rd145], %fd40;

$L__BB14_33:
	ret;

}
	// .globl	conv2d_u8
.visible .entry conv2d_u8(
	.param .u64 conv2d_u8_param_0,
	.param .u64 conv2d_u8_param_1,
	.param .u64 conv2d_u8_param_2,
	.param .u64 conv2d_u8_param_3,
	.param .u64 conv2d_u8_param_4,
	.param .u64 conv2d_u8_param_5,
	.param .u64 conv2d_u8_param_6,
	.param .u64 conv2d_u8_param_7,
	.param .u64 conv2d_u8_param_8,
	.param .u64 conv2d_u8_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<136>;


	ld.param.u64 	%rd67, [conv2d_u8_param_1];
	ld.param.u64 	%rd68, [conv2d_u8_param_2];
	ld.param.u64 	%rd69, [conv2d_u8_param_3];
	ld.param.u64 	%rd70, [conv2d_u8_param_4];
	ld.param.u64 	%rd71, [conv2d_u8_param_5];
	ld.param.u64 	%rd73, [conv2d_u8_param_6];
	ld.param.u64 	%rd74, [conv2d_u8_param_7];
	ld.param.u64 	%rd75, [conv2d_u8_param_8];
	cvta.to.global.u64 	%rd1, %rd75;
	cvta.to.global.u64 	%rd2, %rd74;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd3, %r4;
	cvta.to.global.u64 	%rd4, %rd73;
	ld.global.u64 	%rd5, [%rd4+80];
	ld.global.u64 	%rd6, [%rd4+88];
	ld.global.u64 	%rd7, [%rd4+8];
	mul.lo.s64 	%rd8, %rd68, %rd67;
	ld.global.u64 	%rd9, [%rd4+64];
	mul.lo.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd76, [%rd4];
	mul.lo.s64 	%rd77, %rd10, %rd76;
	setp.le.u64 	%p1, %rd77, %rd3;
	@%p1 bra 	$L__BB15_33;

	ld.global.u64 	%rd11, [%rd4+16];
	ld.global.u64 	%rd12, [%rd4+24];
	and.b64  	%rd78, %rd10, -4294967296;
	setp.eq.s64 	%p2, %rd78, 0;
	@%p2 bra 	$L__BB15_3;

	div.u64 	%rd124, %rd3, %rd10;
	bra.uni 	$L__BB15_4;

$L__BB15_3:
	cvt.u32.u64 	%r5, %rd10;
	cvt.u32.u64 	%r6, %rd3;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd124, %r7;

$L__BB15_4:
	and.b64  	%rd79, %rd8, -4294967296;
	setp.eq.s64 	%p3, %rd79, 0;
	@%p3 bra 	$L__BB15_6;

	div.u64 	%rd125, %rd3, %rd8;
	bra.uni 	$L__BB15_7;

$L__BB15_6:
	cvt.u32.u64 	%r8, %rd8;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd125, %r10;

$L__BB15_7:
	and.b64  	%rd80, %rd9, -4294967296;
	setp.eq.s64 	%p4, %rd80, 0;
	@%p4 bra 	$L__BB15_9;

	rem.u64 	%rd126, %rd125, %rd9;
	bra.uni 	$L__BB15_10;

$L__BB15_9:
	cvt.u32.u64 	%r11, %rd9;
	cvt.u32.u64 	%r12, %rd125;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd126, %r13;

$L__BB15_10:
	and.b64  	%rd81, %rd67, -4294967296;
	setp.eq.s64 	%p5, %rd81, 0;
	@%p5 bra 	$L__BB15_12;

	div.u64 	%rd127, %rd3, %rd67;
	mul.lo.s64 	%rd82, %rd127, %rd67;
	sub.s64 	%rd128, %rd3, %rd82;
	bra.uni 	$L__BB15_13;

$L__BB15_12:
	cvt.u32.u64 	%r14, %rd67;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd127, %r16;
	cvt.u64.u32 	%rd128, %r18;

$L__BB15_13:
	and.b64  	%rd83, %rd68, -4294967296;
	setp.eq.s64 	%p6, %rd83, 0;
	@%p6 bra 	$L__BB15_15;

	rem.u64 	%rd129, %rd127, %rd68;
	bra.uni 	$L__BB15_16;

$L__BB15_15:
	cvt.u32.u64 	%r19, %rd68;
	cvt.u32.u64 	%r20, %rd127;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd129, %r21;

$L__BB15_16:
	ld.global.u64 	%rd84, [%rd4+32];
	mul.lo.s64 	%rd31, %rd84, %rd124;
	setp.eq.s64 	%p7, %rd6, 0;
	mov.u16 	%rs46, 0;
	@%p7 bra 	$L__BB15_32;

	setp.eq.s64 	%p8, %rd5, 0;
	@%p8 bra 	$L__BB15_32;

	add.s64 	%rd32, %rd7, -1;
	and.b64  	%rd33, %rd7, 3;
	sub.s64 	%rd34, %rd33, %rd7;
	add.s64 	%rd35, %rd2, %rd31;
	add.s64 	%rd36, %rd11, %rd70;
	mul.lo.s64 	%rd37, %rd129, %rd69;
	add.s64 	%rd38, %rd12, %rd70;
	mul.lo.s64 	%rd39, %rd128, %rd69;
	mov.u16 	%rs46, 0;
	mov.u64 	%rd130, 0;

$L__BB15_19:
	mul.lo.s64 	%rd86, %rd130, %rd71;
	add.s64 	%rd41, %rd86, %rd39;
	setp.lt.u64 	%p9, %rd41, %rd70;
	setp.ge.u64 	%p10, %rd41, %rd38;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB15_31;

	setp.eq.s64 	%p12, %rd7, 0;
	@%p12 bra 	$L__BB15_31;

	mov.u64 	%rd131, 0;

$L__BB15_22:
	mul.lo.s64 	%rd88, %rd131, %rd71;
	add.s64 	%rd44, %rd88, %rd37;
	setp.lt.u64 	%p13, %rd44, %rd70;
	setp.ge.u64 	%p14, %rd44, %rd36;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB15_30;

	mul.lo.s64 	%rd123, %rd130, %rd71;
	add.s64 	%rd122, %rd123, %rd39;
	sub.s64 	%rd121, %rd122, %rd70;
	setp.lt.u64 	%p16, %rd32, 3;
	ld.global.u64 	%rd45, [%rd4+40];
	mov.u64 	%rd135, 0;
	ld.global.u64 	%rd90, [%rd4+48];
	sub.s64 	%rd91, %rd44, %rd70;
	mul.lo.s64 	%rd46, %rd90, %rd91;
	ld.global.u64 	%rd92, [%rd4+56];
	mul.lo.s64 	%rd47, %rd92, %rd121;
	ld.global.u64 	%rd93, [%rd4+96];
	mul.lo.s64 	%rd48, %rd93, %rd126;
	ld.global.u64 	%rd49, [%rd4+104];
	ld.global.u64 	%rd94, [%rd4+112];
	mul.lo.s64 	%rd50, %rd94, %rd131;
	ld.global.u64 	%rd95, [%rd4+120];
	mul.lo.s64 	%rd51, %rd95, %rd130;
	@%p16 bra 	$L__BB15_26;

	add.s64 	%rd97, %rd47, %rd46;
	add.s64 	%rd133, %rd35, %rd97;
	add.s64 	%rd98, %rd48, %rd51;
	add.s64 	%rd99, %rd98, %rd50;
	add.s64 	%rd134, %rd1, %rd99;
	mov.u64 	%rd135, 0;

$L__BB15_25:
	ld.global.u8 	%rs17, [%rd134];
	ld.global.u8 	%rs18, [%rd133];
	mul.lo.s16 	%rs19, %rs17, %rs18;
	add.s16 	%rs20, %rs19, %rs46;
	add.s64 	%rd100, %rd134, %rd49;
	ld.global.u8 	%rs21, [%rd100];
	add.s64 	%rd101, %rd133, %rd45;
	ld.global.u8 	%rs22, [%rd101];
	mul.lo.s16 	%rs23, %rs21, %rs22;
	add.s16 	%rs24, %rs23, %rs20;
	add.s64 	%rd102, %rd101, %rd45;
	add.s64 	%rd103, %rd100, %rd49;
	ld.global.u8 	%rs25, [%rd103];
	ld.global.u8 	%rs26, [%rd102];
	mul.lo.s16 	%rs27, %rs25, %rs26;
	add.s16 	%rs28, %rs27, %rs24;
	add.s64 	%rd104, %rd102, %rd45;
	add.s64 	%rd133, %rd104, %rd45;
	add.s64 	%rd105, %rd103, %rd49;
	add.s64 	%rd134, %rd105, %rd49;
	ld.global.u8 	%rs29, [%rd105];
	ld.global.u8 	%rs30, [%rd104];
	mul.lo.s16 	%rs31, %rs29, %rs30;
	add.s16 	%rs46, %rs31, %rs28;
	add.s64 	%rd135, %rd135, 4;
	add.s64 	%rd106, %rd34, %rd135;
	setp.ne.s64 	%p17, %rd106, 0;
	@%p17 bra 	$L__BB15_25;

$L__BB15_26:
	setp.eq.s64 	%p18, %rd33, 0;
	@%p18 bra 	$L__BB15_30;

	setp.eq.s64 	%p19, %rd33, 1;
	mul.lo.s64 	%rd107, %rd45, %rd135;
	add.s64 	%rd108, %rd107, %rd31;
	add.s64 	%rd109, %rd108, %rd46;
	add.s64 	%rd110, %rd109, %rd47;
	mul.lo.s64 	%rd111, %rd49, %rd135;
	add.s64 	%rd112, %rd111, %rd48;
	add.s64 	%rd113, %rd112, %rd50;
	add.s64 	%rd114, %rd113, %rd51;
	add.s64 	%rd61, %rd2, %rd110;
	add.s64 	%rd62, %rd1, %rd114;
	ld.global.u8 	%rs32, [%rd62];
	ld.global.u8 	%rs33, [%rd61];
	mul.lo.s16 	%rs34, %rs32, %rs33;
	add.s16 	%rs46, %rs34, %rs46;
	@%p19 bra 	$L__BB15_30;

	setp.eq.s64 	%p20, %rd33, 2;
	add.s64 	%rd63, %rd62, %rd49;
	ld.global.u8 	%rs35, [%rd63];
	add.s64 	%rd64, %rd61, %rd45;
	ld.global.u8 	%rs36, [%rd64];
	mul.lo.s16 	%rs37, %rs35, %rs36;
	add.s16 	%rs46, %rs37, %rs46;
	@%p20 bra 	$L__BB15_30;

	add.s64 	%rd115, %rd64, %rd45;
	add.s64 	%rd116, %rd63, %rd49;
	ld.global.u8 	%rs38, [%rd116];
	ld.global.u8 	%rs39, [%rd115];
	mul.lo.s16 	%rs40, %rs38, %rs39;
	add.s16 	%rs46, %rs40, %rs46;

$L__BB15_30:
	add.s64 	%rd131, %rd131, 1;
	setp.lt.u64 	%p21, %rd131, %rd5;
	@%p21 bra 	$L__BB15_22;

$L__BB15_31:
	add.s64 	%rd130, %rd130, 1;
	setp.lt.u64 	%p22, %rd130, %rd6;
	@%p22 bra 	$L__BB15_19;

$L__BB15_32:
	ld.param.u64 	%rd120, [conv2d_u8_param_9];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r22, %r23, %r24, %r25;
	cvt.u64.u32 	%rd119, %r22;
	cvta.to.global.u64 	%rd117, %rd120;
	add.s64 	%rd118, %rd117, %rd119;
	st.global.u8 	[%rd118], %rs46;

$L__BB15_33:
	ret;

}
	// .globl	conv2d_u32
.visible .entry conv2d_u32(
	.param .u64 conv2d_u32_param_0,
	.param .u64 conv2d_u32_param_1,
	.param .u64 conv2d_u32_param_2,
	.param .u64 conv2d_u32_param_3,
	.param .u64 conv2d_u32_param_4,
	.param .u64 conv2d_u32_param_5,
	.param .u64 conv2d_u32_param_6,
	.param .u64 conv2d_u32_param_7,
	.param .u64 conv2d_u32_param_8,
	.param .u64 conv2d_u32_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<67>;
	.reg .b64 	%rd<170>;


	ld.param.u64 	%rd75, [conv2d_u32_param_1];
	ld.param.u64 	%rd76, [conv2d_u32_param_2];
	ld.param.u64 	%rd77, [conv2d_u32_param_3];
	ld.param.u64 	%rd78, [conv2d_u32_param_4];
	ld.param.u64 	%rd79, [conv2d_u32_param_5];
	ld.param.u64 	%rd81, [conv2d_u32_param_6];
	ld.param.u64 	%rd82, [conv2d_u32_param_7];
	ld.param.u64 	%rd83, [conv2d_u32_param_8];
	cvta.to.global.u64 	%rd1, %rd83;
	cvta.to.global.u64 	%rd2, %rd82;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r16, %r14, %r13, %r15;
	cvt.u64.u32 	%rd3, %r16;
	cvta.to.global.u64 	%rd4, %rd81;
	ld.global.u64 	%rd5, [%rd4+80];
	ld.global.u64 	%rd6, [%rd4+88];
	ld.global.u64 	%rd7, [%rd4+8];
	mul.lo.s64 	%rd8, %rd76, %rd75;
	ld.global.u64 	%rd9, [%rd4+64];
	mul.lo.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd84, [%rd4];
	mul.lo.s64 	%rd85, %rd10, %rd84;
	setp.le.u64 	%p1, %rd85, %rd3;
	@%p1 bra 	$L__BB16_33;

	ld.global.u64 	%rd11, [%rd4+16];
	ld.global.u64 	%rd12, [%rd4+24];
	and.b64  	%rd86, %rd10, -4294967296;
	setp.eq.s64 	%p2, %rd86, 0;
	@%p2 bra 	$L__BB16_3;

	div.u64 	%rd158, %rd3, %rd10;
	bra.uni 	$L__BB16_4;

$L__BB16_3:
	cvt.u32.u64 	%r17, %rd10;
	cvt.u32.u64 	%r18, %rd3;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd158, %r19;

$L__BB16_4:
	and.b64  	%rd87, %rd8, -4294967296;
	setp.eq.s64 	%p3, %rd87, 0;
	@%p3 bra 	$L__BB16_6;

	div.u64 	%rd159, %rd3, %rd8;
	bra.uni 	$L__BB16_7;

$L__BB16_6:
	cvt.u32.u64 	%r20, %rd8;
	cvt.u32.u64 	%r21, %rd3;
	div.u32 	%r22, %r21, %r20;
	cvt.u64.u32 	%rd159, %r22;

$L__BB16_7:
	and.b64  	%rd88, %rd9, -4294967296;
	setp.eq.s64 	%p4, %rd88, 0;
	@%p4 bra 	$L__BB16_9;

	rem.u64 	%rd160, %rd159, %rd9;
	bra.uni 	$L__BB16_10;

$L__BB16_9:
	cvt.u32.u64 	%r23, %rd9;
	cvt.u32.u64 	%r24, %rd159;
	rem.u32 	%r25, %r24, %r23;
	cvt.u64.u32 	%rd160, %r25;

$L__BB16_10:
	and.b64  	%rd89, %rd75, -4294967296;
	setp.eq.s64 	%p5, %rd89, 0;
	@%p5 bra 	$L__BB16_12;

	div.u64 	%rd161, %rd3, %rd75;
	mul.lo.s64 	%rd90, %rd161, %rd75;
	sub.s64 	%rd162, %rd3, %rd90;
	bra.uni 	$L__BB16_13;

$L__BB16_12:
	cvt.u32.u64 	%r26, %rd75;
	cvt.u32.u64 	%r27, %rd3;
	div.u32 	%r28, %r27, %r26;
	mul.lo.s32 	%r29, %r28, %r26;
	sub.s32 	%r30, %r27, %r29;
	cvt.u64.u32 	%rd161, %r28;
	cvt.u64.u32 	%rd162, %r30;

$L__BB16_13:
	and.b64  	%rd91, %rd76, -4294967296;
	setp.eq.s64 	%p6, %rd91, 0;
	@%p6 bra 	$L__BB16_15;

	rem.u64 	%rd163, %rd161, %rd76;
	bra.uni 	$L__BB16_16;

$L__BB16_15:
	cvt.u32.u64 	%r31, %rd76;
	cvt.u32.u64 	%r32, %rd161;
	rem.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;

$L__BB16_16:
	ld.global.u64 	%rd92, [%rd4+32];
	mul.lo.s64 	%rd31, %rd92, %rd158;
	setp.eq.s64 	%p7, %rd6, 0;
	mov.u32 	%r64, 0;
	@%p7 bra 	$L__BB16_32;

	setp.eq.s64 	%p8, %rd5, 0;
	@%p8 bra 	$L__BB16_32;

	add.s64 	%rd32, %rd7, -1;
	and.b64  	%rd33, %rd7, 3;
	sub.s64 	%rd34, %rd33, %rd7;
	shl.b64 	%rd94, %rd31, 2;
	add.s64 	%rd35, %rd2, %rd94;
	mul.lo.s64 	%rd42, %rd162, %rd77;
	shl.b64 	%rd95, %rd42, 2;
	shl.b64 	%rd96, %rd78, 2;
	sub.s64 	%rd36, %rd95, %rd96;
	shl.b64 	%rd37, %rd79, 2;
	mul.lo.s64 	%rd40, %rd163, %rd77;
	shl.b64 	%rd97, %rd40, 2;
	sub.s64 	%rd38, %rd97, %rd96;
	add.s64 	%rd39, %rd11, %rd78;
	add.s64 	%rd41, %rd12, %rd78;
	mov.u32 	%r64, 0;
	mov.u64 	%rd164, 0;

$L__BB16_19:
	mul.lo.s64 	%rd98, %rd164, %rd79;
	add.s64 	%rd44, %rd98, %rd42;
	setp.lt.u64 	%p9, %rd44, %rd78;
	setp.ge.u64 	%p10, %rd44, %rd41;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB16_31;

	setp.eq.s64 	%p12, %rd7, 0;
	@%p12 bra 	$L__BB16_31;

	mov.u64 	%rd165, 0;

$L__BB16_22:
	mul.lo.s64 	%rd101, %rd165, %rd79;
	add.s64 	%rd49, %rd101, %rd40;
	setp.lt.u64 	%p13, %rd49, %rd78;
	setp.ge.u64 	%p14, %rd49, %rd39;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB16_30;

	setp.lt.u64 	%p16, %rd32, 3;
	ld.global.u64 	%rd50, [%rd4+40];
	mov.u64 	%rd169, 0;
	ld.global.u64 	%rd51, [%rd4+48];
	ld.global.u64 	%rd52, [%rd4+56];
	ld.global.u64 	%rd103, [%rd4+96];
	mul.lo.s64 	%rd53, %rd103, %rd160;
	ld.global.u64 	%rd54, [%rd4+104];
	ld.global.u64 	%rd55, [%rd4+112];
	ld.global.u64 	%rd56, [%rd4+120];
	@%p16 bra 	$L__BB16_26;

	mul.lo.s64 	%rd157, %rd37, %rd164;
	add.s64 	%rd156, %rd36, %rd157;
	shl.b64 	%rd150, %rd164, 2;
	mul.lo.s64 	%rd105, %rd156, %rd52;
	mul.lo.s64 	%rd106, %rd37, %rd165;
	add.s64 	%rd107, %rd38, %rd106;
	mul.lo.s64 	%rd108, %rd51, %rd107;
	add.s64 	%rd109, %rd105, %rd108;
	add.s64 	%rd167, %rd35, %rd109;
	shl.b64 	%rd110, %rd53, 2;
	add.s64 	%rd111, %rd1, %rd110;
	mul.lo.s64 	%rd112, %rd165, %rd55;
	shl.b64 	%rd113, %rd112, 2;
	mul.lo.s64 	%rd114, %rd150, %rd56;
	add.s64 	%rd115, %rd114, %rd113;
	add.s64 	%rd168, %rd111, %rd115;
	mov.u64 	%rd169, 0;

$L__BB16_25:
	shl.b64 	%rd147, %rd54, 2;
	shl.b64 	%rd146, %rd50, 2;
	ld.global.u32 	%r38, [%rd168];
	ld.global.u32 	%r39, [%rd167];
	mad.lo.s32 	%r40, %r38, %r39, %r64;
	add.s64 	%rd116, %rd167, %rd146;
	add.s64 	%rd117, %rd168, %rd147;
	ld.global.u32 	%r41, [%rd117];
	ld.global.u32 	%r42, [%rd116];
	mad.lo.s32 	%r43, %r41, %r42, %r40;
	add.s64 	%rd118, %rd116, %rd146;
	add.s64 	%rd119, %rd117, %rd147;
	ld.global.u32 	%r44, [%rd119];
	ld.global.u32 	%r45, [%rd118];
	mad.lo.s32 	%r46, %r44, %r45, %r43;
	add.s64 	%rd120, %rd118, %rd146;
	add.s64 	%rd167, %rd120, %rd146;
	add.s64 	%rd121, %rd119, %rd147;
	add.s64 	%rd168, %rd121, %rd147;
	ld.global.u32 	%r47, [%rd121];
	ld.global.u32 	%r48, [%rd120];
	mad.lo.s32 	%r64, %r47, %r48, %r46;
	add.s64 	%rd169, %rd169, 4;
	add.s64 	%rd122, %rd34, %rd169;
	setp.ne.s64 	%p17, %rd122, 0;
	@%p17 bra 	$L__BB16_25;

$L__BB16_26:
	mul.lo.s64 	%rd152, %rd165, %rd79;
	add.s64 	%rd151, %rd152, %rd40;
	setp.eq.s64 	%p18, %rd33, 0;
	sub.s64 	%rd123, %rd151, %rd78;
	mul.lo.s64 	%rd68, %rd51, %rd123;
	@%p18 bra 	$L__BB16_30;

	mul.lo.s64 	%rd155, %rd164, %rd79;
	add.s64 	%rd154, %rd155, %rd42;
	sub.s64 	%rd153, %rd154, %rd78;
	setp.eq.s64 	%p19, %rd33, 1;
	mul.lo.s64 	%rd124, %rd50, %rd169;
	add.s64 	%rd125, %rd124, %rd31;
	add.s64 	%rd126, %rd125, %rd68;
	mul.lo.s64 	%rd127, %rd52, %rd153;
	add.s64 	%rd128, %rd126, %rd127;
	mul.lo.s64 	%rd129, %rd54, %rd169;
	add.s64 	%rd130, %rd129, %rd53;
	mul.lo.s64 	%rd131, %rd55, %rd165;
	add.s64 	%rd132, %rd130, %rd131;
	mul.lo.s64 	%rd133, %rd56, %rd164;
	add.s64 	%rd134, %rd132, %rd133;
	shl.b64 	%rd135, %rd128, 2;
	add.s64 	%rd69, %rd2, %rd135;
	shl.b64 	%rd136, %rd134, 2;
	add.s64 	%rd70, %rd1, %rd136;
	ld.global.u32 	%r49, [%rd70];
	ld.global.u32 	%r50, [%rd69];
	mad.lo.s32 	%r64, %r49, %r50, %r64;
	@%p19 bra 	$L__BB16_30;

	setp.eq.s64 	%p20, %rd33, 2;
	shl.b64 	%rd137, %rd54, 2;
	add.s64 	%rd71, %rd70, %rd137;
	ld.global.u32 	%r51, [%rd71];
	shl.b64 	%rd138, %rd50, 2;
	add.s64 	%rd72, %rd69, %rd138;
	ld.global.u32 	%r52, [%rd72];
	mad.lo.s32 	%r64, %r51, %r52, %r64;
	@%p20 bra 	$L__BB16_30;

	add.s64 	%rd140, %rd72, %rd138;
	add.s64 	%rd142, %rd71, %rd137;
	ld.global.u32 	%r53, [%rd142];
	ld.global.u32 	%r54, [%rd140];
	mad.lo.s32 	%r64, %r53, %r54, %r64;

$L__BB16_30:
	add.s64 	%rd165, %rd165, 1;
	setp.lt.u64 	%p21, %rd165, %rd5;
	@%p21 bra 	$L__BB16_22;

$L__BB16_31:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p22, %rd164, %rd6;
	@%p22 bra 	$L__BB16_19;

$L__BB16_32:
	ld.param.u64 	%rd149, [conv2d_u32_param_9];
	mov.u32 	%r58, %tid.x;
	mov.u32 	%r57, %ntid.x;
	mov.u32 	%r56, %ctaid.x;
	mad.lo.s32 	%r55, %r56, %r57, %r58;
	cvt.u64.u32 	%rd148, %r55;
	cvta.to.global.u64 	%rd143, %rd149;
	shl.b64 	%rd144, %rd148, 2;
	add.s64 	%rd145, %rd143, %rd144;
	st.global.u32 	[%rd145], %r64;

$L__BB16_33:
	ret;

}
	// .globl	conv_transpose1d_f32
.visible .entry conv_transpose1d_f32(
	.param .u64 conv_transpose1d_f32_param_0,
	.param .u64 conv_transpose1d_f32_param_1,
	.param .u64 conv_transpose1d_f32_param_2,
	.param .u64 conv_transpose1d_f32_param_3,
	.param .u64 conv_transpose1d_f32_param_4,
	.param .u64 conv_transpose1d_f32_param_5,
	.param .u64 conv_transpose1d_f32_param_6,
	.param .u64 conv_transpose1d_f32_param_7,
	.param .u64 conv_transpose1d_f32_param_8,
	.param .u64 conv_transpose1d_f32_param_9
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<130>;


	ld.param.u64 	%rd62, [conv_transpose1d_f32_param_1];
	ld.param.u64 	%rd63, [conv_transpose1d_f32_param_2];
	ld.param.u64 	%rd64, [conv_transpose1d_f32_param_3];
	ld.param.u64 	%rd65, [conv_transpose1d_f32_param_5];
	ld.param.u64 	%rd67, [conv_transpose1d_f32_param_6];
	ld.param.u64 	%rd68, [conv_transpose1d_f32_param_7];
	ld.param.u64 	%rd69, [conv_transpose1d_f32_param_8];
	ld.param.u64 	%rd66, [conv_transpose1d_f32_param_9];
	cvta.to.global.u64 	%rd1, %rd69;
	cvta.to.global.u64 	%rd2, %rd68;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	cvt.u64.u32 	%rd3, %r7;
	cvta.to.global.u64 	%rd4, %rd67;
	ld.global.u64 	%rd5, [%rd4+8];
	ld.global.u64 	%rd6, [%rd4+56];
	mul.lo.s64 	%rd7, %rd6, %rd62;
	ld.global.u64 	%rd70, [%rd4];
	mul.lo.s64 	%rd71, %rd7, %rd70;
	setp.le.u64 	%p1, %rd71, %rd3;
	@%p1 bra 	$L__BB17_31;

	ld.global.u64 	%rd8, [%rd4+16];
	ld.global.u64 	%rd9, [%rd4+64];
	and.b64  	%rd72, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd72, 0;
	@%p2 bra 	$L__BB17_3;

	div.u64 	%rd118, %rd3, %rd7;
	bra.uni 	$L__BB17_4;

$L__BB17_3:
	cvt.u32.u64 	%r8, %rd7;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd118, %r10;

$L__BB17_4:
	and.b64  	%rd73, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd73, 0;
	@%p3 bra 	$L__BB17_6;

	div.u64 	%rd119, %rd3, %rd62;
	mul.lo.s64 	%rd74, %rd119, %rd62;
	sub.s64 	%rd120, %rd3, %rd74;
	bra.uni 	$L__BB17_7;

$L__BB17_6:
	cvt.u32.u64 	%r11, %rd62;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	mul.lo.s32 	%r14, %r13, %r11;
	sub.s32 	%r15, %r12, %r14;
	cvt.u64.u32 	%rd119, %r13;
	cvt.u64.u32 	%rd120, %r15;

$L__BB17_7:
	and.b64  	%rd75, %rd6, -4294967296;
	setp.eq.s64 	%p4, %rd75, 0;
	@%p4 bra 	$L__BB17_9;

	rem.u64 	%rd121, %rd119, %rd6;
	bra.uni 	$L__BB17_10;

$L__BB17_9:
	cvt.u32.u64 	%r16, %rd6;
	cvt.u32.u64 	%r17, %rd119;
	rem.u32 	%r18, %r17, %r16;
	cvt.u64.u32 	%rd121, %r18;

$L__BB17_10:
	ld.global.u64 	%rd76, [%rd4+24];
	mul.lo.s64 	%rd22, %rd76, %rd118;
	cvt.u32.u64 	%r1, %rd9;
	setp.lt.s32 	%p5, %r1, 1;
	mov.f32 	%f37, 0f00000000;
	@%p5 bra 	$L__BB17_30;

	setp.eq.s64 	%p6, %rd5, 0;
	@%p6 bra 	$L__BB17_30;

	add.s64 	%rd23, %rd5, -1;
	and.b64  	%rd24, %rd5, 3;
	sub.s64 	%rd25, %rd5, %rd24;
	shl.b64 	%rd78, %rd22, 2;
	add.s64 	%rd26, %rd2, %rd78;
	add.s64 	%rd27, %rd120, %rd64;
	mov.f32 	%f37, 0f00000000;
	mov.u32 	%r28, 0;
	mov.u64 	%rd122, 0;
	cvt.u32.u64 	%r21, %rd63;

$L__BB17_13:
	cvt.s64.s32 	%rd29, %r28;
	mul.lo.s64 	%rd79, %rd29, %rd65;
	sub.s64 	%rd30, %rd27, %rd79;
	cvt.u32.u64 	%r20, %rd30;
	setp.lt.s32 	%p7, %r20, 0;
	@%p7 bra 	$L__BB17_29;

	cvt.s64.s32 	%rd31, %rd30;
	or.b64  	%rd80, %rd31, %rd63;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p8, %rd81, 0;
	@%p8 bra 	$L__BB17_16;

	rem.u64 	%rd123, %rd31, %rd63;
	bra.uni 	$L__BB17_17;

$L__BB17_16:
	cvt.u32.u64 	%r22, %rd31;
	rem.u32 	%r23, %r22, %r21;
	cvt.u64.u32 	%rd123, %r23;

$L__BB17_17:
	setp.ne.s64 	%p9, %rd123, 0;
	@%p9 bra 	$L__BB17_29;

	@%p8 bra 	$L__BB17_20;

	div.u64 	%rd124, %rd31, %rd63;
	bra.uni 	$L__BB17_21;

$L__BB17_20:
	cvt.u32.u64 	%r25, %rd31;
	div.u32 	%r26, %r25, %r21;
	cvt.u64.u32 	%rd124, %r26;

$L__BB17_21:
	cvt.s64.s32 	%rd38, %rd124;
	setp.ge.u64 	%p11, %rd38, %rd8;
	@%p11 bra 	$L__BB17_29;

	setp.lt.u64 	%p12, %rd23, 3;
	ld.global.u64 	%rd39, [%rd4+32];
	mov.u64 	%rd129, 0;
	ld.global.u64 	%rd85, [%rd4+40];
	mul.lo.s64 	%rd40, %rd85, %rd38;
	ld.global.u64 	%rd41, [%rd4+72];
	ld.global.u64 	%rd86, [%rd4+80];
	mul.lo.s64 	%rd42, %rd86, %rd121;
	ld.global.u64 	%rd43, [%rd4+88];
	@%p12 bra 	$L__BB17_25;

	shl.b64 	%rd88, %rd40, 2;
	add.s64 	%rd127, %rd26, %rd88;
	shl.b64 	%rd89, %rd42, 2;
	add.s64 	%rd90, %rd1, %rd89;
	mul.lo.s64 	%rd91, %rd43, %rd122;
	add.s64 	%rd128, %rd90, %rd91;
	mov.u64 	%rd129, 0;
	mov.u64 	%rd126, %rd25;

$L__BB17_24:
	shl.b64 	%rd117, %rd41, 2;
	shl.b64 	%rd116, %rd39, 2;
	ld.global.f32 	%f15, [%rd128];
	ld.global.f32 	%f16, [%rd127];
	fma.rn.f32 	%f17, %f16, %f15, %f37;
	add.s64 	%rd92, %rd127, %rd116;
	add.s64 	%rd93, %rd128, %rd117;
	ld.global.f32 	%f18, [%rd93];
	ld.global.f32 	%f19, [%rd92];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	add.s64 	%rd94, %rd92, %rd116;
	add.s64 	%rd95, %rd93, %rd117;
	ld.global.f32 	%f21, [%rd95];
	ld.global.f32 	%f22, [%rd94];
	fma.rn.f32 	%f23, %f22, %f21, %f20;
	add.s64 	%rd96, %rd94, %rd116;
	add.s64 	%rd127, %rd96, %rd116;
	add.s64 	%rd97, %rd95, %rd117;
	add.s64 	%rd128, %rd97, %rd117;
	ld.global.f32 	%f24, [%rd97];
	ld.global.f32 	%f25, [%rd96];
	fma.rn.f32 	%f37, %f25, %f24, %f23;
	add.s64 	%rd129, %rd129, 4;
	add.s64 	%rd126, %rd126, -4;
	setp.ne.s64 	%p13, %rd126, 0;
	@%p13 bra 	$L__BB17_24;

$L__BB17_25:
	setp.eq.s64 	%p14, %rd24, 0;
	@%p14 bra 	$L__BB17_29;

	setp.eq.s64 	%p15, %rd24, 1;
	mul.lo.s64 	%rd98, %rd39, %rd129;
	add.s64 	%rd99, %rd98, %rd22;
	add.s64 	%rd100, %rd99, %rd40;
	mul.lo.s64 	%rd101, %rd41, %rd129;
	add.s64 	%rd102, %rd42, %rd101;
	mul.lo.s64 	%rd103, %rd43, %rd29;
	add.s64 	%rd104, %rd102, %rd103;
	shl.b64 	%rd105, %rd100, 2;
	add.s64 	%rd57, %rd2, %rd105;
	shl.b64 	%rd106, %rd104, 2;
	add.s64 	%rd58, %rd1, %rd106;
	ld.global.f32 	%f26, [%rd58];
	ld.global.f32 	%f27, [%rd57];
	fma.rn.f32 	%f37, %f27, %f26, %f37;
	@%p15 bra 	$L__BB17_29;

	setp.eq.s64 	%p16, %rd24, 2;
	shl.b64 	%rd107, %rd41, 2;
	add.s64 	%rd59, %rd58, %rd107;
	ld.global.f32 	%f28, [%rd59];
	shl.b64 	%rd108, %rd39, 2;
	add.s64 	%rd60, %rd57, %rd108;
	ld.global.f32 	%f29, [%rd60];
	fma.rn.f32 	%f37, %f29, %f28, %f37;
	@%p16 bra 	$L__BB17_29;

	add.s64 	%rd110, %rd60, %rd108;
	add.s64 	%rd112, %rd59, %rd107;
	ld.global.f32 	%f30, [%rd112];
	ld.global.f32 	%f31, [%rd110];
	fma.rn.f32 	%f37, %f31, %f30, %f37;

$L__BB17_29:
	cvt.u32.u64 	%r27, %rd29;
	add.s32 	%r28, %r27, 1;
	setp.lt.s32 	%p17, %r28, %r1;
	add.s64 	%rd122, %rd122, 4;
	@%p17 bra 	$L__BB17_13;

$L__BB17_30:
	cvta.to.global.u64 	%rd113, %rd66;
	shl.b64 	%rd114, %rd3, 2;
	add.s64 	%rd115, %rd113, %rd114;
	st.global.f32 	[%rd115], %f37;

$L__BB17_31:
	ret;

}
	// .globl	conv_transpose1d_f64
.visible .entry conv_transpose1d_f64(
	.param .u64 conv_transpose1d_f64_param_0,
	.param .u64 conv_transpose1d_f64_param_1,
	.param .u64 conv_transpose1d_f64_param_2,
	.param .u64 conv_transpose1d_f64_param_3,
	.param .u64 conv_transpose1d_f64_param_4,
	.param .u64 conv_transpose1d_f64_param_5,
	.param .u64 conv_transpose1d_f64_param_6,
	.param .u64 conv_transpose1d_f64_param_7,
	.param .u64 conv_transpose1d_f64_param_8,
	.param .u64 conv_transpose1d_f64_param_9
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<33>;
	.reg .f64 	%fd<39>;
	.reg .b64 	%rd<132>;


	ld.param.u64 	%rd62, [conv_transpose1d_f64_param_1];
	ld.param.u64 	%rd63, [conv_transpose1d_f64_param_2];
	ld.param.u64 	%rd64, [conv_transpose1d_f64_param_3];
	ld.param.u64 	%rd67, [conv_transpose1d_f64_param_6];
	ld.param.u64 	%rd68, [conv_transpose1d_f64_param_7];
	ld.param.u64 	%rd69, [conv_transpose1d_f64_param_8];
	ld.param.u64 	%rd66, [conv_transpose1d_f64_param_9];
	cvta.to.global.u64 	%rd1, %rd69;
	cvta.to.global.u64 	%rd2, %rd68;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	cvt.u64.u32 	%rd3, %r7;
	cvta.to.global.u64 	%rd4, %rd67;
	ld.global.u64 	%rd5, [%rd4+8];
	ld.global.u64 	%rd6, [%rd4+56];
	mul.lo.s64 	%rd7, %rd6, %rd62;
	ld.global.u64 	%rd70, [%rd4];
	mul.lo.s64 	%rd71, %rd7, %rd70;
	setp.le.u64 	%p1, %rd71, %rd3;
	@%p1 bra 	$L__BB18_31;

	ld.global.u64 	%rd8, [%rd4+16];
	ld.global.u64 	%rd9, [%rd4+64];
	and.b64  	%rd72, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd72, 0;
	@%p2 bra 	$L__BB18_3;

	div.u64 	%rd120, %rd3, %rd7;
	bra.uni 	$L__BB18_4;

$L__BB18_3:
	cvt.u32.u64 	%r8, %rd7;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd120, %r10;

$L__BB18_4:
	and.b64  	%rd73, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd73, 0;
	@%p3 bra 	$L__BB18_6;

	div.u64 	%rd121, %rd3, %rd62;
	mul.lo.s64 	%rd74, %rd121, %rd62;
	sub.s64 	%rd122, %rd3, %rd74;
	bra.uni 	$L__BB18_7;

$L__BB18_6:
	cvt.u32.u64 	%r11, %rd62;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	mul.lo.s32 	%r14, %r13, %r11;
	sub.s32 	%r15, %r12, %r14;
	cvt.u64.u32 	%rd121, %r13;
	cvt.u64.u32 	%rd122, %r15;

$L__BB18_7:
	and.b64  	%rd75, %rd6, -4294967296;
	setp.eq.s64 	%p4, %rd75, 0;
	@%p4 bra 	$L__BB18_9;

	rem.u64 	%rd123, %rd121, %rd6;
	bra.uni 	$L__BB18_10;

$L__BB18_9:
	cvt.u32.u64 	%r16, %rd6;
	cvt.u32.u64 	%r17, %rd121;
	rem.u32 	%r18, %r17, %r16;
	cvt.u64.u32 	%rd123, %r18;

$L__BB18_10:
	ld.global.u64 	%rd76, [%rd4+24];
	mul.lo.s64 	%rd22, %rd76, %rd120;
	cvt.u32.u64 	%r1, %rd9;
	setp.lt.s32 	%p5, %r1, 1;
	mov.f64 	%fd37, 0d0000000000000000;
	@%p5 bra 	$L__BB18_30;

	setp.eq.s64 	%p6, %rd5, 0;
	@%p6 bra 	$L__BB18_30;

	add.s64 	%rd23, %rd5, -1;
	and.b64  	%rd24, %rd5, 3;
	sub.s64 	%rd25, %rd5, %rd24;
	shl.b64 	%rd78, %rd22, 3;
	add.s64 	%rd26, %rd2, %rd78;
	add.s64 	%rd27, %rd122, %rd64;
	mov.f64 	%fd37, 0d0000000000000000;
	mov.u32 	%r32, 0;
	mov.u64 	%rd124, 0;
	cvt.u32.u64 	%r21, %rd63;

$L__BB18_13:
	ld.param.u64 	%rd119, [conv_transpose1d_f64_param_5];
	cvt.s64.s32 	%rd29, %r32;
	mul.lo.s64 	%rd79, %rd29, %rd119;
	sub.s64 	%rd30, %rd27, %rd79;
	cvt.u32.u64 	%r20, %rd30;
	setp.lt.s32 	%p7, %r20, 0;
	@%p7 bra 	$L__BB18_29;

	cvt.s64.s32 	%rd31, %rd30;
	or.b64  	%rd80, %rd31, %rd63;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p8, %rd81, 0;
	@%p8 bra 	$L__BB18_16;

	rem.u64 	%rd125, %rd31, %rd63;
	bra.uni 	$L__BB18_17;

$L__BB18_16:
	cvt.u32.u64 	%r22, %rd31;
	rem.u32 	%r23, %r22, %r21;
	cvt.u64.u32 	%rd125, %r23;

$L__BB18_17:
	setp.ne.s64 	%p9, %rd125, 0;
	@%p9 bra 	$L__BB18_29;

	@%p8 bra 	$L__BB18_20;

	div.u64 	%rd126, %rd31, %rd63;
	bra.uni 	$L__BB18_21;

$L__BB18_20:
	cvt.u32.u64 	%r25, %rd31;
	div.u32 	%r26, %r25, %r21;
	cvt.u64.u32 	%rd126, %r26;

$L__BB18_21:
	cvt.s64.s32 	%rd38, %rd126;
	setp.ge.u64 	%p11, %rd38, %rd8;
	@%p11 bra 	$L__BB18_29;

	setp.lt.u64 	%p12, %rd23, 3;
	ld.global.u64 	%rd39, [%rd4+32];
	mov.u64 	%rd131, 0;
	ld.global.u64 	%rd85, [%rd4+40];
	mul.lo.s64 	%rd40, %rd85, %rd38;
	ld.global.u64 	%rd41, [%rd4+72];
	ld.global.u64 	%rd86, [%rd4+80];
	mul.lo.s64 	%rd42, %rd86, %rd123;
	ld.global.u64 	%rd43, [%rd4+88];
	@%p12 bra 	$L__BB18_25;

	shl.b64 	%rd88, %rd40, 3;
	add.s64 	%rd129, %rd26, %rd88;
	shl.b64 	%rd89, %rd42, 3;
	add.s64 	%rd90, %rd1, %rd89;
	mul.lo.s64 	%rd91, %rd43, %rd124;
	add.s64 	%rd130, %rd90, %rd91;
	mov.u64 	%rd131, 0;
	mov.u64 	%rd128, %rd25;

$L__BB18_24:
	shl.b64 	%rd117, %rd41, 3;
	shl.b64 	%rd116, %rd39, 3;
	ld.global.f64 	%fd15, [%rd130];
	ld.global.f64 	%fd16, [%rd129];
	fma.rn.f64 	%fd17, %fd16, %fd15, %fd37;
	add.s64 	%rd92, %rd129, %rd116;
	add.s64 	%rd93, %rd130, %rd117;
	ld.global.f64 	%fd18, [%rd93];
	ld.global.f64 	%fd19, [%rd92];
	fma.rn.f64 	%fd20, %fd19, %fd18, %fd17;
	add.s64 	%rd94, %rd92, %rd116;
	add.s64 	%rd95, %rd93, %rd117;
	ld.global.f64 	%fd21, [%rd95];
	ld.global.f64 	%fd22, [%rd94];
	fma.rn.f64 	%fd23, %fd22, %fd21, %fd20;
	add.s64 	%rd96, %rd94, %rd116;
	add.s64 	%rd129, %rd96, %rd116;
	add.s64 	%rd97, %rd95, %rd117;
	add.s64 	%rd130, %rd97, %rd117;
	ld.global.f64 	%fd24, [%rd97];
	ld.global.f64 	%fd25, [%rd96];
	fma.rn.f64 	%fd37, %fd25, %fd24, %fd23;
	add.s64 	%rd131, %rd131, 4;
	add.s64 	%rd128, %rd128, -4;
	setp.ne.s64 	%p13, %rd128, 0;
	@%p13 bra 	$L__BB18_24;

$L__BB18_25:
	setp.eq.s64 	%p14, %rd24, 0;
	@%p14 bra 	$L__BB18_29;

	setp.eq.s64 	%p15, %rd24, 1;
	mul.lo.s64 	%rd98, %rd39, %rd131;
	add.s64 	%rd99, %rd98, %rd22;
	add.s64 	%rd100, %rd99, %rd40;
	mul.lo.s64 	%rd101, %rd41, %rd131;
	add.s64 	%rd102, %rd42, %rd101;
	mul.lo.s64 	%rd103, %rd43, %rd29;
	add.s64 	%rd104, %rd102, %rd103;
	shl.b64 	%rd105, %rd100, 3;
	add.s64 	%rd57, %rd2, %rd105;
	shl.b64 	%rd106, %rd104, 3;
	add.s64 	%rd58, %rd1, %rd106;
	ld.global.f64 	%fd26, [%rd58];
	ld.global.f64 	%fd27, [%rd57];
	fma.rn.f64 	%fd37, %fd27, %fd26, %fd37;
	@%p15 bra 	$L__BB18_29;

	setp.eq.s64 	%p16, %rd24, 2;
	shl.b64 	%rd107, %rd41, 3;
	add.s64 	%rd59, %rd58, %rd107;
	ld.global.f64 	%fd28, [%rd59];
	shl.b64 	%rd108, %rd39, 3;
	add.s64 	%rd60, %rd57, %rd108;
	ld.global.f64 	%fd29, [%rd60];
	fma.rn.f64 	%fd37, %fd29, %fd28, %fd37;
	@%p16 bra 	$L__BB18_29;

	add.s64 	%rd110, %rd60, %rd108;
	add.s64 	%rd112, %rd59, %rd107;
	ld.global.f64 	%fd30, [%rd112];
	ld.global.f64 	%fd31, [%rd110];
	fma.rn.f64 	%fd37, %fd31, %fd30, %fd37;

$L__BB18_29:
	cvt.u32.u64 	%r27, %rd29;
	add.s32 	%r32, %r27, 1;
	setp.lt.s32 	%p17, %r32, %r1;
	add.s64 	%rd124, %rd124, 8;
	@%p17 bra 	$L__BB18_13;

$L__BB18_30:
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	cvt.u64.u32 	%rd118, %r28;
	cvta.to.global.u64 	%rd113, %rd66;
	shl.b64 	%rd114, %rd118, 3;
	add.s64 	%rd115, %rd113, %rd114;
	st.global.f64 	[%rd115], %fd37;

$L__BB18_31:
	ret;

}
	// .globl	conv_transpose1d_u8
.visible .entry conv_transpose1d_u8(
	.param .u64 conv_transpose1d_u8_param_0,
	.param .u64 conv_transpose1d_u8_param_1,
	.param .u64 conv_transpose1d_u8_param_2,
	.param .u64 conv_transpose1d_u8_param_3,
	.param .u64 conv_transpose1d_u8_param_4,
	.param .u64 conv_transpose1d_u8_param_5,
	.param .u64 conv_transpose1d_u8_param_6,
	.param .u64 conv_transpose1d_u8_param_7,
	.param .u64 conv_transpose1d_u8_param_8,
	.param .u64 conv_transpose1d_u8_param_9
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<111>;


	ld.param.u64 	%rd58, [conv_transpose1d_u8_param_1];
	ld.param.u64 	%rd59, [conv_transpose1d_u8_param_2];
	ld.param.u64 	%rd60, [conv_transpose1d_u8_param_3];
	ld.param.u64 	%rd61, [conv_transpose1d_u8_param_5];
	ld.param.u64 	%rd63, [conv_transpose1d_u8_param_6];
	ld.param.u64 	%rd64, [conv_transpose1d_u8_param_7];
	ld.param.u64 	%rd65, [conv_transpose1d_u8_param_8];
	ld.param.u64 	%rd62, [conv_transpose1d_u8_param_9];
	cvta.to.global.u64 	%rd1, %rd65;
	cvta.to.global.u64 	%rd2, %rd64;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	cvt.u64.u32 	%rd3, %r7;
	cvta.to.global.u64 	%rd4, %rd63;
	ld.global.u64 	%rd5, [%rd4+8];
	ld.global.u64 	%rd6, [%rd4+56];
	mul.lo.s64 	%rd7, %rd6, %rd58;
	ld.global.u64 	%rd66, [%rd4];
	mul.lo.s64 	%rd67, %rd7, %rd66;
	setp.le.u64 	%p1, %rd67, %rd3;
	@%p1 bra 	$L__BB19_31;

	ld.global.u64 	%rd8, [%rd4+16];
	ld.global.u64 	%rd9, [%rd4+64];
	and.b64  	%rd68, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB19_3;

	div.u64 	%rd100, %rd3, %rd7;
	bra.uni 	$L__BB19_4;

$L__BB19_3:
	cvt.u32.u64 	%r8, %rd7;
	cvt.u32.u64 	%r9, %rd3;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd100, %r10;

$L__BB19_4:
	and.b64  	%rd69, %rd58, -4294967296;
	setp.eq.s64 	%p3, %rd69, 0;
	@%p3 bra 	$L__BB19_6;

	div.u64 	%rd101, %rd3, %rd58;
	mul.lo.s64 	%rd70, %rd101, %rd58;
	sub.s64 	%rd102, %rd3, %rd70;
	bra.uni 	$L__BB19_7;

$L__BB19_6:
	cvt.u32.u64 	%r11, %rd58;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	mul.lo.s32 	%r14, %r13, %r11;
	sub.s32 	%r15, %r12, %r14;
	cvt.u64.u32 	%rd101, %r13;
	cvt.u64.u32 	%rd102, %r15;

$L__BB19_7:
	and.b64  	%rd71, %rd6, -4294967296;
	setp.eq.s64 	%p4, %rd71, 0;
	@%p4 bra 	$L__BB19_9;

	rem.u64 	%rd103, %rd101, %rd6;
	bra.uni 	$L__BB19_10;

$L__BB19_9:
	cvt.u32.u64 	%r16, %rd6;
	cvt.u32.u64 	%r17, %rd101;
	rem.u32 	%r18, %r17, %r16;
	cvt.u64.u32 	%rd103, %r18;

$L__BB19_10:
	ld.global.u64 	%rd72, [%rd4+24];
	mul.lo.s64 	%rd22, %rd72, %rd100;
	cvt.u32.u64 	%r1, %rd9;
	setp.lt.s32 	%p5, %r1, 1;
	mov.u16 	%rs43, 0;
	@%p5 bra 	$L__BB19_30;

	setp.eq.s64 	%p6, %rd5, 0;
	@%p6 bra 	$L__BB19_30;

	add.s64 	%rd23, %rd5, -1;
	and.b64  	%rd24, %rd5, 3;
	sub.s64 	%rd25, %rd5, %rd24;
	add.s64 	%rd26, %rd2, %rd22;
	add.s64 	%rd27, %rd102, %rd60;
	mov.u16 	%rs43, 0;
	mov.u32 	%r28, 0;
	cvt.u32.u64 	%r21, %rd59;

$L__BB19_13:
	cvt.s64.s32 	%rd28, %r28;
	mul.lo.s64 	%rd73, %rd28, %rd61;
	sub.s64 	%rd29, %rd27, %rd73;
	cvt.u32.u64 	%r20, %rd29;
	setp.lt.s32 	%p7, %r20, 0;
	@%p7 bra 	$L__BB19_29;

	cvt.s64.s32 	%rd30, %rd29;
	or.b64  	%rd74, %rd30, %rd59;
	and.b64  	%rd75, %rd74, -4294967296;
	setp.eq.s64 	%p8, %rd75, 0;
	@%p8 bra 	$L__BB19_16;

	rem.u64 	%rd104, %rd30, %rd59;
	bra.uni 	$L__BB19_17;

$L__BB19_16:
	cvt.u32.u64 	%r22, %rd30;
	rem.u32 	%r23, %r22, %r21;
	cvt.u64.u32 	%rd104, %r23;

$L__BB19_17:
	setp.ne.s64 	%p9, %rd104, 0;
	@%p9 bra 	$L__BB19_29;

	@%p8 bra 	$L__BB19_20;

	div.u64 	%rd105, %rd30, %rd59;
	bra.uni 	$L__BB19_21;

$L__BB19_20:
	cvt.u32.u64 	%r25, %rd30;
	div.u32 	%r26, %r25, %r21;
	cvt.u64.u32 	%rd105, %r26;

$L__BB19_21:
	cvt.s64.s32 	%rd37, %rd105;
	setp.ge.u64 	%p11, %rd37, %rd8;
	@%p11 bra 	$L__BB19_29;

	setp.lt.u64 	%p12, %rd23, 3;
	ld.global.u64 	%rd38, [%rd4+32];
	mov.u64 	%rd110, 0;
	ld.global.u64 	%rd79, [%rd4+40];
	mul.lo.s64 	%rd39, %rd79, %rd37;
	ld.global.u64 	%rd40, [%rd4+72];
	ld.global.u64 	%rd80, [%rd4+80];
	mul.lo.s64 	%rd41, %rd80, %rd103;
	ld.global.u64 	%rd81, [%rd4+88];
	mul.lo.s64 	%rd42, %rd81, %rd28;
	@%p12 bra 	$L__BB19_25;

	add.s64 	%rd108, %rd26, %rd39;
	add.s64 	%rd83, %rd41, %rd42;
	add.s64 	%rd109, %rd1, %rd83;
	mov.u64 	%rd110, 0;
	mov.u64 	%rd107, %rd25;

$L__BB19_24:
	ld.global.u8 	%rs15, [%rd109];
	ld.global.u8 	%rs16, [%rd108];
	mul.lo.s16 	%rs17, %rs15, %rs16;
	add.s16 	%rs18, %rs17, %rs43;
	add.s64 	%rd84, %rd109, %rd40;
	ld.global.u8 	%rs19, [%rd84];
	add.s64 	%rd85, %rd108, %rd38;
	ld.global.u8 	%rs20, [%rd85];
	mul.lo.s16 	%rs21, %rs19, %rs20;
	add.s16 	%rs22, %rs21, %rs18;
	add.s64 	%rd86, %rd85, %rd38;
	add.s64 	%rd87, %rd84, %rd40;
	ld.global.u8 	%rs23, [%rd87];
	ld.global.u8 	%rs24, [%rd86];
	mul.lo.s16 	%rs25, %rs23, %rs24;
	add.s16 	%rs26, %rs25, %rs22;
	add.s64 	%rd88, %rd86, %rd38;
	add.s64 	%rd108, %rd88, %rd38;
	add.s64 	%rd89, %rd87, %rd40;
	add.s64 	%rd109, %rd89, %rd40;
	ld.global.u8 	%rs27, [%rd89];
	ld.global.u8 	%rs28, [%rd88];
	mul.lo.s16 	%rs29, %rs27, %rs28;
	add.s16 	%rs43, %rs29, %rs26;
	add.s64 	%rd110, %rd110, 4;
	add.s64 	%rd107, %rd107, -4;
	setp.ne.s64 	%p13, %rd107, 0;
	@%p13 bra 	$L__BB19_24;

$L__BB19_25:
	setp.eq.s64 	%p14, %rd24, 0;
	@%p14 bra 	$L__BB19_29;

	setp.eq.s64 	%p15, %rd24, 1;
	mul.lo.s64 	%rd90, %rd38, %rd110;
	add.s64 	%rd91, %rd90, %rd22;
	add.s64 	%rd92, %rd91, %rd39;
	mul.lo.s64 	%rd93, %rd40, %rd110;
	add.s64 	%rd94, %rd41, %rd93;
	add.s64 	%rd95, %rd94, %rd42;
	add.s64 	%rd54, %rd2, %rd92;
	add.s64 	%rd55, %rd1, %rd95;
	ld.global.u8 	%rs30, [%rd55];
	ld.global.u8 	%rs31, [%rd54];
	mul.lo.s16 	%rs32, %rs30, %rs31;
	add.s16 	%rs43, %rs32, %rs43;
	@%p15 bra 	$L__BB19_29;

	setp.eq.s64 	%p16, %rd24, 2;
	add.s64 	%rd56, %rd55, %rd40;
	ld.global.u8 	%rs33, [%rd56];
	add.s64 	%rd57, %rd54, %rd38;
	ld.global.u8 	%rs34, [%rd57];
	mul.lo.s16 	%rs35, %rs33, %rs34;
	add.s16 	%rs43, %rs35, %rs43;
	@%p16 bra 	$L__BB19_29;

	add.s64 	%rd96, %rd57, %rd38;
	add.s64 	%rd97, %rd56, %rd40;
	ld.global.u8 	%rs36, [%rd97];
	ld.global.u8 	%rs37, [%rd96];
	mul.lo.s16 	%rs38, %rs36, %rs37;
	add.s16 	%rs43, %rs38, %rs43;

$L__BB19_29:
	cvt.u32.u64 	%r27, %rd28;
	add.s32 	%r28, %r27, 1;
	setp.lt.s32 	%p17, %r28, %r1;
	@%p17 bra 	$L__BB19_13;

$L__BB19_30:
	cvta.to.global.u64 	%rd98, %rd62;
	add.s64 	%rd99, %rd98, %rd3;
	st.global.u8 	[%rd99], %rs43;

$L__BB19_31:
	ret;

}
	// .globl	conv_transpose1d_u32
.visible .entry conv_transpose1d_u32(
	.param .u64 conv_transpose1d_u32_param_0,
	.param .u64 conv_transpose1d_u32_param_1,
	.param .u64 conv_transpose1d_u32_param_2,
	.param .u64 conv_transpose1d_u32_param_3,
	.param .u64 conv_transpose1d_u32_param_4,
	.param .u64 conv_transpose1d_u32_param_5,
	.param .u64 conv_transpose1d_u32_param_6,
	.param .u64 conv_transpose1d_u32_param_7,
	.param .u64 conv_transpose1d_u32_param_8,
	.param .u64 conv_transpose1d_u32_param_9
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<130>;


	ld.param.u64 	%rd62, [conv_transpose1d_u32_param_1];
	ld.param.u64 	%rd63, [conv_transpose1d_u32_param_2];
	ld.param.u64 	%rd64, [conv_transpose1d_u32_param_3];
	ld.param.u64 	%rd65, [conv_transpose1d_u32_param_5];
	ld.param.u64 	%rd67, [conv_transpose1d_u32_param_6];
	ld.param.u64 	%rd68, [conv_transpose1d_u32_param_7];
	ld.param.u64 	%rd69, [conv_transpose1d_u32_param_8];
	ld.param.u64 	%rd66, [conv_transpose1d_u32_param_9];
	cvta.to.global.u64 	%rd1, %rd69;
	cvta.to.global.u64 	%rd2, %rd68;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r17, %r15, %r14, %r16;
	cvt.u64.u32 	%rd3, %r17;
	cvta.to.global.u64 	%rd4, %rd67;
	ld.global.u64 	%rd5, [%rd4+8];
	ld.global.u64 	%rd6, [%rd4+56];
	mul.lo.s64 	%rd7, %rd6, %rd62;
	ld.global.u64 	%rd70, [%rd4];
	mul.lo.s64 	%rd71, %rd7, %rd70;
	setp.le.u64 	%p1, %rd71, %rd3;
	@%p1 bra 	$L__BB20_31;

	ld.global.u64 	%rd8, [%rd4+16];
	ld.global.u64 	%rd9, [%rd4+64];
	and.b64  	%rd72, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd72, 0;
	@%p2 bra 	$L__BB20_3;

	div.u64 	%rd118, %rd3, %rd7;
	bra.uni 	$L__BB20_4;

$L__BB20_3:
	cvt.u32.u64 	%r18, %rd7;
	cvt.u32.u64 	%r19, %rd3;
	div.u32 	%r20, %r19, %r18;
	cvt.u64.u32 	%rd118, %r20;

$L__BB20_4:
	and.b64  	%rd73, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd73, 0;
	@%p3 bra 	$L__BB20_6;

	div.u64 	%rd119, %rd3, %rd62;
	mul.lo.s64 	%rd74, %rd119, %rd62;
	sub.s64 	%rd120, %rd3, %rd74;
	bra.uni 	$L__BB20_7;

$L__BB20_6:
	cvt.u32.u64 	%r21, %rd62;
	cvt.u32.u64 	%r22, %rd3;
	div.u32 	%r23, %r22, %r21;
	mul.lo.s32 	%r24, %r23, %r21;
	sub.s32 	%r25, %r22, %r24;
	cvt.u64.u32 	%rd119, %r23;
	cvt.u64.u32 	%rd120, %r25;

$L__BB20_7:
	and.b64  	%rd75, %rd6, -4294967296;
	setp.eq.s64 	%p4, %rd75, 0;
	@%p4 bra 	$L__BB20_9;

	rem.u64 	%rd121, %rd119, %rd6;
	bra.uni 	$L__BB20_10;

$L__BB20_9:
	cvt.u32.u64 	%r26, %rd6;
	cvt.u32.u64 	%r27, %rd119;
	rem.u32 	%r28, %r27, %r26;
	cvt.u64.u32 	%rd121, %r28;

$L__BB20_10:
	ld.global.u64 	%rd76, [%rd4+24];
	mul.lo.s64 	%rd22, %rd76, %rd118;
	cvt.u32.u64 	%r1, %rd9;
	setp.lt.s32 	%p5, %r1, 1;
	mov.u32 	%r64, 0;
	@%p5 bra 	$L__BB20_30;

	setp.eq.s64 	%p6, %rd5, 0;
	@%p6 bra 	$L__BB20_30;

	add.s64 	%rd23, %rd5, -1;
	and.b64  	%rd24, %rd5, 3;
	sub.s64 	%rd25, %rd5, %rd24;
	shl.b64 	%rd78, %rd22, 2;
	add.s64 	%rd26, %rd2, %rd78;
	add.s64 	%rd27, %rd120, %rd64;
	mov.u32 	%r59, 0;
	mov.u64 	%rd122, 0;
	cvt.u32.u64 	%r34, %rd63;
	mov.u32 	%r64, %r59;

$L__BB20_13:
	cvt.s64.s32 	%rd29, %r59;
	mul.lo.s64 	%rd79, %rd29, %rd65;
	sub.s64 	%rd30, %rd27, %rd79;
	cvt.u32.u64 	%r33, %rd30;
	setp.lt.s32 	%p7, %r33, 0;
	@%p7 bra 	$L__BB20_29;

	cvt.s64.s32 	%rd31, %rd30;
	or.b64  	%rd80, %rd31, %rd63;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p8, %rd81, 0;
	@%p8 bra 	$L__BB20_16;

	rem.u64 	%rd123, %rd31, %rd63;
	bra.uni 	$L__BB20_17;

$L__BB20_16:
	cvt.u32.u64 	%r35, %rd31;
	rem.u32 	%r36, %r35, %r34;
	cvt.u64.u32 	%rd123, %r36;

$L__BB20_17:
	setp.ne.s64 	%p9, %rd123, 0;
	@%p9 bra 	$L__BB20_29;

	@%p8 bra 	$L__BB20_20;

	div.u64 	%rd124, %rd31, %rd63;
	bra.uni 	$L__BB20_21;

$L__BB20_20:
	cvt.u32.u64 	%r38, %rd31;
	div.u32 	%r39, %r38, %r34;
	cvt.u64.u32 	%rd124, %r39;

$L__BB20_21:
	cvt.s64.s32 	%rd38, %rd124;
	setp.ge.u64 	%p11, %rd38, %rd8;
	@%p11 bra 	$L__BB20_29;

	setp.lt.u64 	%p12, %rd23, 3;
	ld.global.u64 	%rd39, [%rd4+32];
	mov.u64 	%rd129, 0;
	ld.global.u64 	%rd85, [%rd4+40];
	mul.lo.s64 	%rd40, %rd85, %rd38;
	ld.global.u64 	%rd41, [%rd4+72];
	ld.global.u64 	%rd86, [%rd4+80];
	mul.lo.s64 	%rd42, %rd86, %rd121;
	ld.global.u64 	%rd43, [%rd4+88];
	@%p12 bra 	$L__BB20_25;

	shl.b64 	%rd88, %rd40, 2;
	add.s64 	%rd127, %rd26, %rd88;
	shl.b64 	%rd89, %rd42, 2;
	add.s64 	%rd90, %rd1, %rd89;
	mul.lo.s64 	%rd91, %rd43, %rd122;
	add.s64 	%rd128, %rd90, %rd91;
	mov.u64 	%rd129, 0;
	mov.u64 	%rd126, %rd25;

$L__BB20_24:
	shl.b64 	%rd117, %rd41, 2;
	shl.b64 	%rd116, %rd39, 2;
	ld.global.u32 	%r41, [%rd128];
	ld.global.u32 	%r42, [%rd127];
	mad.lo.s32 	%r43, %r41, %r42, %r64;
	add.s64 	%rd92, %rd127, %rd116;
	add.s64 	%rd93, %rd128, %rd117;
	ld.global.u32 	%r44, [%rd93];
	ld.global.u32 	%r45, [%rd92];
	mad.lo.s32 	%r46, %r44, %r45, %r43;
	add.s64 	%rd94, %rd92, %rd116;
	add.s64 	%rd95, %rd93, %rd117;
	ld.global.u32 	%r47, [%rd95];
	ld.global.u32 	%r48, [%rd94];
	mad.lo.s32 	%r49, %r47, %r48, %r46;
	add.s64 	%rd96, %rd94, %rd116;
	add.s64 	%rd127, %rd96, %rd116;
	add.s64 	%rd97, %rd95, %rd117;
	add.s64 	%rd128, %rd97, %rd117;
	ld.global.u32 	%r50, [%rd97];
	ld.global.u32 	%r51, [%rd96];
	mad.lo.s32 	%r64, %r50, %r51, %r49;
	add.s64 	%rd129, %rd129, 4;
	add.s64 	%rd126, %rd126, -4;
	setp.ne.s64 	%p13, %rd126, 0;
	@%p13 bra 	$L__BB20_24;

$L__BB20_25:
	setp.eq.s64 	%p14, %rd24, 0;
	@%p14 bra 	$L__BB20_29;

	setp.eq.s64 	%p15, %rd24, 1;
	mul.lo.s64 	%rd98, %rd39, %rd129;
	add.s64 	%rd99, %rd98, %rd22;
	add.s64 	%rd100, %rd99, %rd40;
	mul.lo.s64 	%rd101, %rd41, %rd129;
	add.s64 	%rd102, %rd42, %rd101;
	mul.lo.s64 	%rd103, %rd43, %rd29;
	add.s64 	%rd104, %rd102, %rd103;
	shl.b64 	%rd105, %rd100, 2;
	add.s64 	%rd57, %rd2, %rd105;
	shl.b64 	%rd106, %rd104, 2;
	add.s64 	%rd58, %rd1, %rd106;
	ld.global.u32 	%r52, [%rd58];
	ld.global.u32 	%r53, [%rd57];
	mad.lo.s32 	%r64, %r52, %r53, %r64;
	@%p15 bra 	$L__BB20_29;

	setp.eq.s64 	%p16, %rd24, 2;
	shl.b64 	%rd107, %rd41, 2;
	add.s64 	%rd59, %rd58, %rd107;
	ld.global.u32 	%r54, [%rd59];
	shl.b64 	%rd108, %rd39, 2;
	add.s64 	%rd60, %rd57, %rd108;
	ld.global.u32 	%r55, [%rd60];
	mad.lo.s32 	%r64, %r54, %r55, %r64;
	@%p16 bra 	$L__BB20_29;

	add.s64 	%rd110, %rd60, %rd108;
	add.s64 	%rd112, %rd59, %rd107;
	ld.global.u32 	%r56, [%rd112];
	ld.global.u32 	%r57, [%rd110];
	mad.lo.s32 	%r64, %r56, %r57, %r64;

$L__BB20_29:
	cvt.u32.u64 	%r58, %rd29;
	add.s32 	%r59, %r58, 1;
	setp.lt.s32 	%p17, %r59, %r1;
	add.s64 	%rd122, %rd122, 4;
	@%p17 bra 	$L__BB20_13;

$L__BB20_30:
	cvta.to.global.u64 	%rd113, %rd66;
	shl.b64 	%rd114, %rd3, 2;
	add.s64 	%rd115, %rd113, %rd114;
	st.global.u32 	[%rd115], %r64;

$L__BB20_31:
	ret;

}
	// .globl	conv_transpose2d_f32
.visible .entry conv_transpose2d_f32(
	.param .u64 conv_transpose2d_f32_param_0,
	.param .u64 conv_transpose2d_f32_param_1,
	.param .u64 conv_transpose2d_f32_param_2,
	.param .u64 conv_transpose2d_f32_param_3,
	.param .u64 conv_transpose2d_f32_param_4,
	.param .u64 conv_transpose2d_f32_param_5,
	.param .u64 conv_transpose2d_f32_param_6,
	.param .u64 conv_transpose2d_f32_param_7,
	.param .u64 conv_transpose2d_f32_param_8,
	.param .u64 conv_transpose2d_f32_param_9,
	.param .u64 conv_transpose2d_f32_param_10
)
{
	.reg .pred 	%p<28>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd84, [conv_transpose2d_f32_param_1];
	ld.param.u64 	%rd85, [conv_transpose2d_f32_param_2];
	ld.param.u64 	%rd86, [conv_transpose2d_f32_param_3];
	ld.param.u64 	%rd87, [conv_transpose2d_f32_param_4];
	ld.param.u64 	%rd88, [conv_transpose2d_f32_param_6];
	ld.param.u64 	%rd90, [conv_transpose2d_f32_param_7];
	ld.param.u64 	%rd91, [conv_transpose2d_f32_param_8];
	ld.param.u64 	%rd92, [conv_transpose2d_f32_param_9];
	cvta.to.global.u64 	%rd1, %rd92;
	cvta.to.global.u64 	%rd2, %rd91;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r10, %r8, %r7, %r9;
	cvt.u64.u32 	%rd3, %r10;
	cvta.to.global.u64 	%rd4, %rd90;
	ld.global.u64 	%rd5, [%rd4+8];
	mul.lo.s64 	%rd6, %rd85, %rd84;
	ld.global.u64 	%rd7, [%rd4+72];
	mul.lo.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd93, [%rd4];
	mul.lo.s64 	%rd94, %rd8, %rd93;
	setp.le.u64 	%p1, %rd94, %rd3;
	@%p1 bra 	$L__BB21_48;

	ld.global.u64 	%rd9, [%rd4+80];
	ld.global.u64 	%rd10, [%rd4+16];
	ld.global.u64 	%rd11, [%rd4+24];
	ld.global.u64 	%rd12, [%rd4+88];
	and.b64  	%rd95, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd95, 0;
	@%p2 bra 	$L__BB21_3;

	div.u64 	%rd160, %rd3, %rd8;
	bra.uni 	$L__BB21_4;

$L__BB21_3:
	cvt.u32.u64 	%r11, %rd8;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB21_4:
	and.b64  	%rd96, %rd6, -4294967296;
	setp.eq.s64 	%p3, %rd96, 0;
	@%p3 bra 	$L__BB21_6;

	div.u64 	%rd161, %rd3, %rd6;
	bra.uni 	$L__BB21_7;

$L__BB21_6:
	cvt.u32.u64 	%r14, %rd6;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd161, %r16;

$L__BB21_7:
	and.b64  	%rd97, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd97, 0;
	@%p4 bra 	$L__BB21_9;

	rem.u64 	%rd162, %rd161, %rd7;
	bra.uni 	$L__BB21_10;

$L__BB21_9:
	cvt.u32.u64 	%r17, %rd7;
	cvt.u32.u64 	%r18, %rd161;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd162, %r19;

$L__BB21_10:
	and.b64  	%rd98, %rd84, -4294967296;
	setp.eq.s64 	%p5, %rd98, 0;
	@%p5 bra 	$L__BB21_12;

	div.u64 	%rd163, %rd3, %rd84;
	mul.lo.s64 	%rd99, %rd163, %rd84;
	sub.s64 	%rd164, %rd3, %rd99;
	bra.uni 	$L__BB21_13;

$L__BB21_12:
	cvt.u32.u64 	%r20, %rd84;
	cvt.u32.u64 	%r21, %rd3;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd163, %r22;
	cvt.u64.u32 	%rd164, %r24;

$L__BB21_13:
	and.b64  	%rd100, %rd85, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB21_15;

	rem.u64 	%rd165, %rd163, %rd85;
	bra.uni 	$L__BB21_16;

$L__BB21_15:
	cvt.u32.u64 	%r25, %rd85;
	cvt.u32.u64 	%r26, %rd163;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd165, %r27;

$L__BB21_16:
	ld.global.u64 	%rd101, [%rd4+32];
	mul.lo.s64 	%rd31, %rd101, %rd160;
	cvt.u32.u64 	%r1, %rd12;
	setp.lt.s32 	%p7, %r1, 1;
	mov.f32 	%f40, 0f00000000;
	@%p7 bra 	$L__BB21_47;

	cvt.u32.u64 	%r2, %rd9;
	setp.lt.s32 	%p8, %r2, 1;
	@%p8 bra 	$L__BB21_47;

	add.s64 	%rd32, %rd5, -1;
	and.b64  	%rd33, %rd5, 3;
	sub.s64 	%rd34, %rd33, %rd5;
	shl.b64 	%rd103, %rd31, 2;
	add.s64 	%rd35, %rd2, %rd103;
	add.s64 	%rd36, %rd165, %rd87;
	add.s64 	%rd37, %rd164, %rd87;
	mov.f32 	%f40, 0f00000000;
	mov.u32 	%r50, 0;
	mov.u64 	%rd166, 0;
	cvt.u32.u64 	%r30, %rd86;

$L__BB21_19:
	cvt.s64.s32 	%rd39, %r50;
	mul.lo.s64 	%rd104, %rd39, %rd88;
	sub.s64 	%rd40, %rd37, %rd104;
	cvt.u32.u64 	%r29, %rd40;
	setp.lt.s32 	%p9, %r29, 0;
	@%p9 bra 	$L__BB21_46;

	cvt.s64.s32 	%rd41, %rd40;
	or.b64  	%rd105, %rd41, %rd86;
	and.b64  	%rd106, %rd105, -4294967296;
	setp.eq.s64 	%p10, %rd106, 0;
	@%p10 bra 	$L__BB21_22;

	rem.u64 	%rd167, %rd41, %rd86;
	bra.uni 	$L__BB21_23;

$L__BB21_22:
	cvt.u32.u64 	%r31, %rd41;
	rem.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd167, %r32;

$L__BB21_23:
	setp.ne.s64 	%p11, %rd167, 0;
	@%p11 bra 	$L__BB21_46;

	@%p10 bra 	$L__BB21_26;

	div.u64 	%rd168, %rd41, %rd86;
	bra.uni 	$L__BB21_27;

$L__BB21_26:
	cvt.u32.u64 	%r34, %rd41;
	div.u32 	%r35, %r34, %r30;
	cvt.u64.u32 	%rd168, %r35;

$L__BB21_27:
	cvt.s64.s32 	%rd48, %rd168;
	setp.ge.u64 	%p13, %rd48, %rd11;
	setp.eq.s64 	%p14, %rd5, 0;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB21_46;

	mov.u32 	%r51, 0;
	mov.u64 	%rd169, 0;

$L__BB21_29:
	cvt.s64.s32 	%rd50, %r51;
	mul.lo.s64 	%rd110, %rd50, %rd88;
	sub.s64 	%rd51, %rd36, %rd110;
	cvt.u32.u64 	%r37, %rd51;
	setp.lt.s32 	%p16, %r37, 0;
	@%p16 bra 	$L__BB21_45;

	cvt.s64.s32 	%rd52, %rd51;
	or.b64  	%rd111, %rd52, %rd86;
	and.b64  	%rd112, %rd111, -4294967296;
	setp.eq.s64 	%p17, %rd112, 0;
	@%p17 bra 	$L__BB21_32;

	rem.u64 	%rd170, %rd52, %rd86;
	bra.uni 	$L__BB21_33;

$L__BB21_32:
	cvt.u32.u64 	%r39, %rd52;
	rem.u32 	%r40, %r39, %r30;
	cvt.u64.u32 	%rd170, %r40;

$L__BB21_33:
	setp.ne.s64 	%p18, %rd170, 0;
	@%p18 bra 	$L__BB21_45;

	@%p17 bra 	$L__BB21_36;

	div.u64 	%rd171, %rd52, %rd86;
	bra.uni 	$L__BB21_37;

$L__BB21_36:
	cvt.u32.u64 	%r42, %rd52;
	div.u32 	%r43, %r42, %r30;
	cvt.u64.u32 	%rd171, %r43;

$L__BB21_37:
	cvt.s64.s32 	%rd59, %rd171;
	setp.ge.u64 	%p20, %rd59, %rd10;
	@%p20 bra 	$L__BB21_45;

	setp.lt.u64 	%p21, %rd32, 3;
	ld.global.u64 	%rd60, [%rd4+40];
	mov.u64 	%rd175, 0;
	ld.global.u64 	%rd116, [%rd4+48];
	mul.lo.s64 	%rd61, %rd116, %rd59;
	ld.global.u64 	%rd117, [%rd4+56];
	mul.lo.s64 	%rd62, %rd117, %rd48;
	ld.global.u64 	%rd63, [%rd4+96];
	ld.global.u64 	%rd118, [%rd4+104];
	mul.lo.s64 	%rd64, %rd118, %rd162;
	ld.global.u64 	%rd65, [%rd4+112];
	ld.global.u64 	%rd66, [%rd4+120];
	@%p21 bra 	$L__BB21_41;

	add.s64 	%rd120, %rd62, %rd61;
	shl.b64 	%rd121, %rd120, 2;
	add.s64 	%rd173, %rd35, %rd121;
	shl.b64 	%rd122, %rd64, 2;
	add.s64 	%rd123, %rd1, %rd122;
	mul.lo.s64 	%rd124, %rd169, %rd65;
	shl.b64 	%rd125, %rd124, 2;
	mul.lo.s64 	%rd126, %rd166, %rd66;
	add.s64 	%rd127, %rd126, %rd125;
	add.s64 	%rd174, %rd123, %rd127;
	mov.u64 	%rd175, 0;

$L__BB21_40:
	shl.b64 	%rd157, %rd63, 2;
	shl.b64 	%rd156, %rd60, 2;
	ld.global.f32 	%f17, [%rd174];
	ld.global.f32 	%f18, [%rd173];
	fma.rn.f32 	%f19, %f18, %f17, %f40;
	add.s64 	%rd128, %rd173, %rd156;
	add.s64 	%rd129, %rd174, %rd157;
	ld.global.f32 	%f20, [%rd129];
	ld.global.f32 	%f21, [%rd128];
	fma.rn.f32 	%f22, %f21, %f20, %f19;
	add.s64 	%rd130, %rd128, %rd156;
	add.s64 	%rd131, %rd129, %rd157;
	ld.global.f32 	%f23, [%rd131];
	ld.global.f32 	%f24, [%rd130];
	fma.rn.f32 	%f25, %f24, %f23, %f22;
	add.s64 	%rd132, %rd130, %rd156;
	add.s64 	%rd173, %rd132, %rd156;
	add.s64 	%rd133, %rd131, %rd157;
	add.s64 	%rd174, %rd133, %rd157;
	ld.global.f32 	%f26, [%rd133];
	ld.global.f32 	%f27, [%rd132];
	fma.rn.f32 	%f40, %f27, %f26, %f25;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd134, %rd34, %rd175;
	setp.ne.s64 	%p22, %rd134, 0;
	@%p22 bra 	$L__BB21_40;

$L__BB21_41:
	setp.eq.s64 	%p23, %rd33, 0;
	@%p23 bra 	$L__BB21_45;

	setp.eq.s64 	%p24, %rd33, 1;
	mul.lo.s64 	%rd135, %rd60, %rd175;
	add.s64 	%rd136, %rd135, %rd31;
	add.s64 	%rd137, %rd136, %rd61;
	add.s64 	%rd138, %rd137, %rd62;
	mul.lo.s64 	%rd139, %rd63, %rd175;
	add.s64 	%rd140, %rd64, %rd139;
	mul.lo.s64 	%rd141, %rd65, %rd50;
	add.s64 	%rd142, %rd140, %rd141;
	mul.lo.s64 	%rd143, %rd66, %rd39;
	add.s64 	%rd144, %rd142, %rd143;
	shl.b64 	%rd145, %rd138, 2;
	add.s64 	%rd78, %rd2, %rd145;
	shl.b64 	%rd146, %rd144, 2;
	add.s64 	%rd79, %rd1, %rd146;
	ld.global.f32 	%f28, [%rd79];
	ld.global.f32 	%f29, [%rd78];
	fma.rn.f32 	%f40, %f29, %f28, %f40;
	@%p24 bra 	$L__BB21_45;

	setp.eq.s64 	%p25, %rd33, 2;
	shl.b64 	%rd147, %rd63, 2;
	add.s64 	%rd80, %rd79, %rd147;
	ld.global.f32 	%f30, [%rd80];
	shl.b64 	%rd148, %rd60, 2;
	add.s64 	%rd81, %rd78, %rd148;
	ld.global.f32 	%f31, [%rd81];
	fma.rn.f32 	%f40, %f31, %f30, %f40;
	@%p25 bra 	$L__BB21_45;

	add.s64 	%rd150, %rd81, %rd148;
	add.s64 	%rd152, %rd80, %rd147;
	ld.global.f32 	%f32, [%rd152];
	ld.global.f32 	%f33, [%rd150];
	fma.rn.f32 	%f40, %f33, %f32, %f40;

$L__BB21_45:
	cvt.u32.u64 	%r44, %rd50;
	add.s32 	%r51, %r44, 1;
	setp.lt.s32 	%p26, %r51, %r2;
	add.s64 	%rd169, %rd169, 1;
	@%p26 bra 	$L__BB21_29;

$L__BB21_46:
	cvt.u32.u64 	%r45, %rd39;
	add.s32 	%r50, %r45, 1;
	setp.lt.s32 	%p27, %r50, %r1;
	add.s64 	%rd166, %rd166, 4;
	@%p27 bra 	$L__BB21_19;

$L__BB21_47:
	ld.param.u64 	%rd159, [conv_transpose2d_f32_param_10];
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r47, %ctaid.x;
	mad.lo.s32 	%r46, %r47, %r48, %r49;
	cvt.u64.u32 	%rd158, %r46;
	cvta.to.global.u64 	%rd153, %rd159;
	shl.b64 	%rd154, %rd158, 2;
	add.s64 	%rd155, %rd153, %rd154;
	st.global.f32 	[%rd155], %f40;

$L__BB21_48:
	ret;

}
	// .globl	conv_transpose2d_f64
.visible .entry conv_transpose2d_f64(
	.param .u64 conv_transpose2d_f64_param_0,
	.param .u64 conv_transpose2d_f64_param_1,
	.param .u64 conv_transpose2d_f64_param_2,
	.param .u64 conv_transpose2d_f64_param_3,
	.param .u64 conv_transpose2d_f64_param_4,
	.param .u64 conv_transpose2d_f64_param_5,
	.param .u64 conv_transpose2d_f64_param_6,
	.param .u64 conv_transpose2d_f64_param_7,
	.param .u64 conv_transpose2d_f64_param_8,
	.param .u64 conv_transpose2d_f64_param_9,
	.param .u64 conv_transpose2d_f64_param_10
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<52>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd84, [conv_transpose2d_f64_param_1];
	ld.param.u64 	%rd85, [conv_transpose2d_f64_param_2];
	ld.param.u64 	%rd86, [conv_transpose2d_f64_param_3];
	ld.param.u64 	%rd87, [conv_transpose2d_f64_param_4];
	ld.param.u64 	%rd88, [conv_transpose2d_f64_param_6];
	ld.param.u64 	%rd90, [conv_transpose2d_f64_param_7];
	ld.param.u64 	%rd91, [conv_transpose2d_f64_param_8];
	ld.param.u64 	%rd92, [conv_transpose2d_f64_param_9];
	cvta.to.global.u64 	%rd1, %rd92;
	cvta.to.global.u64 	%rd2, %rd91;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r10, %r8, %r7, %r9;
	cvt.u64.u32 	%rd3, %r10;
	cvta.to.global.u64 	%rd4, %rd90;
	ld.global.u64 	%rd5, [%rd4+8];
	mul.lo.s64 	%rd6, %rd85, %rd84;
	ld.global.u64 	%rd7, [%rd4+72];
	mul.lo.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd93, [%rd4];
	mul.lo.s64 	%rd94, %rd8, %rd93;
	setp.le.u64 	%p1, %rd94, %rd3;
	@%p1 bra 	$L__BB22_48;

	ld.global.u64 	%rd9, [%rd4+80];
	ld.global.u64 	%rd10, [%rd4+16];
	ld.global.u64 	%rd11, [%rd4+24];
	ld.global.u64 	%rd12, [%rd4+88];
	and.b64  	%rd95, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd95, 0;
	@%p2 bra 	$L__BB22_3;

	div.u64 	%rd160, %rd3, %rd8;
	bra.uni 	$L__BB22_4;

$L__BB22_3:
	cvt.u32.u64 	%r11, %rd8;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd160, %r13;

$L__BB22_4:
	and.b64  	%rd96, %rd6, -4294967296;
	setp.eq.s64 	%p3, %rd96, 0;
	@%p3 bra 	$L__BB22_6;

	div.u64 	%rd161, %rd3, %rd6;
	bra.uni 	$L__BB22_7;

$L__BB22_6:
	cvt.u32.u64 	%r14, %rd6;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd161, %r16;

$L__BB22_7:
	and.b64  	%rd97, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd97, 0;
	@%p4 bra 	$L__BB22_9;

	rem.u64 	%rd162, %rd161, %rd7;
	bra.uni 	$L__BB22_10;

$L__BB22_9:
	cvt.u32.u64 	%r17, %rd7;
	cvt.u32.u64 	%r18, %rd161;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd162, %r19;

$L__BB22_10:
	and.b64  	%rd98, %rd84, -4294967296;
	setp.eq.s64 	%p5, %rd98, 0;
	@%p5 bra 	$L__BB22_12;

	div.u64 	%rd163, %rd3, %rd84;
	mul.lo.s64 	%rd99, %rd163, %rd84;
	sub.s64 	%rd164, %rd3, %rd99;
	bra.uni 	$L__BB22_13;

$L__BB22_12:
	cvt.u32.u64 	%r20, %rd84;
	cvt.u32.u64 	%r21, %rd3;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd163, %r22;
	cvt.u64.u32 	%rd164, %r24;

$L__BB22_13:
	and.b64  	%rd100, %rd85, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB22_15;

	rem.u64 	%rd165, %rd163, %rd85;
	bra.uni 	$L__BB22_16;

$L__BB22_15:
	cvt.u32.u64 	%r25, %rd85;
	cvt.u32.u64 	%r26, %rd163;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd165, %r27;

$L__BB22_16:
	ld.global.u64 	%rd101, [%rd4+32];
	mul.lo.s64 	%rd31, %rd101, %rd160;
	cvt.u32.u64 	%r1, %rd12;
	setp.lt.s32 	%p7, %r1, 1;
	mov.f64 	%fd40, 0d0000000000000000;
	@%p7 bra 	$L__BB22_47;

	cvt.u32.u64 	%r2, %rd9;
	setp.lt.s32 	%p8, %r2, 1;
	@%p8 bra 	$L__BB22_47;

	add.s64 	%rd32, %rd5, -1;
	and.b64  	%rd33, %rd5, 3;
	sub.s64 	%rd34, %rd33, %rd5;
	shl.b64 	%rd103, %rd31, 3;
	add.s64 	%rd35, %rd2, %rd103;
	add.s64 	%rd36, %rd165, %rd87;
	add.s64 	%rd37, %rd164, %rd87;
	mov.f64 	%fd40, 0d0000000000000000;
	mov.u32 	%r50, 0;
	mov.u64 	%rd166, 0;
	cvt.u32.u64 	%r30, %rd86;

$L__BB22_19:
	cvt.s64.s32 	%rd39, %r50;
	mul.lo.s64 	%rd104, %rd39, %rd88;
	sub.s64 	%rd40, %rd37, %rd104;
	cvt.u32.u64 	%r29, %rd40;
	setp.lt.s32 	%p9, %r29, 0;
	@%p9 bra 	$L__BB22_46;

	cvt.s64.s32 	%rd41, %rd40;
	or.b64  	%rd105, %rd41, %rd86;
	and.b64  	%rd106, %rd105, -4294967296;
	setp.eq.s64 	%p10, %rd106, 0;
	@%p10 bra 	$L__BB22_22;

	rem.u64 	%rd167, %rd41, %rd86;
	bra.uni 	$L__BB22_23;

$L__BB22_22:
	cvt.u32.u64 	%r31, %rd41;
	rem.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd167, %r32;

$L__BB22_23:
	setp.ne.s64 	%p11, %rd167, 0;
	@%p11 bra 	$L__BB22_46;

	@%p10 bra 	$L__BB22_26;

	div.u64 	%rd168, %rd41, %rd86;
	bra.uni 	$L__BB22_27;

$L__BB22_26:
	cvt.u32.u64 	%r34, %rd41;
	div.u32 	%r35, %r34, %r30;
	cvt.u64.u32 	%rd168, %r35;

$L__BB22_27:
	cvt.s64.s32 	%rd48, %rd168;
	setp.ge.u64 	%p13, %rd48, %rd11;
	setp.eq.s64 	%p14, %rd5, 0;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB22_46;

	mov.u32 	%r51, 0;
	mov.u64 	%rd169, 0;

$L__BB22_29:
	cvt.s64.s32 	%rd50, %r51;
	mul.lo.s64 	%rd110, %rd50, %rd88;
	sub.s64 	%rd51, %rd36, %rd110;
	cvt.u32.u64 	%r37, %rd51;
	setp.lt.s32 	%p16, %r37, 0;
	@%p16 bra 	$L__BB22_45;

	cvt.s64.s32 	%rd52, %rd51;
	or.b64  	%rd111, %rd52, %rd86;
	and.b64  	%rd112, %rd111, -4294967296;
	setp.eq.s64 	%p17, %rd112, 0;
	@%p17 bra 	$L__BB22_32;

	rem.u64 	%rd170, %rd52, %rd86;
	bra.uni 	$L__BB22_33;

$L__BB22_32:
	cvt.u32.u64 	%r39, %rd52;
	rem.u32 	%r40, %r39, %r30;
	cvt.u64.u32 	%rd170, %r40;

$L__BB22_33:
	setp.ne.s64 	%p18, %rd170, 0;
	@%p18 bra 	$L__BB22_45;

	@%p17 bra 	$L__BB22_36;

	div.u64 	%rd171, %rd52, %rd86;
	bra.uni 	$L__BB22_37;

$L__BB22_36:
	cvt.u32.u64 	%r42, %rd52;
	div.u32 	%r43, %r42, %r30;
	cvt.u64.u32 	%rd171, %r43;

$L__BB22_37:
	cvt.s64.s32 	%rd59, %rd171;
	setp.ge.u64 	%p20, %rd59, %rd10;
	@%p20 bra 	$L__BB22_45;

	setp.lt.u64 	%p21, %rd32, 3;
	ld.global.u64 	%rd60, [%rd4+40];
	mov.u64 	%rd175, 0;
	ld.global.u64 	%rd116, [%rd4+48];
	mul.lo.s64 	%rd61, %rd116, %rd59;
	ld.global.u64 	%rd117, [%rd4+56];
	mul.lo.s64 	%rd62, %rd117, %rd48;
	ld.global.u64 	%rd63, [%rd4+96];
	ld.global.u64 	%rd118, [%rd4+104];
	mul.lo.s64 	%rd64, %rd118, %rd162;
	ld.global.u64 	%rd65, [%rd4+112];
	ld.global.u64 	%rd66, [%rd4+120];
	@%p21 bra 	$L__BB22_41;

	add.s64 	%rd120, %rd62, %rd61;
	shl.b64 	%rd121, %rd120, 3;
	add.s64 	%rd173, %rd35, %rd121;
	shl.b64 	%rd122, %rd64, 3;
	add.s64 	%rd123, %rd1, %rd122;
	mul.lo.s64 	%rd124, %rd169, %rd65;
	shl.b64 	%rd125, %rd124, 3;
	mul.lo.s64 	%rd126, %rd166, %rd66;
	add.s64 	%rd127, %rd126, %rd125;
	add.s64 	%rd174, %rd123, %rd127;
	mov.u64 	%rd175, 0;

$L__BB22_40:
	shl.b64 	%rd157, %rd63, 3;
	shl.b64 	%rd156, %rd60, 3;
	ld.global.f64 	%fd17, [%rd174];
	ld.global.f64 	%fd18, [%rd173];
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd40;
	add.s64 	%rd128, %rd173, %rd156;
	add.s64 	%rd129, %rd174, %rd157;
	ld.global.f64 	%fd20, [%rd129];
	ld.global.f64 	%fd21, [%rd128];
	fma.rn.f64 	%fd22, %fd21, %fd20, %fd19;
	add.s64 	%rd130, %rd128, %rd156;
	add.s64 	%rd131, %rd129, %rd157;
	ld.global.f64 	%fd23, [%rd131];
	ld.global.f64 	%fd24, [%rd130];
	fma.rn.f64 	%fd25, %fd24, %fd23, %fd22;
	add.s64 	%rd132, %rd130, %rd156;
	add.s64 	%rd173, %rd132, %rd156;
	add.s64 	%rd133, %rd131, %rd157;
	add.s64 	%rd174, %rd133, %rd157;
	ld.global.f64 	%fd26, [%rd133];
	ld.global.f64 	%fd27, [%rd132];
	fma.rn.f64 	%fd40, %fd27, %fd26, %fd25;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd134, %rd34, %rd175;
	setp.ne.s64 	%p22, %rd134, 0;
	@%p22 bra 	$L__BB22_40;

$L__BB22_41:
	setp.eq.s64 	%p23, %rd33, 0;
	@%p23 bra 	$L__BB22_45;

	setp.eq.s64 	%p24, %rd33, 1;
	mul.lo.s64 	%rd135, %rd60, %rd175;
	add.s64 	%rd136, %rd135, %rd31;
	add.s64 	%rd137, %rd136, %rd61;
	add.s64 	%rd138, %rd137, %rd62;
	mul.lo.s64 	%rd139, %rd63, %rd175;
	add.s64 	%rd140, %rd64, %rd139;
	mul.lo.s64 	%rd141, %rd65, %rd50;
	add.s64 	%rd142, %rd140, %rd141;
	mul.lo.s64 	%rd143, %rd66, %rd39;
	add.s64 	%rd144, %rd142, %rd143;
	shl.b64 	%rd145, %rd138, 3;
	add.s64 	%rd78, %rd2, %rd145;
	shl.b64 	%rd146, %rd144, 3;
	add.s64 	%rd79, %rd1, %rd146;
	ld.global.f64 	%fd28, [%rd79];
	ld.global.f64 	%fd29, [%rd78];
	fma.rn.f64 	%fd40, %fd29, %fd28, %fd40;
	@%p24 bra 	$L__BB22_45;

	setp.eq.s64 	%p25, %rd33, 2;
	shl.b64 	%rd147, %rd63, 3;
	add.s64 	%rd80, %rd79, %rd147;
	ld.global.f64 	%fd30, [%rd80];
	shl.b64 	%rd148, %rd60, 3;
	add.s64 	%rd81, %rd78, %rd148;
	ld.global.f64 	%fd31, [%rd81];
	fma.rn.f64 	%fd40, %fd31, %fd30, %fd40;
	@%p25 bra 	$L__BB22_45;

	add.s64 	%rd150, %rd81, %rd148;
	add.s64 	%rd152, %rd80, %rd147;
	ld.global.f64 	%fd32, [%rd152];
	ld.global.f64 	%fd33, [%rd150];
	fma.rn.f64 	%fd40, %fd33, %fd32, %fd40;

$L__BB22_45:
	cvt.u32.u64 	%r44, %rd50;
	add.s32 	%r51, %r44, 1;
	setp.lt.s32 	%p26, %r51, %r2;
	add.s64 	%rd169, %rd169, 1;
	@%p26 bra 	$L__BB22_29;

$L__BB22_46:
	cvt.u32.u64 	%r45, %rd39;
	add.s32 	%r50, %r45, 1;
	setp.lt.s32 	%p27, %r50, %r1;
	add.s64 	%rd166, %rd166, 8;
	@%p27 bra 	$L__BB22_19;

$L__BB22_47:
	ld.param.u64 	%rd159, [conv_transpose2d_f64_param_10];
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r47, %ctaid.x;
	mad.lo.s32 	%r46, %r47, %r48, %r49;
	cvt.u64.u32 	%rd158, %r46;
	cvta.to.global.u64 	%rd153, %rd159;
	shl.b64 	%rd154, %rd158, 3;
	add.s64 	%rd155, %rd153, %rd154;
	st.global.f64 	[%rd155], %fd40;

$L__BB22_48:
	ret;

}
	// .globl	conv_transpose2d_u8
.visible .entry conv_transpose2d_u8(
	.param .u64 conv_transpose2d_u8_param_0,
	.param .u64 conv_transpose2d_u8_param_1,
	.param .u64 conv_transpose2d_u8_param_2,
	.param .u64 conv_transpose2d_u8_param_3,
	.param .u64 conv_transpose2d_u8_param_4,
	.param .u64 conv_transpose2d_u8_param_5,
	.param .u64 conv_transpose2d_u8_param_6,
	.param .u64 conv_transpose2d_u8_param_7,
	.param .u64 conv_transpose2d_u8_param_8,
	.param .u64 conv_transpose2d_u8_param_9,
	.param .u64 conv_transpose2d_u8_param_10
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<151>;


	ld.param.u64 	%rd78, [conv_transpose2d_u8_param_1];
	ld.param.u64 	%rd79, [conv_transpose2d_u8_param_2];
	ld.param.u64 	%rd80, [conv_transpose2d_u8_param_3];
	ld.param.u64 	%rd81, [conv_transpose2d_u8_param_4];
	ld.param.u64 	%rd82, [conv_transpose2d_u8_param_6];
	ld.param.u64 	%rd84, [conv_transpose2d_u8_param_7];
	ld.param.u64 	%rd85, [conv_transpose2d_u8_param_8];
	ld.param.u64 	%rd86, [conv_transpose2d_u8_param_9];
	cvta.to.global.u64 	%rd1, %rd86;
	cvta.to.global.u64 	%rd2, %rd85;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r10, %r8, %r7, %r9;
	cvt.u64.u32 	%rd3, %r10;
	cvta.to.global.u64 	%rd4, %rd84;
	ld.global.u64 	%rd5, [%rd4+8];
	mul.lo.s64 	%rd6, %rd79, %rd78;
	ld.global.u64 	%rd7, [%rd4+72];
	mul.lo.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd87, [%rd4];
	mul.lo.s64 	%rd88, %rd8, %rd87;
	setp.le.u64 	%p1, %rd88, %rd3;
	@%p1 bra 	$L__BB23_48;

	ld.global.u64 	%rd9, [%rd4+80];
	ld.global.u64 	%rd10, [%rd4+16];
	ld.global.u64 	%rd11, [%rd4+24];
	ld.global.u64 	%rd12, [%rd4+88];
	and.b64  	%rd89, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd89, 0;
	@%p2 bra 	$L__BB23_3;

	div.u64 	%rd137, %rd3, %rd8;
	bra.uni 	$L__BB23_4;

$L__BB23_3:
	cvt.u32.u64 	%r11, %rd8;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd137, %r13;

$L__BB23_4:
	and.b64  	%rd90, %rd6, -4294967296;
	setp.eq.s64 	%p3, %rd90, 0;
	@%p3 bra 	$L__BB23_6;

	div.u64 	%rd138, %rd3, %rd6;
	bra.uni 	$L__BB23_7;

$L__BB23_6:
	cvt.u32.u64 	%r14, %rd6;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd138, %r16;

$L__BB23_7:
	and.b64  	%rd91, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd91, 0;
	@%p4 bra 	$L__BB23_9;

	rem.u64 	%rd139, %rd138, %rd7;
	bra.uni 	$L__BB23_10;

$L__BB23_9:
	cvt.u32.u64 	%r17, %rd7;
	cvt.u32.u64 	%r18, %rd138;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd139, %r19;

$L__BB23_10:
	and.b64  	%rd92, %rd78, -4294967296;
	setp.eq.s64 	%p5, %rd92, 0;
	@%p5 bra 	$L__BB23_12;

	div.u64 	%rd140, %rd3, %rd78;
	mul.lo.s64 	%rd93, %rd140, %rd78;
	sub.s64 	%rd141, %rd3, %rd93;
	bra.uni 	$L__BB23_13;

$L__BB23_12:
	cvt.u32.u64 	%r20, %rd78;
	cvt.u32.u64 	%r21, %rd3;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd140, %r22;
	cvt.u64.u32 	%rd141, %r24;

$L__BB23_13:
	and.b64  	%rd94, %rd79, -4294967296;
	setp.eq.s64 	%p6, %rd94, 0;
	@%p6 bra 	$L__BB23_15;

	rem.u64 	%rd142, %rd140, %rd79;
	bra.uni 	$L__BB23_16;

$L__BB23_15:
	cvt.u32.u64 	%r25, %rd79;
	cvt.u32.u64 	%r26, %rd140;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd142, %r27;

$L__BB23_16:
	ld.global.u64 	%rd95, [%rd4+32];
	mul.lo.s64 	%rd31, %rd95, %rd137;
	cvt.u32.u64 	%r1, %rd12;
	setp.lt.s32 	%p7, %r1, 1;
	mov.u16 	%rs46, 0;
	@%p7 bra 	$L__BB23_47;

	cvt.u32.u64 	%r2, %rd9;
	setp.lt.s32 	%p8, %r2, 1;
	@%p8 bra 	$L__BB23_47;

	add.s64 	%rd32, %rd5, -1;
	and.b64  	%rd33, %rd5, 3;
	sub.s64 	%rd34, %rd33, %rd5;
	add.s64 	%rd35, %rd2, %rd31;
	add.s64 	%rd36, %rd142, %rd81;
	add.s64 	%rd37, %rd141, %rd81;
	mov.u16 	%rs46, 0;
	mov.u32 	%r50, 0;
	cvt.u32.u64 	%r30, %rd80;

$L__BB23_19:
	cvt.s64.s32 	%rd38, %r50;
	mul.lo.s64 	%rd96, %rd38, %rd82;
	sub.s64 	%rd39, %rd37, %rd96;
	cvt.u32.u64 	%r29, %rd39;
	setp.lt.s32 	%p9, %r29, 0;
	@%p9 bra 	$L__BB23_46;

	cvt.s64.s32 	%rd40, %rd39;
	or.b64  	%rd97, %rd40, %rd80;
	and.b64  	%rd98, %rd97, -4294967296;
	setp.eq.s64 	%p10, %rd98, 0;
	@%p10 bra 	$L__BB23_22;

	rem.u64 	%rd143, %rd40, %rd80;
	bra.uni 	$L__BB23_23;

$L__BB23_22:
	cvt.u32.u64 	%r31, %rd40;
	rem.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd143, %r32;

$L__BB23_23:
	setp.ne.s64 	%p11, %rd143, 0;
	@%p11 bra 	$L__BB23_46;

	@%p10 bra 	$L__BB23_26;

	div.u64 	%rd144, %rd40, %rd80;
	bra.uni 	$L__BB23_27;

$L__BB23_26:
	cvt.u32.u64 	%r34, %rd40;
	div.u32 	%r35, %r34, %r30;
	cvt.u64.u32 	%rd144, %r35;

$L__BB23_27:
	cvt.s64.s32 	%rd47, %rd144;
	setp.ge.u64 	%p13, %rd47, %rd11;
	setp.eq.s64 	%p14, %rd5, 0;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB23_46;

	mov.u32 	%r51, 0;

$L__BB23_29:
	cvt.s64.s32 	%rd48, %r51;
	mul.lo.s64 	%rd101, %rd48, %rd82;
	sub.s64 	%rd49, %rd36, %rd101;
	cvt.u32.u64 	%r37, %rd49;
	setp.lt.s32 	%p16, %r37, 0;
	@%p16 bra 	$L__BB23_45;

	cvt.s64.s32 	%rd50, %rd49;
	or.b64  	%rd102, %rd50, %rd80;
	and.b64  	%rd103, %rd102, -4294967296;
	setp.eq.s64 	%p17, %rd103, 0;
	@%p17 bra 	$L__BB23_32;

	rem.u64 	%rd145, %rd50, %rd80;
	bra.uni 	$L__BB23_33;

$L__BB23_32:
	cvt.u32.u64 	%r39, %rd50;
	rem.u32 	%r40, %r39, %r30;
	cvt.u64.u32 	%rd145, %r40;

$L__BB23_33:
	setp.ne.s64 	%p18, %rd145, 0;
	@%p18 bra 	$L__BB23_45;

	@%p17 bra 	$L__BB23_36;

	div.u64 	%rd146, %rd50, %rd80;
	bra.uni 	$L__BB23_37;

$L__BB23_36:
	cvt.u32.u64 	%r42, %rd50;
	div.u32 	%r43, %r42, %r30;
	cvt.u64.u32 	%rd146, %r43;

$L__BB23_37:
	cvt.s64.s32 	%rd57, %rd146;
	setp.ge.u64 	%p20, %rd57, %rd10;
	@%p20 bra 	$L__BB23_45;

	setp.lt.u64 	%p21, %rd32, 3;
	ld.global.u64 	%rd58, [%rd4+40];
	mov.u64 	%rd150, 0;
	ld.global.u64 	%rd107, [%rd4+48];
	mul.lo.s64 	%rd59, %rd107, %rd57;
	ld.global.u64 	%rd108, [%rd4+56];
	mul.lo.s64 	%rd60, %rd108, %rd47;
	ld.global.u64 	%rd61, [%rd4+96];
	ld.global.u64 	%rd109, [%rd4+104];
	mul.lo.s64 	%rd62, %rd109, %rd139;
	ld.global.u64 	%rd110, [%rd4+112];
	mul.lo.s64 	%rd63, %rd110, %rd48;
	ld.global.u64 	%rd111, [%rd4+120];
	mul.lo.s64 	%rd64, %rd111, %rd38;
	@%p21 bra 	$L__BB23_41;

	add.s64 	%rd113, %rd60, %rd59;
	add.s64 	%rd148, %rd35, %rd113;
	add.s64 	%rd114, %rd62, %rd64;
	add.s64 	%rd115, %rd114, %rd63;
	add.s64 	%rd149, %rd1, %rd115;
	mov.u64 	%rd150, 0;

$L__BB23_40:
	ld.global.u8 	%rs17, [%rd149];
	ld.global.u8 	%rs18, [%rd148];
	mul.lo.s16 	%rs19, %rs17, %rs18;
	add.s16 	%rs20, %rs19, %rs46;
	add.s64 	%rd116, %rd149, %rd61;
	ld.global.u8 	%rs21, [%rd116];
	add.s64 	%rd117, %rd148, %rd58;
	ld.global.u8 	%rs22, [%rd117];
	mul.lo.s16 	%rs23, %rs21, %rs22;
	add.s16 	%rs24, %rs23, %rs20;
	add.s64 	%rd118, %rd117, %rd58;
	add.s64 	%rd119, %rd116, %rd61;
	ld.global.u8 	%rs25, [%rd119];
	ld.global.u8 	%rs26, [%rd118];
	mul.lo.s16 	%rs27, %rs25, %rs26;
	add.s16 	%rs28, %rs27, %rs24;
	add.s64 	%rd120, %rd118, %rd58;
	add.s64 	%rd148, %rd120, %rd58;
	add.s64 	%rd121, %rd119, %rd61;
	add.s64 	%rd149, %rd121, %rd61;
	ld.global.u8 	%rs29, [%rd121];
	ld.global.u8 	%rs30, [%rd120];
	mul.lo.s16 	%rs31, %rs29, %rs30;
	add.s16 	%rs46, %rs31, %rs28;
	add.s64 	%rd150, %rd150, 4;
	add.s64 	%rd122, %rd34, %rd150;
	setp.ne.s64 	%p22, %rd122, 0;
	@%p22 bra 	$L__BB23_40;

$L__BB23_41:
	setp.eq.s64 	%p23, %rd33, 0;
	@%p23 bra 	$L__BB23_45;

	setp.eq.s64 	%p24, %rd33, 1;
	mul.lo.s64 	%rd123, %rd58, %rd150;
	add.s64 	%rd124, %rd123, %rd31;
	add.s64 	%rd125, %rd124, %rd59;
	add.s64 	%rd126, %rd125, %rd60;
	mul.lo.s64 	%rd127, %rd61, %rd150;
	add.s64 	%rd128, %rd62, %rd127;
	add.s64 	%rd129, %rd128, %rd63;
	add.s64 	%rd130, %rd129, %rd64;
	add.s64 	%rd74, %rd2, %rd126;
	add.s64 	%rd75, %rd1, %rd130;
	ld.global.u8 	%rs32, [%rd75];
	ld.global.u8 	%rs33, [%rd74];
	mul.lo.s16 	%rs34, %rs32, %rs33;
	add.s16 	%rs46, %rs34, %rs46;
	@%p24 bra 	$L__BB23_45;

	setp.eq.s64 	%p25, %rd33, 2;
	add.s64 	%rd76, %rd75, %rd61;
	ld.global.u8 	%rs35, [%rd76];
	add.s64 	%rd77, %rd74, %rd58;
	ld.global.u8 	%rs36, [%rd77];
	mul.lo.s16 	%rs37, %rs35, %rs36;
	add.s16 	%rs46, %rs37, %rs46;
	@%p25 bra 	$L__BB23_45;

	add.s64 	%rd131, %rd77, %rd58;
	add.s64 	%rd132, %rd76, %rd61;
	ld.global.u8 	%rs38, [%rd132];
	ld.global.u8 	%rs39, [%rd131];
	mul.lo.s16 	%rs40, %rs38, %rs39;
	add.s16 	%rs46, %rs40, %rs46;

$L__BB23_45:
	cvt.u32.u64 	%r44, %rd48;
	add.s32 	%r51, %r44, 1;
	setp.lt.s32 	%p26, %r51, %r2;
	@%p26 bra 	$L__BB23_29;

$L__BB23_46:
	cvt.u32.u64 	%r45, %rd38;
	add.s32 	%r50, %r45, 1;
	setp.lt.s32 	%p27, %r50, %r1;
	@%p27 bra 	$L__BB23_19;

$L__BB23_47:
	ld.param.u64 	%rd136, [conv_transpose2d_u8_param_10];
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r47, %ctaid.x;
	mad.lo.s32 	%r46, %r47, %r48, %r49;
	cvt.u64.u32 	%rd135, %r46;
	cvta.to.global.u64 	%rd133, %rd136;
	add.s64 	%rd134, %rd133, %rd135;
	st.global.u8 	[%rd134], %rs46;

$L__BB23_48:
	ret;

}
	// .globl	conv_transpose2d_u32
.visible .entry conv_transpose2d_u32(
	.param .u64 conv_transpose2d_u32_param_0,
	.param .u64 conv_transpose2d_u32_param_1,
	.param .u64 conv_transpose2d_u32_param_2,
	.param .u64 conv_transpose2d_u32_param_3,
	.param .u64 conv_transpose2d_u32_param_4,
	.param .u64 conv_transpose2d_u32_param_5,
	.param .u64 conv_transpose2d_u32_param_6,
	.param .u64 conv_transpose2d_u32_param_7,
	.param .u64 conv_transpose2d_u32_param_8,
	.param .u64 conv_transpose2d_u32_param_9,
	.param .u64 conv_transpose2d_u32_param_10
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<93>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd84, [conv_transpose2d_u32_param_1];
	ld.param.u64 	%rd85, [conv_transpose2d_u32_param_2];
	ld.param.u64 	%rd86, [conv_transpose2d_u32_param_3];
	ld.param.u64 	%rd87, [conv_transpose2d_u32_param_4];
	ld.param.u64 	%rd88, [conv_transpose2d_u32_param_6];
	ld.param.u64 	%rd90, [conv_transpose2d_u32_param_7];
	ld.param.u64 	%rd91, [conv_transpose2d_u32_param_8];
	ld.param.u64 	%rd92, [conv_transpose2d_u32_param_9];
	cvta.to.global.u64 	%rd1, %rd92;
	cvta.to.global.u64 	%rd2, %rd91;
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r22, %r20, %r19, %r21;
	cvt.u64.u32 	%rd3, %r22;
	cvta.to.global.u64 	%rd4, %rd90;
	ld.global.u64 	%rd5, [%rd4+8];
	mul.lo.s64 	%rd6, %rd85, %rd84;
	ld.global.u64 	%rd7, [%rd4+72];
	mul.lo.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd93, [%rd4];
	mul.lo.s64 	%rd94, %rd8, %rd93;
	setp.le.u64 	%p1, %rd94, %rd3;
	@%p1 bra 	$L__BB24_48;

	ld.global.u64 	%rd9, [%rd4+80];
	ld.global.u64 	%rd10, [%rd4+16];
	ld.global.u64 	%rd11, [%rd4+24];
	ld.global.u64 	%rd12, [%rd4+88];
	and.b64  	%rd95, %rd8, -4294967296;
	setp.eq.s64 	%p2, %rd95, 0;
	@%p2 bra 	$L__BB24_3;

	div.u64 	%rd160, %rd3, %rd8;
	bra.uni 	$L__BB24_4;

$L__BB24_3:
	cvt.u32.u64 	%r23, %rd8;
	cvt.u32.u64 	%r24, %rd3;
	div.u32 	%r25, %r24, %r23;
	cvt.u64.u32 	%rd160, %r25;

$L__BB24_4:
	and.b64  	%rd96, %rd6, -4294967296;
	setp.eq.s64 	%p3, %rd96, 0;
	@%p3 bra 	$L__BB24_6;

	div.u64 	%rd161, %rd3, %rd6;
	bra.uni 	$L__BB24_7;

$L__BB24_6:
	cvt.u32.u64 	%r26, %rd6;
	cvt.u32.u64 	%r27, %rd3;
	div.u32 	%r28, %r27, %r26;
	cvt.u64.u32 	%rd161, %r28;

$L__BB24_7:
	and.b64  	%rd97, %rd7, -4294967296;
	setp.eq.s64 	%p4, %rd97, 0;
	@%p4 bra 	$L__BB24_9;

	rem.u64 	%rd162, %rd161, %rd7;
	bra.uni 	$L__BB24_10;

$L__BB24_9:
	cvt.u32.u64 	%r29, %rd7;
	cvt.u32.u64 	%r30, %rd161;
	rem.u32 	%r31, %r30, %r29;
	cvt.u64.u32 	%rd162, %r31;

$L__BB24_10:
	and.b64  	%rd98, %rd84, -4294967296;
	setp.eq.s64 	%p5, %rd98, 0;
	@%p5 bra 	$L__BB24_12;

	div.u64 	%rd163, %rd3, %rd84;
	mul.lo.s64 	%rd99, %rd163, %rd84;
	sub.s64 	%rd164, %rd3, %rd99;
	bra.uni 	$L__BB24_13;

$L__BB24_12:
	cvt.u32.u64 	%r32, %rd84;
	cvt.u32.u64 	%r33, %rd3;
	div.u32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, %r32;
	sub.s32 	%r36, %r33, %r35;
	cvt.u64.u32 	%rd163, %r34;
	cvt.u64.u32 	%rd164, %r36;

$L__BB24_13:
	and.b64  	%rd100, %rd85, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB24_15;

	rem.u64 	%rd165, %rd163, %rd85;
	bra.uni 	$L__BB24_16;

$L__BB24_15:
	cvt.u32.u64 	%r37, %rd85;
	cvt.u32.u64 	%r38, %rd163;
	rem.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd165, %r39;

$L__BB24_16:
	ld.global.u64 	%rd101, [%rd4+32];
	mul.lo.s64 	%rd31, %rd101, %rd160;
	cvt.u32.u64 	%r1, %rd12;
	setp.lt.s32 	%p7, %r1, 1;
	mov.u32 	%r90, 0;
	@%p7 bra 	$L__BB24_47;

	cvt.u32.u64 	%r2, %rd9;
	setp.lt.s32 	%p8, %r2, 1;
	@%p8 bra 	$L__BB24_47;

	add.s64 	%rd32, %rd5, -1;
	and.b64  	%rd33, %rd5, 3;
	sub.s64 	%rd34, %rd33, %rd5;
	shl.b64 	%rd103, %rd31, 2;
	add.s64 	%rd35, %rd2, %rd103;
	add.s64 	%rd36, %rd165, %rd87;
	add.s64 	%rd37, %rd164, %rd87;
	mov.u32 	%r83, 0;
	mov.u64 	%rd166, 0;
	mov.u32 	%r90, %r83;

$L__BB24_19:
	cvt.s64.s32 	%rd39, %r83;
	mul.lo.s64 	%rd104, %rd39, %rd88;
	sub.s64 	%rd40, %rd37, %rd104;
	cvt.u32.u64 	%r44, %rd40;
	setp.lt.s32 	%p9, %r44, 0;
	@%p9 bra 	$L__BB24_46;

	cvt.s64.s32 	%rd41, %rd40;
	or.b64  	%rd105, %rd41, %rd86;
	and.b64  	%rd106, %rd105, -4294967296;
	setp.eq.s64 	%p10, %rd106, 0;
	@%p10 bra 	$L__BB24_22;

	rem.u64 	%rd167, %rd41, %rd86;
	bra.uni 	$L__BB24_23;

$L__BB24_22:
	cvt.u32.u64 	%r45, %rd86;
	cvt.u32.u64 	%r46, %rd41;
	rem.u32 	%r47, %r46, %r45;
	cvt.u64.u32 	%rd167, %r47;

$L__BB24_23:
	setp.ne.s64 	%p11, %rd167, 0;
	@%p11 bra 	$L__BB24_46;

	@%p10 bra 	$L__BB24_26;

	div.u64 	%rd168, %rd41, %rd86;
	bra.uni 	$L__BB24_27;

$L__BB24_26:
	cvt.u32.u64 	%r48, %rd86;
	cvt.u32.u64 	%r49, %rd41;
	div.u32 	%r50, %r49, %r48;
	cvt.u64.u32 	%rd168, %r50;

$L__BB24_27:
	cvt.s64.s32 	%rd48, %rd168;
	setp.ge.u64 	%p13, %rd48, %rd11;
	setp.eq.s64 	%p14, %rd5, 0;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB24_46;

	mov.u32 	%r85, 0;
	mov.u64 	%rd169, 0;

$L__BB24_29:
	cvt.s64.s32 	%rd50, %r85;
	mul.lo.s64 	%rd110, %rd50, %rd88;
	sub.s64 	%rd51, %rd36, %rd110;
	cvt.u32.u64 	%r52, %rd51;
	setp.lt.s32 	%p16, %r52, 0;
	@%p16 bra 	$L__BB24_45;

	cvt.s64.s32 	%rd52, %rd51;
	or.b64  	%rd111, %rd52, %rd86;
	and.b64  	%rd112, %rd111, -4294967296;
	setp.eq.s64 	%p17, %rd112, 0;
	@%p17 bra 	$L__BB24_32;

	rem.u64 	%rd170, %rd52, %rd86;
	bra.uni 	$L__BB24_33;

$L__BB24_32:
	cvt.u32.u64 	%r53, %rd86;
	cvt.u32.u64 	%r54, %rd52;
	rem.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd170, %r55;

$L__BB24_33:
	setp.ne.s64 	%p18, %rd170, 0;
	@%p18 bra 	$L__BB24_45;

	@%p17 bra 	$L__BB24_36;

	div.u64 	%rd171, %rd52, %rd86;
	bra.uni 	$L__BB24_37;

$L__BB24_36:
	cvt.u32.u64 	%r56, %rd86;
	cvt.u32.u64 	%r57, %rd52;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd171, %r58;

$L__BB24_37:
	cvt.s64.s32 	%rd59, %rd171;
	setp.ge.u64 	%p20, %rd59, %rd10;
	@%p20 bra 	$L__BB24_45;

	setp.lt.u64 	%p21, %rd32, 3;
	ld.global.u64 	%rd60, [%rd4+40];
	mov.u64 	%rd175, 0;
	ld.global.u64 	%rd116, [%rd4+48];
	mul.lo.s64 	%rd61, %rd116, %rd59;
	ld.global.u64 	%rd117, [%rd4+56];
	mul.lo.s64 	%rd62, %rd117, %rd48;
	ld.global.u64 	%rd63, [%rd4+96];
	ld.global.u64 	%rd118, [%rd4+104];
	mul.lo.s64 	%rd64, %rd118, %rd162;
	ld.global.u64 	%rd65, [%rd4+112];
	ld.global.u64 	%rd66, [%rd4+120];
	@%p21 bra 	$L__BB24_41;

	add.s64 	%rd120, %rd62, %rd61;
	shl.b64 	%rd121, %rd120, 2;
	add.s64 	%rd173, %rd35, %rd121;
	shl.b64 	%rd122, %rd64, 2;
	add.s64 	%rd123, %rd1, %rd122;
	mul.lo.s64 	%rd124, %rd169, %rd65;
	shl.b64 	%rd125, %rd124, 2;
	mul.lo.s64 	%rd126, %rd166, %rd66;
	add.s64 	%rd127, %rd126, %rd125;
	add.s64 	%rd174, %rd123, %rd127;
	mov.u64 	%rd175, 0;

$L__BB24_40:
	shl.b64 	%rd157, %rd63, 2;
	shl.b64 	%rd156, %rd60, 2;
	ld.global.u32 	%r60, [%rd174];
	ld.global.u32 	%r61, [%rd173];
	mad.lo.s32 	%r62, %r60, %r61, %r90;
	add.s64 	%rd128, %rd173, %rd156;
	add.s64 	%rd129, %rd174, %rd157;
	ld.global.u32 	%r63, [%rd129];
	ld.global.u32 	%r64, [%rd128];
	mad.lo.s32 	%r65, %r63, %r64, %r62;
	add.s64 	%rd130, %rd128, %rd156;
	add.s64 	%rd131, %rd129, %rd157;
	ld.global.u32 	%r66, [%rd131];
	ld.global.u32 	%r67, [%rd130];
	mad.lo.s32 	%r68, %r66, %r67, %r65;
	add.s64 	%rd132, %rd130, %rd156;
	add.s64 	%rd173, %rd132, %rd156;
	add.s64 	%rd133, %rd131, %rd157;
	add.s64 	%rd174, %rd133, %rd157;
	ld.global.u32 	%r69, [%rd133];
	ld.global.u32 	%r70, [%rd132];
	mad.lo.s32 	%r90, %r69, %r70, %r68;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd134, %rd34, %rd175;
	setp.ne.s64 	%p22, %rd134, 0;
	@%p22 bra 	$L__BB24_40;

$L__BB24_41:
	setp.eq.s64 	%p23, %rd33, 0;
	@%p23 bra 	$L__BB24_45;

	setp.eq.s64 	%p24, %rd33, 1;
	mul.lo.s64 	%rd135, %rd60, %rd175;
	add.s64 	%rd136, %rd135, %rd31;
	add.s64 	%rd137, %rd136, %rd61;
	add.s64 	%rd138, %rd137, %rd62;
	mul.lo.s64 	%rd139, %rd63, %rd175;
	add.s64 	%rd140, %rd64, %rd139;
	mul.lo.s64 	%rd141, %rd65, %rd50;
	add.s64 	%rd142, %rd140, %rd141;
	mul.lo.s64 	%rd143, %rd66, %rd39;
	add.s64 	%rd144, %rd142, %rd143;
	shl.b64 	%rd145, %rd138, 2;
	add.s64 	%rd78, %rd2, %rd145;
	shl.b64 	%rd146, %rd144, 2;
	add.s64 	%rd79, %rd1, %rd146;
	ld.global.u32 	%r71, [%rd79];
	ld.global.u32 	%r72, [%rd78];
	mad.lo.s32 	%r90, %r71, %r72, %r90;
	@%p24 bra 	$L__BB24_45;

	setp.eq.s64 	%p25, %rd33, 2;
	shl.b64 	%rd147, %rd63, 2;
	add.s64 	%rd80, %rd79, %rd147;
	ld.global.u32 	%r73, [%rd80];
	shl.b64 	%rd148, %rd60, 2;
	add.s64 	%rd81, %rd78, %rd148;
	ld.global.u32 	%r74, [%rd81];
	mad.lo.s32 	%r90, %r73, %r74, %r90;
	@%p25 bra 	$L__BB24_45;

	add.s64 	%rd150, %rd81, %rd148;
	add.s64 	%rd152, %rd80, %rd147;
	ld.global.u32 	%r75, [%rd152];
	ld.global.u32 	%r76, [%rd150];
	mad.lo.s32 	%r90, %r75, %r76, %r90;

$L__BB24_45:
	cvt.u32.u64 	%r77, %rd50;
	add.s32 	%r85, %r77, 1;
	setp.lt.s32 	%p26, %r85, %r2;
	add.s64 	%rd169, %rd169, 1;
	@%p26 bra 	$L__BB24_29;

$L__BB24_46:
	cvt.u32.u64 	%r78, %rd39;
	add.s32 	%r83, %r78, 1;
	setp.lt.s32 	%p27, %r83, %r1;
	add.s64 	%rd166, %rd166, 4;
	@%p27 bra 	$L__BB24_19;

$L__BB24_47:
	ld.param.u64 	%rd159, [conv_transpose2d_u32_param_10];
	mov.u32 	%r82, %tid.x;
	mov.u32 	%r81, %ntid.x;
	mov.u32 	%r80, %ctaid.x;
	mad.lo.s32 	%r79, %r80, %r81, %r82;
	cvt.u64.u32 	%rd158, %r79;
	cvta.to.global.u64 	%rd153, %rd159;
	shl.b64 	%rd154, %rd158, 2;
	add.s64 	%rd155, %rd153, %rd154;
	st.global.u32 	[%rd155], %r90;

$L__BB24_48:
	ret;

}
	// .globl	avg_pool2d_f32
.visible .entry avg_pool2d_f32(
	.param .u64 avg_pool2d_f32_param_0,
	.param .u64 avg_pool2d_f32_param_1,
	.param .u64 avg_pool2d_f32_param_2,
	.param .u64 avg_pool2d_f32_param_3,
	.param .u64 avg_pool2d_f32_param_4,
	.param .u64 avg_pool2d_f32_param_5,
	.param .u64 avg_pool2d_f32_param_6,
	.param .u64 avg_pool2d_f32_param_7
)
{
	.reg .pred 	%p<25>;
	.reg .f32 	%f<46>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd58, [avg_pool2d_f32_param_1];
	ld.param.u64 	%rd59, [avg_pool2d_f32_param_2];
	ld.param.u64 	%rd60, [avg_pool2d_f32_param_3];
	ld.param.u64 	%rd61, [avg_pool2d_f32_param_4];
	ld.param.u64 	%rd63, [avg_pool2d_f32_param_5];
	ld.param.u64 	%rd64, [avg_pool2d_f32_param_6];
	ld.param.u64 	%rd62, [avg_pool2d_f32_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB25_2;

	div.u64 	%rd164, %rd7, %rd60;
	bra.uni 	$L__BB25_3;

$L__BB25_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd164, %r7;

$L__BB25_3:
	add.s64 	%rd11, %rd164, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB25_5;

	div.u64 	%rd165, %rd12, %rd61;
	bra.uni 	$L__BB25_6;

$L__BB25_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd165, %r10;

$L__BB25_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd165, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB25_48;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB25_9;

	div.u64 	%rd166, %rd2, %rd18;
	bra.uni 	$L__BB25_10;

$L__BB25_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd166, %r13;

$L__BB25_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB25_12;

	div.u64 	%rd167, %rd2, %rd17;
	bra.uni 	$L__BB25_13;

$L__BB25_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd167, %r16;

$L__BB25_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB25_15;

	rem.u64 	%rd168, %rd167, %rd4;
	bra.uni 	$L__BB25_16;

$L__BB25_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd167;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd168, %r19;

$L__BB25_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB25_18;

	div.u64 	%rd169, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd169, %rd16;
	sub.s64 	%rd170, %rd2, %rd77;
	bra.uni 	$L__BB25_19;

$L__BB25_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd169, %r22;
	cvt.u64.u32 	%rd170, %r24;

$L__BB25_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB25_21;

	rem.u64 	%rd171, %rd169, %rd11;
	bra.uni 	$L__BB25_22;

$L__BB25_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd169;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd171, %r27;

$L__BB25_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd166;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.f32 	%f36, 0f00000000;
	@%p9 bra 	$L__BB25_47;

	mul.lo.s64 	%rd38, %rd170, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB25_47;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd171, %rd60;
	mov.f32 	%f36, 0f00000000;
	mov.u64 	%rd172, 0;

$L__BB25_25:
	add.s64 	%rd44, %rd172, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB25_46;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd175, 0;
	@%p12 bra 	$L__BB25_37;

	mov.u64 	%rd175, 0;
	mov.u64 	%rd174, %rd41;

$L__BB25_28:
	add.s64 	%rd47, %rd175, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	@%p13 bra 	$L__BB25_30;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd168;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.f32 	%f24, [%rd93];
	add.f32 	%f36, %f36, %f24;

$L__BB25_30:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p14, %rd48, %rd5;
	@%p14 bra 	$L__BB25_32;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd168;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f32 	%f25, [%rd104];
	add.f32 	%f36, %f36, %f25;

$L__BB25_32:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p15, %rd49, %rd5;
	@%p15 bra 	$L__BB25_34;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd168;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.f32 	%f26, [%rd115];
	add.f32 	%f36, %f36, %f26;

$L__BB25_34:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p16, %rd50, %rd5;
	@%p16 bra 	$L__BB25_36;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd168;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 2;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.f32 	%f27, [%rd126];
	add.f32 	%f36, %f36, %f27;

$L__BB25_36:
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, -4;
	setp.ne.s64 	%p17, %rd174, 0;
	@%p17 bra 	$L__BB25_28;

$L__BB25_37:
	setp.eq.s64 	%p18, %rd40, 0;
	@%p18 bra 	$L__BB25_46;

	add.s64 	%rd54, %rd175, %rd38;
	setp.ge.u64 	%p19, %rd54, %rd5;
	@%p19 bra 	$L__BB25_40;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd168;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.f32 	%f28, [%rd137];
	add.f32 	%f36, %f36, %f28;

$L__BB25_40:
	setp.eq.s64 	%p20, %rd40, 1;
	@%p20 bra 	$L__BB25_46;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p21, %rd55, %rd5;
	@%p21 bra 	$L__BB25_43;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd168;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 2;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.f32 	%f29, [%rd148];
	add.f32 	%f36, %f36, %f29;

$L__BB25_43:
	setp.eq.s64 	%p22, %rd40, 2;
	@%p22 bra 	$L__BB25_46;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p23, %rd56, %rd5;
	@%p23 bra 	$L__BB25_46;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd168;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 2;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.f32 	%f30, [%rd159];
	add.f32 	%f36, %f36, %f30;

$L__BB25_46:
	add.s64 	%rd172, %rd172, 1;
	setp.lt.u64 	%p24, %rd172, %rd58;
	@%p24 bra 	$L__BB25_25;

$L__BB25_47:
	cvta.to.global.u64 	%rd160, %rd62;
	shl.b64 	%rd161, %rd2, 2;
	add.s64 	%rd162, %rd160, %rd161;
	mul.lo.s64 	%rd163, %rd59, %rd58;
	cvt.rn.f64.u64 	%fd1, %rd163;
	rcp.rn.f64 	%fd2, %fd1;
	cvt.rn.f32.f64 	%f31, %fd2;
	mul.f32 	%f32, %f36, %f31;
	st.global.f32 	[%rd162], %f32;

$L__BB25_48:
	ret;

}
	// .globl	avg_pool2d_f64
.visible .entry avg_pool2d_f64(
	.param .u64 avg_pool2d_f64_param_0,
	.param .u64 avg_pool2d_f64_param_1,
	.param .u64 avg_pool2d_f64_param_2,
	.param .u64 avg_pool2d_f64_param_3,
	.param .u64 avg_pool2d_f64_param_4,
	.param .u64 avg_pool2d_f64_param_5,
	.param .u64 avg_pool2d_f64_param_6,
	.param .u64 avg_pool2d_f64_param_7
)
{
	.reg .pred 	%p<25>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<48>;
	.reg .b64 	%rd<179>;


	ld.param.u64 	%rd58, [avg_pool2d_f64_param_1];
	ld.param.u64 	%rd59, [avg_pool2d_f64_param_2];
	ld.param.u64 	%rd60, [avg_pool2d_f64_param_3];
	ld.param.u64 	%rd61, [avg_pool2d_f64_param_4];
	ld.param.u64 	%rd63, [avg_pool2d_f64_param_5];
	ld.param.u64 	%rd64, [avg_pool2d_f64_param_6];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB26_2;

	div.u64 	%rd167, %rd7, %rd60;
	bra.uni 	$L__BB26_3;

$L__BB26_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd167, %r7;

$L__BB26_3:
	add.s64 	%rd11, %rd167, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB26_5;

	div.u64 	%rd168, %rd12, %rd61;
	bra.uni 	$L__BB26_6;

$L__BB26_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd168, %r10;

$L__BB26_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd168, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB26_48;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB26_9;

	div.u64 	%rd169, %rd2, %rd18;
	bra.uni 	$L__BB26_10;

$L__BB26_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd169, %r13;

$L__BB26_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB26_12;

	div.u64 	%rd170, %rd2, %rd17;
	bra.uni 	$L__BB26_13;

$L__BB26_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd170, %r16;

$L__BB26_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB26_15;

	rem.u64 	%rd171, %rd170, %rd4;
	bra.uni 	$L__BB26_16;

$L__BB26_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd170;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd171, %r19;

$L__BB26_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB26_18;

	div.u64 	%rd172, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd172, %rd16;
	sub.s64 	%rd173, %rd2, %rd77;
	bra.uni 	$L__BB26_19;

$L__BB26_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd172, %r22;
	cvt.u64.u32 	%rd173, %r24;

$L__BB26_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB26_21;

	rem.u64 	%rd174, %rd172, %rd11;
	bra.uni 	$L__BB26_22;

$L__BB26_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd172;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd174, %r27;

$L__BB26_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd169;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.f64 	%fd38, 0d0000000000000000;
	@%p9 bra 	$L__BB26_47;

	mul.lo.s64 	%rd38, %rd173, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB26_47;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd174, %rd60;
	mov.f64 	%fd38, 0d0000000000000000;
	mov.u64 	%rd175, 0;

$L__BB26_25:
	add.s64 	%rd44, %rd175, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB26_46;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd178, 0;
	@%p12 bra 	$L__BB26_37;

	mov.u64 	%rd178, 0;
	mov.u64 	%rd177, %rd41;

$L__BB26_28:
	add.s64 	%rd47, %rd178, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	@%p13 bra 	$L__BB26_30;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd171;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 3;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.f64 	%fd24, [%rd93];
	add.f64 	%fd38, %fd38, %fd24;

$L__BB26_30:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p14, %rd48, %rd5;
	@%p14 bra 	$L__BB26_32;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd171;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 3;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f64 	%fd25, [%rd104];
	add.f64 	%fd38, %fd38, %fd25;

$L__BB26_32:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p15, %rd49, %rd5;
	@%p15 bra 	$L__BB26_34;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd171;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 3;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.f64 	%fd26, [%rd115];
	add.f64 	%fd38, %fd38, %fd26;

$L__BB26_34:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p16, %rd50, %rd5;
	@%p16 bra 	$L__BB26_36;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd171;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 3;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.f64 	%fd27, [%rd126];
	add.f64 	%fd38, %fd38, %fd27;

$L__BB26_36:
	add.s64 	%rd178, %rd178, 4;
	add.s64 	%rd177, %rd177, -4;
	setp.ne.s64 	%p17, %rd177, 0;
	@%p17 bra 	$L__BB26_28;

$L__BB26_37:
	setp.eq.s64 	%p18, %rd40, 0;
	@%p18 bra 	$L__BB26_46;

	add.s64 	%rd54, %rd178, %rd38;
	setp.ge.u64 	%p19, %rd54, %rd5;
	@%p19 bra 	$L__BB26_40;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd171;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 3;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.f64 	%fd28, [%rd137];
	add.f64 	%fd38, %fd38, %fd28;

$L__BB26_40:
	setp.eq.s64 	%p20, %rd40, 1;
	@%p20 bra 	$L__BB26_46;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p21, %rd55, %rd5;
	@%p21 bra 	$L__BB26_43;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd171;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 3;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.f64 	%fd29, [%rd148];
	add.f64 	%fd38, %fd38, %fd29;

$L__BB26_43:
	setp.eq.s64 	%p22, %rd40, 2;
	@%p22 bra 	$L__BB26_46;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p23, %rd56, %rd5;
	@%p23 bra 	$L__BB26_46;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd171;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 3;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.f64 	%fd30, [%rd159];
	add.f64 	%fd38, %fd38, %fd30;

$L__BB26_46:
	add.s64 	%rd175, %rd175, 1;
	setp.lt.u64 	%p24, %rd175, %rd58;
	@%p24 bra 	$L__BB26_25;

$L__BB26_47:
	ld.param.u64 	%rd166, [avg_pool2d_f64_param_7];
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	cvt.u64.u32 	%rd165, %r28;
	ld.param.u64 	%rd164, [avg_pool2d_f64_param_2];
	mul.lo.s64 	%rd160, %rd164, %rd58;
	cvt.rn.f64.u64 	%fd31, %rd160;
	rcp.rn.f64 	%fd32, %fd31;
	cvt.rn.f32.f64 	%f1, %fd32;
	cvt.f64.f32 	%fd33, %f1;
	mul.f64 	%fd34, %fd38, %fd33;
	cvta.to.global.u64 	%rd161, %rd166;
	shl.b64 	%rd162, %rd165, 3;
	add.s64 	%rd163, %rd161, %rd162;
	st.global.f64 	[%rd163], %fd34;

$L__BB26_48:
	ret;

}
	// .globl	avg_pool2d_u8
.visible .entry avg_pool2d_u8(
	.param .u64 avg_pool2d_u8_param_0,
	.param .u64 avg_pool2d_u8_param_1,
	.param .u64 avg_pool2d_u8_param_2,
	.param .u64 avg_pool2d_u8_param_3,
	.param .u64 avg_pool2d_u8_param_4,
	.param .u64 avg_pool2d_u8_param_5,
	.param .u64 avg_pool2d_u8_param_6,
	.param .u64 avg_pool2d_u8_param_7
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<44>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<168>;


	ld.param.u64 	%rd58, [avg_pool2d_u8_param_1];
	ld.param.u64 	%rd59, [avg_pool2d_u8_param_2];
	ld.param.u64 	%rd60, [avg_pool2d_u8_param_3];
	ld.param.u64 	%rd61, [avg_pool2d_u8_param_4];
	ld.param.u64 	%rd63, [avg_pool2d_u8_param_5];
	ld.param.u64 	%rd64, [avg_pool2d_u8_param_6];
	ld.param.u64 	%rd62, [avg_pool2d_u8_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB27_2;

	div.u64 	%rd156, %rd7, %rd60;
	bra.uni 	$L__BB27_3;

$L__BB27_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd156, %r7;

$L__BB27_3:
	add.s64 	%rd11, %rd156, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB27_5;

	div.u64 	%rd157, %rd12, %rd61;
	bra.uni 	$L__BB27_6;

$L__BB27_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd157, %r10;

$L__BB27_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd157, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB27_48;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB27_9;

	div.u64 	%rd158, %rd2, %rd18;
	bra.uni 	$L__BB27_10;

$L__BB27_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd158, %r13;

$L__BB27_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB27_12;

	div.u64 	%rd159, %rd2, %rd17;
	bra.uni 	$L__BB27_13;

$L__BB27_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd159, %r16;

$L__BB27_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB27_15;

	rem.u64 	%rd160, %rd159, %rd4;
	bra.uni 	$L__BB27_16;

$L__BB27_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd159;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd160, %r19;

$L__BB27_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB27_18;

	div.u64 	%rd161, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd161, %rd16;
	sub.s64 	%rd162, %rd2, %rd77;
	bra.uni 	$L__BB27_19;

$L__BB27_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd161, %r22;
	cvt.u64.u32 	%rd162, %r24;

$L__BB27_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB27_21;

	rem.u64 	%rd163, %rd161, %rd11;
	bra.uni 	$L__BB27_22;

$L__BB27_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd161;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd163, %r27;

$L__BB27_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd158;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.u16 	%rs34, 0;
	@%p9 bra 	$L__BB27_47;

	mul.lo.s64 	%rd38, %rd162, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB27_47;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd163, %rd60;
	mov.u16 	%rs34, 0;
	mov.u64 	%rd164, 0;

$L__BB27_25:
	add.s64 	%rd44, %rd164, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB27_46;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd167, 0;
	@%p12 bra 	$L__BB27_37;

	mov.u64 	%rd167, 0;
	mov.u64 	%rd166, %rd41;

$L__BB27_28:
	add.s64 	%rd47, %rd167, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	@%p13 bra 	$L__BB27_30;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd160;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	add.s64 	%rd92, %rd1, %rd91;
	ld.global.u8 	%rs24, [%rd92];
	add.s16 	%rs34, %rs24, %rs34;

$L__BB27_30:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p14, %rd48, %rd5;
	@%p14 bra 	$L__BB27_32;

	ld.global.u64 	%rd93, [%rd3+40];
	mul.lo.s64 	%rd94, %rd93, %rd160;
	add.s64 	%rd95, %rd94, %rd37;
	ld.global.u64 	%rd96, [%rd3+48];
	mul.lo.s64 	%rd97, %rd96, %rd44;
	add.s64 	%rd98, %rd95, %rd97;
	ld.global.u64 	%rd99, [%rd3+56];
	mul.lo.s64 	%rd100, %rd99, %rd48;
	add.s64 	%rd101, %rd98, %rd100;
	add.s64 	%rd102, %rd1, %rd101;
	ld.global.u8 	%rs25, [%rd102];
	add.s16 	%rs34, %rs25, %rs34;

$L__BB27_32:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p15, %rd49, %rd5;
	@%p15 bra 	$L__BB27_34;

	ld.global.u64 	%rd103, [%rd3+40];
	mul.lo.s64 	%rd104, %rd103, %rd160;
	add.s64 	%rd105, %rd104, %rd37;
	ld.global.u64 	%rd106, [%rd3+48];
	mul.lo.s64 	%rd107, %rd106, %rd44;
	add.s64 	%rd108, %rd105, %rd107;
	ld.global.u64 	%rd109, [%rd3+56];
	mul.lo.s64 	%rd110, %rd109, %rd49;
	add.s64 	%rd111, %rd108, %rd110;
	add.s64 	%rd112, %rd1, %rd111;
	ld.global.u8 	%rs26, [%rd112];
	add.s16 	%rs34, %rs26, %rs34;

$L__BB27_34:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p16, %rd50, %rd5;
	@%p16 bra 	$L__BB27_36;

	ld.global.u64 	%rd113, [%rd3+40];
	mul.lo.s64 	%rd114, %rd113, %rd160;
	add.s64 	%rd115, %rd114, %rd37;
	ld.global.u64 	%rd116, [%rd3+48];
	mul.lo.s64 	%rd117, %rd116, %rd44;
	add.s64 	%rd118, %rd115, %rd117;
	ld.global.u64 	%rd119, [%rd3+56];
	mul.lo.s64 	%rd120, %rd119, %rd50;
	add.s64 	%rd121, %rd118, %rd120;
	add.s64 	%rd122, %rd1, %rd121;
	ld.global.u8 	%rs27, [%rd122];
	add.s16 	%rs34, %rs27, %rs34;

$L__BB27_36:
	add.s64 	%rd167, %rd167, 4;
	add.s64 	%rd166, %rd166, -4;
	setp.ne.s64 	%p17, %rd166, 0;
	@%p17 bra 	$L__BB27_28;

$L__BB27_37:
	setp.eq.s64 	%p18, %rd40, 0;
	@%p18 bra 	$L__BB27_46;

	add.s64 	%rd54, %rd167, %rd38;
	setp.ge.u64 	%p19, %rd54, %rd5;
	@%p19 bra 	$L__BB27_40;

	ld.global.u64 	%rd123, [%rd3+40];
	mul.lo.s64 	%rd124, %rd123, %rd160;
	add.s64 	%rd125, %rd124, %rd37;
	ld.global.u64 	%rd126, [%rd3+48];
	mul.lo.s64 	%rd127, %rd126, %rd44;
	add.s64 	%rd128, %rd125, %rd127;
	ld.global.u64 	%rd129, [%rd3+56];
	mul.lo.s64 	%rd130, %rd129, %rd54;
	add.s64 	%rd131, %rd128, %rd130;
	add.s64 	%rd132, %rd1, %rd131;
	ld.global.u8 	%rs28, [%rd132];
	add.s16 	%rs34, %rs28, %rs34;

$L__BB27_40:
	setp.eq.s64 	%p20, %rd40, 1;
	@%p20 bra 	$L__BB27_46;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p21, %rd55, %rd5;
	@%p21 bra 	$L__BB27_43;

	ld.global.u64 	%rd133, [%rd3+40];
	mul.lo.s64 	%rd134, %rd133, %rd160;
	add.s64 	%rd135, %rd134, %rd37;
	ld.global.u64 	%rd136, [%rd3+48];
	mul.lo.s64 	%rd137, %rd136, %rd44;
	add.s64 	%rd138, %rd135, %rd137;
	ld.global.u64 	%rd139, [%rd3+56];
	mul.lo.s64 	%rd140, %rd139, %rd55;
	add.s64 	%rd141, %rd138, %rd140;
	add.s64 	%rd142, %rd1, %rd141;
	ld.global.u8 	%rs29, [%rd142];
	add.s16 	%rs34, %rs29, %rs34;

$L__BB27_43:
	setp.eq.s64 	%p22, %rd40, 2;
	@%p22 bra 	$L__BB27_46;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p23, %rd56, %rd5;
	@%p23 bra 	$L__BB27_46;

	ld.global.u64 	%rd143, [%rd3+40];
	mul.lo.s64 	%rd144, %rd143, %rd160;
	add.s64 	%rd145, %rd144, %rd37;
	ld.global.u64 	%rd146, [%rd3+48];
	mul.lo.s64 	%rd147, %rd146, %rd44;
	add.s64 	%rd148, %rd145, %rd147;
	ld.global.u64 	%rd149, [%rd3+56];
	mul.lo.s64 	%rd150, %rd149, %rd56;
	add.s64 	%rd151, %rd148, %rd150;
	add.s64 	%rd152, %rd1, %rd151;
	ld.global.u8 	%rs30, [%rd152];
	add.s16 	%rs34, %rs30, %rs34;

$L__BB27_46:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p24, %rd164, %rd58;
	@%p24 bra 	$L__BB27_25;

$L__BB27_47:
	and.b16  	%rs31, %rs34, 255;
	cvt.rn.f32.u16 	%f1, %rs31;
	mul.lo.s64 	%rd153, %rd59, %rd58;
	cvt.rn.f64.u64 	%fd1, %rd153;
	rcp.rn.f64 	%fd2, %fd1;
	cvt.rn.f32.f64 	%f2, %fd2;
	mul.f32 	%f3, %f2, %f1;
	cvt.rzi.u32.f32 	%r28, %f3;
	cvta.to.global.u64 	%rd154, %rd62;
	add.s64 	%rd155, %rd154, %rd2;
	st.global.u8 	[%rd155], %r28;

$L__BB27_48:
	ret;

}
	// .globl	avg_pool2d_u32
.visible .entry avg_pool2d_u32(
	.param .u64 avg_pool2d_u32_param_0,
	.param .u64 avg_pool2d_u32_param_1,
	.param .u64 avg_pool2d_u32_param_2,
	.param .u64 avg_pool2d_u32_param_3,
	.param .u64 avg_pool2d_u32_param_4,
	.param .u64 avg_pool2d_u32_param_5,
	.param .u64 avg_pool2d_u32_param_6,
	.param .u64 avg_pool2d_u32_param_7
)
{
	.reg .pred 	%p<25>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<71>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd58, [avg_pool2d_u32_param_1];
	ld.param.u64 	%rd59, [avg_pool2d_u32_param_2];
	ld.param.u64 	%rd60, [avg_pool2d_u32_param_3];
	ld.param.u64 	%rd61, [avg_pool2d_u32_param_4];
	ld.param.u64 	%rd63, [avg_pool2d_u32_param_5];
	ld.param.u64 	%rd64, [avg_pool2d_u32_param_6];
	ld.param.u64 	%rd62, [avg_pool2d_u32_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r23, %r21, %r20, %r22;
	cvt.u64.u32 	%rd2, %r23;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB28_2;

	div.u64 	%rd164, %rd7, %rd60;
	bra.uni 	$L__BB28_3;

$L__BB28_2:
	cvt.u32.u64 	%r24, %rd60;
	cvt.u32.u64 	%r25, %rd7;
	div.u32 	%r26, %r25, %r24;
	cvt.u64.u32 	%rd164, %r26;

$L__BB28_3:
	add.s64 	%rd11, %rd164, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB28_5;

	div.u64 	%rd165, %rd12, %rd61;
	bra.uni 	$L__BB28_6;

$L__BB28_5:
	cvt.u32.u64 	%r27, %rd61;
	cvt.u32.u64 	%r28, %rd12;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd165, %r29;

$L__BB28_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd165, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB28_48;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB28_9;

	div.u64 	%rd166, %rd2, %rd18;
	bra.uni 	$L__BB28_10;

$L__BB28_9:
	cvt.u32.u64 	%r30, %rd18;
	cvt.u32.u64 	%r31, %rd2;
	div.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd166, %r32;

$L__BB28_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB28_12;

	div.u64 	%rd167, %rd2, %rd17;
	bra.uni 	$L__BB28_13;

$L__BB28_12:
	cvt.u32.u64 	%r33, %rd17;
	cvt.u32.u64 	%r34, %rd2;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd167, %r35;

$L__BB28_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB28_15;

	rem.u64 	%rd168, %rd167, %rd4;
	bra.uni 	$L__BB28_16;

$L__BB28_15:
	cvt.u32.u64 	%r36, %rd4;
	cvt.u32.u64 	%r37, %rd167;
	rem.u32 	%r38, %r37, %r36;
	cvt.u64.u32 	%rd168, %r38;

$L__BB28_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB28_18;

	div.u64 	%rd169, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd169, %rd16;
	sub.s64 	%rd170, %rd2, %rd77;
	bra.uni 	$L__BB28_19;

$L__BB28_18:
	cvt.u32.u64 	%r39, %rd16;
	cvt.u32.u64 	%r40, %rd2;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd169, %r41;
	cvt.u64.u32 	%rd170, %r43;

$L__BB28_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB28_21;

	rem.u64 	%rd171, %rd169, %rd11;
	bra.uni 	$L__BB28_22;

$L__BB28_21:
	cvt.u32.u64 	%r44, %rd11;
	cvt.u32.u64 	%r45, %rd169;
	rem.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd171, %r46;

$L__BB28_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd166;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.u32 	%r61, 0;
	@%p9 bra 	$L__BB28_47;

	mul.lo.s64 	%rd38, %rd170, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB28_47;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd171, %rd60;
	mov.u32 	%r61, 0;
	mov.u64 	%rd172, 0;

$L__BB28_25:
	add.s64 	%rd44, %rd172, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB28_46;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd175, 0;
	@%p12 bra 	$L__BB28_37;

	mov.u64 	%rd175, 0;
	mov.u64 	%rd174, %rd41;

$L__BB28_28:
	add.s64 	%rd47, %rd175, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	@%p13 bra 	$L__BB28_30;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd168;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.u32 	%r51, [%rd93];
	add.s32 	%r61, %r51, %r61;

$L__BB28_30:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p14, %rd48, %rd5;
	@%p14 bra 	$L__BB28_32;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd168;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.u32 	%r52, [%rd104];
	add.s32 	%r61, %r52, %r61;

$L__BB28_32:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p15, %rd49, %rd5;
	@%p15 bra 	$L__BB28_34;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd168;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.u32 	%r53, [%rd115];
	add.s32 	%r61, %r53, %r61;

$L__BB28_34:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p16, %rd50, %rd5;
	@%p16 bra 	$L__BB28_36;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd168;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 2;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.u32 	%r54, [%rd126];
	add.s32 	%r61, %r54, %r61;

$L__BB28_36:
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, -4;
	setp.ne.s64 	%p17, %rd174, 0;
	@%p17 bra 	$L__BB28_28;

$L__BB28_37:
	setp.eq.s64 	%p18, %rd40, 0;
	@%p18 bra 	$L__BB28_46;

	add.s64 	%rd54, %rd175, %rd38;
	setp.ge.u64 	%p19, %rd54, %rd5;
	@%p19 bra 	$L__BB28_40;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd168;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.u32 	%r55, [%rd137];
	add.s32 	%r61, %r55, %r61;

$L__BB28_40:
	setp.eq.s64 	%p20, %rd40, 1;
	@%p20 bra 	$L__BB28_46;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p21, %rd55, %rd5;
	@%p21 bra 	$L__BB28_43;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd168;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 2;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.u32 	%r56, [%rd148];
	add.s32 	%r61, %r56, %r61;

$L__BB28_43:
	setp.eq.s64 	%p22, %rd40, 2;
	@%p22 bra 	$L__BB28_46;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p23, %rd56, %rd5;
	@%p23 bra 	$L__BB28_46;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd168;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 2;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.u32 	%r57, [%rd159];
	add.s32 	%r61, %r57, %r61;

$L__BB28_46:
	add.s64 	%rd172, %rd172, 1;
	setp.lt.u64 	%p24, %rd172, %rd58;
	@%p24 bra 	$L__BB28_25;

$L__BB28_47:
	cvt.rn.f32.u32 	%f1, %r61;
	mul.lo.s64 	%rd160, %rd59, %rd58;
	cvt.rn.f64.u64 	%fd1, %rd160;
	rcp.rn.f64 	%fd2, %fd1;
	cvt.rn.f32.f64 	%f2, %fd2;
	mul.f32 	%f3, %f2, %f1;
	cvt.rzi.u32.f32 	%r58, %f3;
	cvta.to.global.u64 	%rd161, %rd62;
	shl.b64 	%rd162, %rd2, 2;
	add.s64 	%rd163, %rd161, %rd162;
	st.global.u32 	[%rd163], %r58;

$L__BB28_48:
	ret;

}
	// .globl	max_pool2d_f32
.visible .entry max_pool2d_f32(
	.param .u64 max_pool2d_f32_param_0,
	.param .u64 max_pool2d_f32_param_1,
	.param .u64 max_pool2d_f32_param_2,
	.param .u64 max_pool2d_f32_param_3,
	.param .u64 max_pool2d_f32_param_4,
	.param .u64 max_pool2d_f32_param_5,
	.param .u64 max_pool2d_f32_param_6,
	.param .u64 max_pool2d_f32_param_7
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<39>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd58, [max_pool2d_f32_param_1];
	ld.param.u64 	%rd59, [max_pool2d_f32_param_2];
	ld.param.u64 	%rd60, [max_pool2d_f32_param_3];
	ld.param.u64 	%rd61, [max_pool2d_f32_param_4];
	ld.param.u64 	%rd63, [max_pool2d_f32_param_5];
	ld.param.u64 	%rd64, [max_pool2d_f32_param_6];
	ld.param.u64 	%rd62, [max_pool2d_f32_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB29_2;

	div.u64 	%rd164, %rd7, %rd60;
	bra.uni 	$L__BB29_3;

$L__BB29_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd164, %r7;

$L__BB29_3:
	add.s64 	%rd11, %rd164, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB29_5;

	div.u64 	%rd165, %rd12, %rd61;
	bra.uni 	$L__BB29_6;

$L__BB29_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd165, %r10;

$L__BB29_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd165, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB29_55;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB29_9;

	div.u64 	%rd166, %rd2, %rd18;
	bra.uni 	$L__BB29_10;

$L__BB29_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd166, %r13;

$L__BB29_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB29_12;

	div.u64 	%rd167, %rd2, %rd17;
	bra.uni 	$L__BB29_13;

$L__BB29_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd167, %r16;

$L__BB29_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB29_15;

	rem.u64 	%rd168, %rd167, %rd4;
	bra.uni 	$L__BB29_16;

$L__BB29_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd167;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd168, %r19;

$L__BB29_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB29_18;

	div.u64 	%rd169, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd169, %rd16;
	sub.s64 	%rd170, %rd2, %rd77;
	bra.uni 	$L__BB29_19;

$L__BB29_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd169, %r22;
	cvt.u64.u32 	%rd170, %r24;

$L__BB29_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB29_21;

	rem.u64 	%rd171, %rd169, %rd11;
	bra.uni 	$L__BB29_22;

$L__BB29_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd169;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd171, %r27;

$L__BB29_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd166;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.f32 	%f42, 0f00000000;
	@%p9 bra 	$L__BB29_54;

	mul.lo.s64 	%rd38, %rd170, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB29_54;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd171, %rd60;
	mov.f32 	%f42, 0f00000000;
	mov.u16 	%rs38, 0;
	mov.u64 	%rd172, 0;

$L__BB29_25:
	add.s64 	%rd44, %rd172, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB29_53;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd175, 0;
	mov.u16 	%rs33, %rs38;
	mov.f32 	%f37, %f42;
	@%p12 bra 	$L__BB29_41;

	mov.u64 	%rd175, 0;
	mov.u16 	%rs33, %rs38;
	mov.f32 	%f37, %f42;
	mov.u64 	%rd174, %rd41;

$L__BB29_28:
	add.s64 	%rd47, %rd175, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	mov.f32 	%f34, %f37;
	mov.u16 	%rs30, %rs33;
	@%p13 bra 	$L__BB29_31;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd168;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.f32 	%f34, [%rd93];
	and.b16  	%rs15, %rs33, 255;
	setp.eq.s16 	%p14, %rs15, 0;
	mov.u16 	%rs30, 1;
	@%p14 bra 	$L__BB29_31;

	max.f32 	%f34, %f37, %f34;
	mov.u16 	%rs30, %rs33;

$L__BB29_31:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p15, %rd48, %rd5;
	mov.f32 	%f35, %f34;
	mov.u16 	%rs31, %rs30;
	@%p15 bra 	$L__BB29_34;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd168;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f32 	%f35, [%rd104];
	and.b16  	%rs17, %rs30, 255;
	setp.eq.s16 	%p16, %rs17, 0;
	mov.u16 	%rs31, 1;
	@%p16 bra 	$L__BB29_34;

	max.f32 	%f35, %f34, %f35;
	mov.u16 	%rs31, %rs30;

$L__BB29_34:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p17, %rd49, %rd5;
	mov.f32 	%f36, %f35;
	mov.u16 	%rs32, %rs31;
	@%p17 bra 	$L__BB29_37;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd168;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.f32 	%f36, [%rd115];
	and.b16  	%rs19, %rs31, 255;
	setp.eq.s16 	%p18, %rs19, 0;
	mov.u16 	%rs32, 1;
	@%p18 bra 	$L__BB29_37;

	max.f32 	%f36, %f35, %f36;
	mov.u16 	%rs32, %rs31;

$L__BB29_37:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p19, %rd50, %rd5;
	mov.f32 	%f37, %f36;
	mov.u16 	%rs33, %rs32;
	@%p19 bra 	$L__BB29_40;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd168;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 2;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.f32 	%f37, [%rd126];
	and.b16  	%rs21, %rs32, 255;
	setp.eq.s16 	%p20, %rs21, 0;
	mov.u16 	%rs33, 1;
	@%p20 bra 	$L__BB29_40;

	max.f32 	%f37, %f36, %f37;
	mov.u16 	%rs33, %rs32;

$L__BB29_40:
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, -4;
	setp.ne.s64 	%p21, %rd174, 0;
	@%p21 bra 	$L__BB29_28;

$L__BB29_41:
	setp.eq.s64 	%p22, %rd40, 0;
	mov.f32 	%f42, %f37;
	mov.u16 	%rs38, %rs33;
	@%p22 bra 	$L__BB29_53;

	add.s64 	%rd54, %rd175, %rd38;
	setp.ge.u64 	%p23, %rd54, %rd5;
	mov.f32 	%f42, %f37;
	mov.u16 	%rs38, %rs33;
	@%p23 bra 	$L__BB29_45;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd168;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.f32 	%f42, [%rd137];
	and.b16  	%rs23, %rs33, 255;
	setp.eq.s16 	%p24, %rs23, 0;
	mov.u16 	%rs38, 1;
	@%p24 bra 	$L__BB29_45;

	max.f32 	%f42, %f37, %f42;
	mov.u16 	%rs38, %rs33;

$L__BB29_45:
	setp.eq.s64 	%p25, %rd40, 1;
	@%p25 bra 	$L__BB29_53;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p26, %rd55, %rd5;
	mov.f32 	%f41, %f42;
	mov.u16 	%rs37, %rs38;
	@%p26 bra 	$L__BB29_49;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd168;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 2;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.f32 	%f41, [%rd148];
	and.b16  	%rs25, %rs38, 255;
	setp.eq.s16 	%p27, %rs25, 0;
	mov.u16 	%rs37, 1;
	@%p27 bra 	$L__BB29_49;

	max.f32 	%f41, %f42, %f41;
	mov.u16 	%rs37, %rs38;

$L__BB29_49:
	setp.eq.s64 	%p28, %rd40, 2;
	mov.f32 	%f42, %f41;
	mov.u16 	%rs38, %rs37;
	@%p28 bra 	$L__BB29_53;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p29, %rd56, %rd5;
	mov.f32 	%f42, %f41;
	mov.u16 	%rs38, %rs37;
	@%p29 bra 	$L__BB29_53;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd168;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 2;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.f32 	%f42, [%rd159];
	and.b16  	%rs27, %rs37, 255;
	setp.eq.s16 	%p30, %rs27, 0;
	mov.u16 	%rs38, 1;
	@%p30 bra 	$L__BB29_53;

	max.f32 	%f42, %f41, %f42;
	mov.u16 	%rs38, %rs37;

$L__BB29_53:
	add.s64 	%rd172, %rd172, 1;
	setp.lt.u64 	%p31, %rd172, %rd58;
	@%p31 bra 	$L__BB29_25;

$L__BB29_54:
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	cvt.u64.u32 	%rd163, %r28;
	cvta.to.global.u64 	%rd160, %rd62;
	shl.b64 	%rd161, %rd163, 2;
	add.s64 	%rd162, %rd160, %rd161;
	st.global.f32 	[%rd162], %f42;

$L__BB29_55:
	ret;

}
	// .globl	max_pool2d_f64
.visible .entry max_pool2d_f64(
	.param .u64 max_pool2d_f64_param_0,
	.param .u64 max_pool2d_f64_param_1,
	.param .u64 max_pool2d_f64_param_2,
	.param .u64 max_pool2d_f64_param_3,
	.param .u64 max_pool2d_f64_param_4,
	.param .u64 max_pool2d_f64_param_5,
	.param .u64 max_pool2d_f64_param_6,
	.param .u64 max_pool2d_f64_param_7
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<39>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<44>;
	.reg .b64 	%rd<190>;


	ld.param.u64 	%rd58, [max_pool2d_f64_param_1];
	ld.param.u64 	%rd59, [max_pool2d_f64_param_2];
	ld.param.u64 	%rd60, [max_pool2d_f64_param_3];
	ld.param.u64 	%rd61, [max_pool2d_f64_param_4];
	ld.param.u64 	%rd63, [max_pool2d_f64_param_5];
	ld.param.u64 	%rd64, [max_pool2d_f64_param_6];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB30_2;

	div.u64 	%rd178, %rd7, %rd60;
	bra.uni 	$L__BB30_3;

$L__BB30_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd178, %r7;

$L__BB30_3:
	add.s64 	%rd11, %rd178, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB30_5;

	div.u64 	%rd179, %rd12, %rd61;
	bra.uni 	$L__BB30_6;

$L__BB30_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd179, %r10;

$L__BB30_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd179, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB30_55;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB30_9;

	div.u64 	%rd180, %rd2, %rd18;
	bra.uni 	$L__BB30_10;

$L__BB30_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd180, %r13;

$L__BB30_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB30_12;

	div.u64 	%rd181, %rd2, %rd17;
	bra.uni 	$L__BB30_13;

$L__BB30_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd181, %r16;

$L__BB30_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB30_15;

	rem.u64 	%rd182, %rd181, %rd4;
	bra.uni 	$L__BB30_16;

$L__BB30_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd181;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd182, %r19;

$L__BB30_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB30_18;

	div.u64 	%rd183, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd183, %rd16;
	sub.s64 	%rd184, %rd2, %rd77;
	bra.uni 	$L__BB30_19;

$L__BB30_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd183, %r22;
	cvt.u64.u32 	%rd184, %r24;

$L__BB30_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB30_21;

	rem.u64 	%rd185, %rd183, %rd11;
	bra.uni 	$L__BB30_22;

$L__BB30_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd183;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd185, %r27;

$L__BB30_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd180;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.f64 	%fd42, 0d0000000000000000;
	@%p9 bra 	$L__BB30_54;

	ld.param.u64 	%rd176, [max_pool2d_f64_param_4];
	mul.lo.s64 	%rd38, %rd184, %rd176;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB30_54;

	ld.param.u64 	%rd177, [max_pool2d_f64_param_3];
	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd185, %rd177;
	mov.f64 	%fd42, 0d0000000000000000;
	mov.u16 	%rs38, 0;
	mov.u64 	%rd186, 0;

$L__BB30_25:
	add.s64 	%rd44, %rd186, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB30_53;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd189, 0;
	mov.u16 	%rs33, %rs38;
	mov.f64 	%fd37, %fd42;
	@%p12 bra 	$L__BB30_41;

	mov.u64 	%rd189, 0;
	mov.u16 	%rs33, %rs38;
	mov.f64 	%fd37, %fd42;
	mov.u64 	%rd188, %rd41;

$L__BB30_28:
	add.s64 	%rd47, %rd189, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	mov.f64 	%fd34, %fd37;
	mov.u16 	%rs30, %rs33;
	@%p13 bra 	$L__BB30_31;

	add.s64 	%rd170, %rd186, %rd42;
	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd182;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd170;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 3;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.f64 	%fd34, [%rd93];
	and.b16  	%rs15, %rs33, 255;
	setp.eq.s16 	%p14, %rs15, 0;
	mov.u16 	%rs30, 1;
	@%p14 bra 	$L__BB30_31;

	max.f64 	%fd34, %fd37, %fd34;
	mov.u16 	%rs30, %rs33;

$L__BB30_31:
	add.s64 	%rd166, %rd189, %rd38;
	add.s64 	%rd48, %rd166, 1;
	setp.ge.u64 	%p15, %rd48, %rd5;
	mov.f64 	%fd35, %fd34;
	mov.u16 	%rs31, %rs30;
	@%p15 bra 	$L__BB30_34;

	add.s64 	%rd169, %rd186, %rd42;
	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd182;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd169;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 3;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f64 	%fd35, [%rd104];
	and.b16  	%rs17, %rs30, 255;
	setp.eq.s16 	%p16, %rs17, 0;
	mov.u16 	%rs31, 1;
	@%p16 bra 	$L__BB30_34;

	max.f64 	%fd35, %fd34, %fd35;
	mov.u16 	%rs31, %rs30;

$L__BB30_34:
	add.s64 	%rd167, %rd189, %rd38;
	add.s64 	%rd49, %rd167, 2;
	setp.ge.u64 	%p17, %rd49, %rd5;
	mov.f64 	%fd36, %fd35;
	mov.u16 	%rs32, %rs31;
	@%p17 bra 	$L__BB30_37;

	add.s64 	%rd175, %rd186, %rd42;
	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd182;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd175;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 3;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.f64 	%fd36, [%rd115];
	and.b16  	%rs19, %rs31, 255;
	setp.eq.s16 	%p18, %rs19, 0;
	mov.u16 	%rs32, 1;
	@%p18 bra 	$L__BB30_37;

	max.f64 	%fd36, %fd35, %fd36;
	mov.u16 	%rs32, %rs31;

$L__BB30_37:
	add.s64 	%rd168, %rd189, %rd38;
	add.s64 	%rd50, %rd168, 3;
	setp.ge.u64 	%p19, %rd50, %rd5;
	mov.f64 	%fd37, %fd36;
	mov.u16 	%rs33, %rs32;
	@%p19 bra 	$L__BB30_40;

	add.s64 	%rd174, %rd186, %rd42;
	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd182;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd174;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 3;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.f64 	%fd37, [%rd126];
	and.b16  	%rs21, %rs32, 255;
	setp.eq.s16 	%p20, %rs21, 0;
	mov.u16 	%rs33, 1;
	@%p20 bra 	$L__BB30_40;

	max.f64 	%fd37, %fd36, %fd37;
	mov.u16 	%rs33, %rs32;

$L__BB30_40:
	add.s64 	%rd189, %rd189, 4;
	add.s64 	%rd188, %rd188, -4;
	setp.ne.s64 	%p21, %rd188, 0;
	@%p21 bra 	$L__BB30_28;

$L__BB30_41:
	setp.eq.s64 	%p22, %rd40, 0;
	mov.f64 	%fd42, %fd37;
	mov.u16 	%rs38, %rs33;
	@%p22 bra 	$L__BB30_53;

	add.s64 	%rd54, %rd189, %rd38;
	setp.ge.u64 	%p23, %rd54, %rd5;
	mov.f64 	%fd42, %fd37;
	mov.u16 	%rs38, %rs33;
	@%p23 bra 	$L__BB30_45;

	add.s64 	%rd173, %rd186, %rd42;
	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd182;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd173;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 3;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.f64 	%fd42, [%rd137];
	and.b16  	%rs23, %rs33, 255;
	setp.eq.s16 	%p24, %rs23, 0;
	mov.u16 	%rs38, 1;
	@%p24 bra 	$L__BB30_45;

	max.f64 	%fd42, %fd37, %fd42;
	mov.u16 	%rs38, %rs33;

$L__BB30_45:
	setp.eq.s64 	%p25, %rd40, 1;
	@%p25 bra 	$L__BB30_53;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p26, %rd55, %rd5;
	mov.f64 	%fd41, %fd42;
	mov.u16 	%rs37, %rs38;
	@%p26 bra 	$L__BB30_49;

	add.s64 	%rd172, %rd186, %rd42;
	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd182;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd172;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 3;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.f64 	%fd41, [%rd148];
	and.b16  	%rs25, %rs38, 255;
	setp.eq.s16 	%p27, %rs25, 0;
	mov.u16 	%rs37, 1;
	@%p27 bra 	$L__BB30_49;

	max.f64 	%fd41, %fd42, %fd41;
	mov.u16 	%rs37, %rs38;

$L__BB30_49:
	setp.eq.s64 	%p28, %rd40, 2;
	mov.f64 	%fd42, %fd41;
	mov.u16 	%rs38, %rs37;
	@%p28 bra 	$L__BB30_53;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p29, %rd56, %rd5;
	mov.f64 	%fd42, %fd41;
	mov.u16 	%rs38, %rs37;
	@%p29 bra 	$L__BB30_53;

	add.s64 	%rd171, %rd186, %rd42;
	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd182;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd171;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 3;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.f64 	%fd42, [%rd159];
	and.b16  	%rs27, %rs37, 255;
	setp.eq.s16 	%p30, %rs27, 0;
	mov.u16 	%rs38, 1;
	@%p30 bra 	$L__BB30_53;

	max.f64 	%fd42, %fd41, %fd42;
	mov.u16 	%rs38, %rs37;

$L__BB30_53:
	ld.param.u64 	%rd163, [max_pool2d_f64_param_1];
	add.s64 	%rd186, %rd186, 1;
	setp.lt.u64 	%p31, %rd186, %rd163;
	@%p31 bra 	$L__BB30_25;

$L__BB30_54:
	ld.param.u64 	%rd165, [max_pool2d_f64_param_7];
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	cvt.u64.u32 	%rd164, %r28;
	cvta.to.global.u64 	%rd160, %rd165;
	shl.b64 	%rd161, %rd164, 3;
	add.s64 	%rd162, %rd160, %rd161;
	st.global.f64 	[%rd162], %fd42;

$L__BB30_55:
	ret;

}
	// .globl	max_pool2d_u8
.visible .entry max_pool2d_u8(
	.param .u64 max_pool2d_u8_param_0,
	.param .u64 max_pool2d_u8_param_1,
	.param .u64 max_pool2d_u8_param_2,
	.param .u64 max_pool2d_u8_param_3,
	.param .u64 max_pool2d_u8_param_4,
	.param .u64 max_pool2d_u8_param_5,
	.param .u64 max_pool2d_u8_param_6,
	.param .u64 max_pool2d_u8_param_7
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<80>;
	.reg .b32 	%r<67>;
	.reg .b64 	%rd<168>;


	ld.param.u64 	%rd58, [max_pool2d_u8_param_1];
	ld.param.u64 	%rd59, [max_pool2d_u8_param_2];
	ld.param.u64 	%rd60, [max_pool2d_u8_param_3];
	ld.param.u64 	%rd61, [max_pool2d_u8_param_4];
	ld.param.u64 	%rd63, [max_pool2d_u8_param_5];
	ld.param.u64 	%rd64, [max_pool2d_u8_param_6];
	ld.param.u64 	%rd62, [max_pool2d_u8_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r4;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB31_2;

	div.u64 	%rd156, %rd7, %rd60;
	bra.uni 	$L__BB31_3;

$L__BB31_2:
	cvt.u32.u64 	%r5, %rd60;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd156, %r7;

$L__BB31_3:
	add.s64 	%rd11, %rd156, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB31_5;

	div.u64 	%rd157, %rd12, %rd61;
	bra.uni 	$L__BB31_6;

$L__BB31_5:
	cvt.u32.u64 	%r8, %rd61;
	cvt.u32.u64 	%r9, %rd12;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd157, %r10;

$L__BB31_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd157, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB31_55;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB31_9;

	div.u64 	%rd158, %rd2, %rd18;
	bra.uni 	$L__BB31_10;

$L__BB31_9:
	cvt.u32.u64 	%r11, %rd18;
	cvt.u32.u64 	%r12, %rd2;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd158, %r13;

$L__BB31_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB31_12;

	div.u64 	%rd159, %rd2, %rd17;
	bra.uni 	$L__BB31_13;

$L__BB31_12:
	cvt.u32.u64 	%r14, %rd17;
	cvt.u32.u64 	%r15, %rd2;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd159, %r16;

$L__BB31_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB31_15;

	rem.u64 	%rd160, %rd159, %rd4;
	bra.uni 	$L__BB31_16;

$L__BB31_15:
	cvt.u32.u64 	%r17, %rd4;
	cvt.u32.u64 	%r18, %rd159;
	rem.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd160, %r19;

$L__BB31_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB31_18;

	div.u64 	%rd161, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd161, %rd16;
	sub.s64 	%rd162, %rd2, %rd77;
	bra.uni 	$L__BB31_19;

$L__BB31_18:
	cvt.u32.u64 	%r20, %rd16;
	cvt.u32.u64 	%r21, %rd2;
	div.u32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, %r20;
	sub.s32 	%r24, %r21, %r23;
	cvt.u64.u32 	%rd161, %r22;
	cvt.u64.u32 	%rd162, %r24;

$L__BB31_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB31_21;

	rem.u64 	%rd163, %rd161, %rd11;
	bra.uni 	$L__BB31_22;

$L__BB31_21:
	cvt.u32.u64 	%r25, %rd11;
	cvt.u32.u64 	%r26, %rd161;
	rem.u32 	%r27, %r26, %r25;
	cvt.u64.u32 	%rd163, %r27;

$L__BB31_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd158;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.u16 	%rs77, 0;
	@%p9 bra 	$L__BB31_54;

	mul.lo.s64 	%rd38, %rd162, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB31_54;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd163, %rd60;
	mov.u16 	%rs78, 0;
	mov.u64 	%rd164, 0;
	mov.u16 	%rs77, %rs78;

$L__BB31_25:
	add.s64 	%rd44, %rd164, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB31_53;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd167, 0;
	mov.u16 	%rs68, %rs78;
	mov.u16 	%rs67, %rs77;
	@%p12 bra 	$L__BB31_41;

	mov.u64 	%rd167, 0;
	mov.u16 	%rs68, %rs78;
	mov.u16 	%rs67, %rs77;
	mov.u64 	%rd166, %rd41;

$L__BB31_28:
	add.s64 	%rd47, %rd167, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	mov.u16 	%rs61, %rs67;
	mov.u16 	%rs62, %rs68;
	@%p13 bra 	$L__BB31_31;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd160;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	add.s64 	%rd92, %rd1, %rd91;
	ld.global.u8 	%rs61, [%rd92];
	and.b16  	%rs44, %rs68, 255;
	setp.eq.s16 	%p14, %rs44, 0;
	mov.u16 	%rs62, 1;
	@%p14 bra 	$L__BB31_31;

	cvt.u32.u16 	%r28, %rs61;
	and.b32  	%r29, %r28, 255;
	cvt.u32.u16 	%r30, %rs67;
	and.b32  	%r31, %r30, 255;
	max.u32 	%r32, %r31, %r29;
	cvt.u16.u32 	%rs61, %r32;
	mov.u16 	%rs62, %rs68;

$L__BB31_31:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p15, %rd48, %rd5;
	mov.u16 	%rs63, %rs61;
	mov.u16 	%rs64, %rs62;
	@%p15 bra 	$L__BB31_34;

	ld.global.u64 	%rd93, [%rd3+40];
	mul.lo.s64 	%rd94, %rd93, %rd160;
	add.s64 	%rd95, %rd94, %rd37;
	ld.global.u64 	%rd96, [%rd3+48];
	mul.lo.s64 	%rd97, %rd96, %rd44;
	add.s64 	%rd98, %rd95, %rd97;
	ld.global.u64 	%rd99, [%rd3+56];
	mul.lo.s64 	%rd100, %rd99, %rd48;
	add.s64 	%rd101, %rd98, %rd100;
	add.s64 	%rd102, %rd1, %rd101;
	ld.global.u8 	%rs63, [%rd102];
	and.b16  	%rs46, %rs62, 255;
	setp.eq.s16 	%p16, %rs46, 0;
	mov.u16 	%rs64, 1;
	@%p16 bra 	$L__BB31_34;

	cvt.u32.u16 	%r33, %rs63;
	and.b32  	%r34, %r33, 255;
	cvt.u32.u16 	%r35, %rs61;
	and.b32  	%r36, %r35, 255;
	max.u32 	%r37, %r36, %r34;
	cvt.u16.u32 	%rs63, %r37;
	mov.u16 	%rs64, %rs62;

$L__BB31_34:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p17, %rd49, %rd5;
	mov.u16 	%rs65, %rs63;
	mov.u16 	%rs66, %rs64;
	@%p17 bra 	$L__BB31_37;

	ld.global.u64 	%rd103, [%rd3+40];
	mul.lo.s64 	%rd104, %rd103, %rd160;
	add.s64 	%rd105, %rd104, %rd37;
	ld.global.u64 	%rd106, [%rd3+48];
	mul.lo.s64 	%rd107, %rd106, %rd44;
	add.s64 	%rd108, %rd105, %rd107;
	ld.global.u64 	%rd109, [%rd3+56];
	mul.lo.s64 	%rd110, %rd109, %rd49;
	add.s64 	%rd111, %rd108, %rd110;
	add.s64 	%rd112, %rd1, %rd111;
	ld.global.u8 	%rs65, [%rd112];
	and.b16  	%rs48, %rs64, 255;
	setp.eq.s16 	%p18, %rs48, 0;
	mov.u16 	%rs66, 1;
	@%p18 bra 	$L__BB31_37;

	cvt.u32.u16 	%r38, %rs65;
	and.b32  	%r39, %r38, 255;
	cvt.u32.u16 	%r40, %rs63;
	and.b32  	%r41, %r40, 255;
	max.u32 	%r42, %r41, %r39;
	cvt.u16.u32 	%rs65, %r42;
	mov.u16 	%rs66, %rs64;

$L__BB31_37:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p19, %rd50, %rd5;
	mov.u16 	%rs67, %rs65;
	mov.u16 	%rs68, %rs66;
	@%p19 bra 	$L__BB31_40;

	ld.global.u64 	%rd113, [%rd3+40];
	mul.lo.s64 	%rd114, %rd113, %rd160;
	add.s64 	%rd115, %rd114, %rd37;
	ld.global.u64 	%rd116, [%rd3+48];
	mul.lo.s64 	%rd117, %rd116, %rd44;
	add.s64 	%rd118, %rd115, %rd117;
	ld.global.u64 	%rd119, [%rd3+56];
	mul.lo.s64 	%rd120, %rd119, %rd50;
	add.s64 	%rd121, %rd118, %rd120;
	add.s64 	%rd122, %rd1, %rd121;
	ld.global.u8 	%rs67, [%rd122];
	and.b16  	%rs50, %rs66, 255;
	setp.eq.s16 	%p20, %rs50, 0;
	mov.u16 	%rs68, 1;
	@%p20 bra 	$L__BB31_40;

	cvt.u32.u16 	%r43, %rs67;
	and.b32  	%r44, %r43, 255;
	cvt.u32.u16 	%r45, %rs65;
	and.b32  	%r46, %r45, 255;
	max.u32 	%r47, %r46, %r44;
	cvt.u16.u32 	%rs67, %r47;
	mov.u16 	%rs68, %rs66;

$L__BB31_40:
	add.s64 	%rd167, %rd167, 4;
	add.s64 	%rd166, %rd166, -4;
	setp.ne.s64 	%p21, %rd166, 0;
	@%p21 bra 	$L__BB31_28;

$L__BB31_41:
	setp.eq.s64 	%p22, %rd40, 0;
	mov.u16 	%rs77, %rs67;
	mov.u16 	%rs78, %rs68;
	@%p22 bra 	$L__BB31_53;

	add.s64 	%rd54, %rd167, %rd38;
	setp.ge.u64 	%p23, %rd54, %rd5;
	mov.u16 	%rs77, %rs67;
	mov.u16 	%rs78, %rs68;
	@%p23 bra 	$L__BB31_45;

	ld.global.u64 	%rd123, [%rd3+40];
	mul.lo.s64 	%rd124, %rd123, %rd160;
	add.s64 	%rd125, %rd124, %rd37;
	ld.global.u64 	%rd126, [%rd3+48];
	mul.lo.s64 	%rd127, %rd126, %rd44;
	add.s64 	%rd128, %rd125, %rd127;
	ld.global.u64 	%rd129, [%rd3+56];
	mul.lo.s64 	%rd130, %rd129, %rd54;
	add.s64 	%rd131, %rd128, %rd130;
	add.s64 	%rd132, %rd1, %rd131;
	ld.global.u8 	%rs77, [%rd132];
	and.b16  	%rs52, %rs68, 255;
	setp.eq.s16 	%p24, %rs52, 0;
	mov.u16 	%rs78, 1;
	@%p24 bra 	$L__BB31_45;

	cvt.u32.u16 	%r48, %rs77;
	and.b32  	%r49, %r48, 255;
	cvt.u32.u16 	%r50, %rs67;
	and.b32  	%r51, %r50, 255;
	max.u32 	%r52, %r51, %r49;
	cvt.u16.u32 	%rs77, %r52;
	mov.u16 	%rs78, %rs68;

$L__BB31_45:
	setp.eq.s64 	%p25, %rd40, 1;
	@%p25 bra 	$L__BB31_53;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p26, %rd55, %rd5;
	mov.u16 	%rs75, %rs77;
	mov.u16 	%rs76, %rs78;
	@%p26 bra 	$L__BB31_49;

	ld.global.u64 	%rd133, [%rd3+40];
	mul.lo.s64 	%rd134, %rd133, %rd160;
	add.s64 	%rd135, %rd134, %rd37;
	ld.global.u64 	%rd136, [%rd3+48];
	mul.lo.s64 	%rd137, %rd136, %rd44;
	add.s64 	%rd138, %rd135, %rd137;
	ld.global.u64 	%rd139, [%rd3+56];
	mul.lo.s64 	%rd140, %rd139, %rd55;
	add.s64 	%rd141, %rd138, %rd140;
	add.s64 	%rd142, %rd1, %rd141;
	ld.global.u8 	%rs75, [%rd142];
	and.b16  	%rs54, %rs78, 255;
	setp.eq.s16 	%p27, %rs54, 0;
	mov.u16 	%rs76, 1;
	@%p27 bra 	$L__BB31_49;

	cvt.u32.u16 	%r53, %rs75;
	and.b32  	%r54, %r53, 255;
	cvt.u32.u16 	%r55, %rs77;
	and.b32  	%r56, %r55, 255;
	max.u32 	%r57, %r56, %r54;
	cvt.u16.u32 	%rs75, %r57;
	mov.u16 	%rs76, %rs78;

$L__BB31_49:
	setp.eq.s64 	%p28, %rd40, 2;
	mov.u16 	%rs77, %rs75;
	mov.u16 	%rs78, %rs76;
	@%p28 bra 	$L__BB31_53;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p29, %rd56, %rd5;
	mov.u16 	%rs77, %rs75;
	mov.u16 	%rs78, %rs76;
	@%p29 bra 	$L__BB31_53;

	ld.global.u64 	%rd143, [%rd3+40];
	mul.lo.s64 	%rd144, %rd143, %rd160;
	add.s64 	%rd145, %rd144, %rd37;
	ld.global.u64 	%rd146, [%rd3+48];
	mul.lo.s64 	%rd147, %rd146, %rd44;
	add.s64 	%rd148, %rd145, %rd147;
	ld.global.u64 	%rd149, [%rd3+56];
	mul.lo.s64 	%rd150, %rd149, %rd56;
	add.s64 	%rd151, %rd148, %rd150;
	add.s64 	%rd152, %rd1, %rd151;
	ld.global.u8 	%rs77, [%rd152];
	and.b16  	%rs56, %rs76, 255;
	setp.eq.s16 	%p30, %rs56, 0;
	mov.u16 	%rs78, 1;
	@%p30 bra 	$L__BB31_53;

	cvt.u32.u16 	%r58, %rs77;
	and.b32  	%r59, %r58, 255;
	cvt.u32.u16 	%r60, %rs75;
	and.b32  	%r61, %r60, 255;
	max.u32 	%r62, %r61, %r59;
	cvt.u16.u32 	%rs77, %r62;
	mov.u16 	%rs78, %rs76;

$L__BB31_53:
	add.s64 	%rd164, %rd164, 1;
	setp.lt.u64 	%p31, %rd164, %rd58;
	@%p31 bra 	$L__BB31_25;

$L__BB31_54:
	mov.u32 	%r66, %tid.x;
	mov.u32 	%r65, %ntid.x;
	mov.u32 	%r64, %ctaid.x;
	mad.lo.s32 	%r63, %r64, %r65, %r66;
	cvt.u64.u32 	%rd155, %r63;
	cvta.to.global.u64 	%rd153, %rd62;
	add.s64 	%rd154, %rd153, %rd155;
	st.global.u8 	[%rd154], %rs77;

$L__BB31_55:
	ret;

}
	// .globl	max_pool2d_u32
.visible .entry max_pool2d_u32(
	.param .u64 max_pool2d_u32_param_0,
	.param .u64 max_pool2d_u32_param_1,
	.param .u64 max_pool2d_u32_param_2,
	.param .u64 max_pool2d_u32_param_3,
	.param .u64 max_pool2d_u32_param_4,
	.param .u64 max_pool2d_u32_param_5,
	.param .u64 max_pool2d_u32_param_6,
	.param .u64 max_pool2d_u32_param_7
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<39>;
	.reg .b32 	%r<74>;
	.reg .b64 	%rd<176>;


	ld.param.u64 	%rd58, [max_pool2d_u32_param_1];
	ld.param.u64 	%rd59, [max_pool2d_u32_param_2];
	ld.param.u64 	%rd60, [max_pool2d_u32_param_3];
	ld.param.u64 	%rd61, [max_pool2d_u32_param_4];
	ld.param.u64 	%rd63, [max_pool2d_u32_param_5];
	ld.param.u64 	%rd64, [max_pool2d_u32_param_6];
	ld.param.u64 	%rd62, [max_pool2d_u32_param_7];
	cvta.to.global.u64 	%rd1, %rd64;
	mov.u32 	%r27, %ntid.x;
	mov.u32 	%r28, %ctaid.x;
	mov.u32 	%r29, %tid.x;
	mad.lo.s32 	%r30, %r28, %r27, %r29;
	cvt.u64.u32 	%rd2, %r30;
	cvta.to.global.u64 	%rd3, %rd63;
	ld.global.u64 	%rd4, [%rd3+8];
	ld.global.u64 	%rd5, [%rd3+24];
	ld.global.u64 	%rd6, [%rd3+16];
	sub.s64 	%rd7, %rd6, %rd58;
	or.b64  	%rd65, %rd7, %rd60;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p1, %rd66, 0;
	@%p1 bra 	$L__BB32_2;

	div.u64 	%rd164, %rd7, %rd60;
	bra.uni 	$L__BB32_3;

$L__BB32_2:
	cvt.u32.u64 	%r31, %rd60;
	cvt.u32.u64 	%r32, %rd7;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd164, %r33;

$L__BB32_3:
	add.s64 	%rd11, %rd164, 1;
	sub.s64 	%rd12, %rd5, %rd59;
	or.b64  	%rd67, %rd12, %rd61;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p2, %rd68, 0;
	@%p2 bra 	$L__BB32_5;

	div.u64 	%rd165, %rd12, %rd61;
	bra.uni 	$L__BB32_6;

$L__BB32_5:
	cvt.u32.u64 	%r34, %rd61;
	cvt.u32.u64 	%r35, %rd12;
	div.u32 	%r36, %r35, %r34;
	cvt.u64.u32 	%rd165, %r36;

$L__BB32_6:
	mul.lo.s64 	%rd69, %rd11, %rd4;
	ld.global.u64 	%rd70, [%rd3];
	mul.lo.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd16, %rd165, 1;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	setp.le.u64 	%p3, %rd72, %rd2;
	@%p3 bra 	$L__BB32_55;

	mul.lo.s64 	%rd17, %rd16, %rd11;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	and.b64  	%rd73, %rd18, -4294967296;
	setp.eq.s64 	%p4, %rd73, 0;
	@%p4 bra 	$L__BB32_9;

	div.u64 	%rd166, %rd2, %rd18;
	bra.uni 	$L__BB32_10;

$L__BB32_9:
	cvt.u32.u64 	%r37, %rd18;
	cvt.u32.u64 	%r38, %rd2;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd166, %r39;

$L__BB32_10:
	and.b64  	%rd74, %rd17, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB32_12;

	div.u64 	%rd167, %rd2, %rd17;
	bra.uni 	$L__BB32_13;

$L__BB32_12:
	cvt.u32.u64 	%r40, %rd17;
	cvt.u32.u64 	%r41, %rd2;
	div.u32 	%r42, %r41, %r40;
	cvt.u64.u32 	%rd167, %r42;

$L__BB32_13:
	and.b64  	%rd75, %rd4, -4294967296;
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB32_15;

	rem.u64 	%rd168, %rd167, %rd4;
	bra.uni 	$L__BB32_16;

$L__BB32_15:
	cvt.u32.u64 	%r43, %rd4;
	cvt.u32.u64 	%r44, %rd167;
	rem.u32 	%r45, %r44, %r43;
	cvt.u64.u32 	%rd168, %r45;

$L__BB32_16:
	and.b64  	%rd76, %rd16, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB32_18;

	div.u64 	%rd169, %rd2, %rd16;
	mul.lo.s64 	%rd77, %rd169, %rd16;
	sub.s64 	%rd170, %rd2, %rd77;
	bra.uni 	$L__BB32_19;

$L__BB32_18:
	cvt.u32.u64 	%r46, %rd16;
	cvt.u32.u64 	%r47, %rd2;
	div.u32 	%r48, %r47, %r46;
	mul.lo.s32 	%r49, %r48, %r46;
	sub.s32 	%r50, %r47, %r49;
	cvt.u64.u32 	%rd169, %r48;
	cvt.u64.u32 	%rd170, %r50;

$L__BB32_19:
	and.b64  	%rd78, %rd11, -4294967296;
	setp.eq.s64 	%p8, %rd78, 0;
	@%p8 bra 	$L__BB32_21;

	rem.u64 	%rd171, %rd169, %rd11;
	bra.uni 	$L__BB32_22;

$L__BB32_21:
	cvt.u32.u64 	%r51, %rd11;
	cvt.u32.u64 	%r52, %rd169;
	rem.u32 	%r53, %r52, %r51;
	cvt.u64.u32 	%rd171, %r53;

$L__BB32_22:
	ld.global.u64 	%rd79, [%rd3+32];
	mul.lo.s64 	%rd37, %rd79, %rd166;
	setp.eq.s64 	%p9, %rd58, 0;
	mov.u32 	%r72, 0;
	@%p9 bra 	$L__BB32_54;

	mul.lo.s64 	%rd38, %rd170, %rd61;
	setp.eq.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB32_54;

	add.s64 	%rd39, %rd59, -1;
	and.b64  	%rd40, %rd59, 3;
	sub.s64 	%rd41, %rd59, %rd40;
	mul.lo.s64 	%rd42, %rd171, %rd60;
	mov.u32 	%r72, 0;
	mov.u16 	%rs38, 0;
	mov.u64 	%rd172, 0;

$L__BB32_25:
	add.s64 	%rd44, %rd172, %rd42;
	setp.ge.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB32_53;

	setp.lt.u64 	%p12, %rd39, 3;
	mov.u64 	%rd175, 0;
	mov.u16 	%rs33, %rs38;
	mov.u32 	%r67, %r72;
	@%p12 bra 	$L__BB32_41;

	mov.u64 	%rd175, 0;
	mov.u16 	%rs33, %rs38;
	mov.u32 	%r67, %r72;
	mov.u64 	%rd174, %rd41;

$L__BB32_28:
	add.s64 	%rd47, %rd175, %rd38;
	setp.ge.u64 	%p13, %rd47, %rd5;
	mov.u32 	%r64, %r67;
	mov.u16 	%rs30, %rs33;
	@%p13 bra 	$L__BB32_31;

	ld.global.u64 	%rd83, [%rd3+40];
	mul.lo.s64 	%rd84, %rd83, %rd168;
	add.s64 	%rd85, %rd84, %rd37;
	ld.global.u64 	%rd86, [%rd3+48];
	mul.lo.s64 	%rd87, %rd86, %rd44;
	add.s64 	%rd88, %rd85, %rd87;
	ld.global.u64 	%rd89, [%rd3+56];
	mul.lo.s64 	%rd90, %rd89, %rd47;
	add.s64 	%rd91, %rd88, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.u32 	%r64, [%rd93];
	and.b16  	%rs15, %rs33, 255;
	setp.eq.s16 	%p14, %rs15, 0;
	mov.u16 	%rs30, 1;
	@%p14 bra 	$L__BB32_31;

	max.u32 	%r64, %r67, %r64;
	mov.u16 	%rs30, %rs33;

$L__BB32_31:
	add.s64 	%rd48, %rd47, 1;
	setp.ge.u64 	%p15, %rd48, %rd5;
	mov.u32 	%r65, %r64;
	mov.u16 	%rs31, %rs30;
	@%p15 bra 	$L__BB32_34;

	ld.global.u64 	%rd94, [%rd3+40];
	mul.lo.s64 	%rd95, %rd94, %rd168;
	add.s64 	%rd96, %rd95, %rd37;
	ld.global.u64 	%rd97, [%rd3+48];
	mul.lo.s64 	%rd98, %rd97, %rd44;
	add.s64 	%rd99, %rd96, %rd98;
	ld.global.u64 	%rd100, [%rd3+56];
	mul.lo.s64 	%rd101, %rd100, %rd48;
	add.s64 	%rd102, %rd99, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.u32 	%r65, [%rd104];
	and.b16  	%rs17, %rs30, 255;
	setp.eq.s16 	%p16, %rs17, 0;
	mov.u16 	%rs31, 1;
	@%p16 bra 	$L__BB32_34;

	max.u32 	%r65, %r64, %r65;
	mov.u16 	%rs31, %rs30;

$L__BB32_34:
	add.s64 	%rd49, %rd47, 2;
	setp.ge.u64 	%p17, %rd49, %rd5;
	mov.u32 	%r66, %r65;
	mov.u16 	%rs32, %rs31;
	@%p17 bra 	$L__BB32_37;

	ld.global.u64 	%rd105, [%rd3+40];
	mul.lo.s64 	%rd106, %rd105, %rd168;
	add.s64 	%rd107, %rd106, %rd37;
	ld.global.u64 	%rd108, [%rd3+48];
	mul.lo.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd110, %rd107, %rd109;
	ld.global.u64 	%rd111, [%rd3+56];
	mul.lo.s64 	%rd112, %rd111, %rd49;
	add.s64 	%rd113, %rd110, %rd112;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.u32 	%r66, [%rd115];
	and.b16  	%rs19, %rs31, 255;
	setp.eq.s16 	%p18, %rs19, 0;
	mov.u16 	%rs32, 1;
	@%p18 bra 	$L__BB32_37;

	max.u32 	%r66, %r65, %r66;
	mov.u16 	%rs32, %rs31;

$L__BB32_37:
	add.s64 	%rd50, %rd47, 3;
	setp.ge.u64 	%p19, %rd50, %rd5;
	mov.u32 	%r67, %r66;
	mov.u16 	%rs33, %rs32;
	@%p19 bra 	$L__BB32_40;

	ld.global.u64 	%rd116, [%rd3+40];
	mul.lo.s64 	%rd117, %rd116, %rd168;
	add.s64 	%rd118, %rd117, %rd37;
	ld.global.u64 	%rd119, [%rd3+48];
	mul.lo.s64 	%rd120, %rd119, %rd44;
	add.s64 	%rd121, %rd118, %rd120;
	ld.global.u64 	%rd122, [%rd3+56];
	mul.lo.s64 	%rd123, %rd122, %rd50;
	add.s64 	%rd124, %rd121, %rd123;
	shl.b64 	%rd125, %rd124, 2;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.u32 	%r67, [%rd126];
	and.b16  	%rs21, %rs32, 255;
	setp.eq.s16 	%p20, %rs21, 0;
	mov.u16 	%rs33, 1;
	@%p20 bra 	$L__BB32_40;

	max.u32 	%r67, %r66, %r67;
	mov.u16 	%rs33, %rs32;

$L__BB32_40:
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, -4;
	setp.ne.s64 	%p21, %rd174, 0;
	@%p21 bra 	$L__BB32_28;

$L__BB32_41:
	setp.eq.s64 	%p22, %rd40, 0;
	mov.u32 	%r72, %r67;
	mov.u16 	%rs38, %rs33;
	@%p22 bra 	$L__BB32_53;

	add.s64 	%rd54, %rd175, %rd38;
	setp.ge.u64 	%p23, %rd54, %rd5;
	mov.u32 	%r72, %r67;
	mov.u16 	%rs38, %rs33;
	@%p23 bra 	$L__BB32_45;

	ld.global.u64 	%rd127, [%rd3+40];
	mul.lo.s64 	%rd128, %rd127, %rd168;
	add.s64 	%rd129, %rd128, %rd37;
	ld.global.u64 	%rd130, [%rd3+48];
	mul.lo.s64 	%rd131, %rd130, %rd44;
	add.s64 	%rd132, %rd129, %rd131;
	ld.global.u64 	%rd133, [%rd3+56];
	mul.lo.s64 	%rd134, %rd133, %rd54;
	add.s64 	%rd135, %rd132, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.u32 	%r72, [%rd137];
	and.b16  	%rs23, %rs33, 255;
	setp.eq.s16 	%p24, %rs23, 0;
	mov.u16 	%rs38, 1;
	@%p24 bra 	$L__BB32_45;

	max.u32 	%r72, %r67, %r72;
	mov.u16 	%rs38, %rs33;

$L__BB32_45:
	setp.eq.s64 	%p25, %rd40, 1;
	@%p25 bra 	$L__BB32_53;

	add.s64 	%rd55, %rd54, 1;
	setp.ge.u64 	%p26, %rd55, %rd5;
	mov.u32 	%r71, %r72;
	mov.u16 	%rs37, %rs38;
	@%p26 bra 	$L__BB32_49;

	ld.global.u64 	%rd138, [%rd3+40];
	mul.lo.s64 	%rd139, %rd138, %rd168;
	add.s64 	%rd140, %rd139, %rd37;
	ld.global.u64 	%rd141, [%rd3+48];
	mul.lo.s64 	%rd142, %rd141, %rd44;
	add.s64 	%rd143, %rd140, %rd142;
	ld.global.u64 	%rd144, [%rd3+56];
	mul.lo.s64 	%rd145, %rd144, %rd55;
	add.s64 	%rd146, %rd143, %rd145;
	shl.b64 	%rd147, %rd146, 2;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.u32 	%r71, [%rd148];
	and.b16  	%rs25, %rs38, 255;
	setp.eq.s16 	%p27, %rs25, 0;
	mov.u16 	%rs37, 1;
	@%p27 bra 	$L__BB32_49;

	max.u32 	%r71, %r72, %r71;
	mov.u16 	%rs37, %rs38;

$L__BB32_49:
	setp.eq.s64 	%p28, %rd40, 2;
	mov.u32 	%r72, %r71;
	mov.u16 	%rs38, %rs37;
	@%p28 bra 	$L__BB32_53;

	add.s64 	%rd56, %rd54, 2;
	setp.ge.u64 	%p29, %rd56, %rd5;
	mov.u32 	%r72, %r71;
	mov.u16 	%rs38, %rs37;
	@%p29 bra 	$L__BB32_53;

	ld.global.u64 	%rd149, [%rd3+40];
	mul.lo.s64 	%rd150, %rd149, %rd168;
	add.s64 	%rd151, %rd150, %rd37;
	ld.global.u64 	%rd152, [%rd3+48];
	mul.lo.s64 	%rd153, %rd152, %rd44;
	add.s64 	%rd154, %rd151, %rd153;
	ld.global.u64 	%rd155, [%rd3+56];
	mul.lo.s64 	%rd156, %rd155, %rd56;
	add.s64 	%rd157, %rd154, %rd156;
	shl.b64 	%rd158, %rd157, 2;
	add.s64 	%rd159, %rd1, %rd158;
	ld.global.u32 	%r72, [%rd159];
	and.b16  	%rs27, %rs37, 255;
	setp.eq.s16 	%p30, %rs27, 0;
	mov.u16 	%rs38, 1;
	@%p30 bra 	$L__BB32_53;

	max.u32 	%r72, %r71, %r72;
	mov.u16 	%rs38, %rs37;

$L__BB32_53:
	add.s64 	%rd172, %rd172, 1;
	setp.lt.u64 	%p31, %rd172, %rd58;
	@%p31 bra 	$L__BB32_25;

$L__BB32_54:
	mov.u32 	%r61, %tid.x;
	mov.u32 	%r60, %ntid.x;
	mov.u32 	%r59, %ctaid.x;
	mad.lo.s32 	%r58, %r59, %r60, %r61;
	cvt.u64.u32 	%rd163, %r58;
	cvta.to.global.u64 	%rd160, %rd62;
	shl.b64 	%rd161, %rd163, 2;
	add.s64 	%rd162, %rd160, %rd161;
	st.global.u32 	[%rd162], %r72;

$L__BB32_55:
	ret;

}
	// .globl	upsample_nearest2d_f32
.visible .entry upsample_nearest2d_f32(
	.param .u64 upsample_nearest2d_f32_param_0,
	.param .u64 upsample_nearest2d_f32_param_1,
	.param .f64 upsample_nearest2d_f32_param_2,
	.param .f64 upsample_nearest2d_f32_param_3,
	.param .u64 upsample_nearest2d_f32_param_4,
	.param .u64 upsample_nearest2d_f32_param_5,
	.param .u64 upsample_nearest2d_f32_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd26, [upsample_nearest2d_f32_param_0];
	ld.param.u64 	%rd27, [upsample_nearest2d_f32_param_1];
	ld.param.f64 	%fd1, [upsample_nearest2d_f32_param_2];
	ld.param.f64 	%fd2, [upsample_nearest2d_f32_param_3];
	ld.param.u64 	%rd30, [upsample_nearest2d_f32_param_4];
	ld.param.u64 	%rd28, [upsample_nearest2d_f32_param_5];
	ld.param.u64 	%rd29, [upsample_nearest2d_f32_param_6];
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	mul.lo.s64 	%rd3, %rd27, %rd26;
	ld.global.u64 	%rd4, [%rd2+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd31, [%rd2];
	mul.lo.s64 	%rd32, %rd5, %rd31;
	setp.le.u64 	%p1, %rd32, %rd1;
	@%p1 bra 	$L__BB33_17;

	ld.global.u64 	%rd6, [%rd2+16];
	ld.global.u64 	%rd7, [%rd2+24];
	and.b64  	%rd33, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB33_3;

	div.u64 	%rd62, %rd1, %rd5;
	bra.uni 	$L__BB33_4;

$L__BB33_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd62, %r7;

$L__BB33_4:
	and.b64  	%rd34, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB33_6;

	div.u64 	%rd63, %rd1, %rd3;
	bra.uni 	$L__BB33_7;

$L__BB33_6:
	cvt.u32.u64 	%r8, %rd3;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd63, %r10;

$L__BB33_7:
	and.b64  	%rd35, %rd4, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB33_9;

	rem.u64 	%rd64, %rd63, %rd4;
	bra.uni 	$L__BB33_10;

$L__BB33_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd63;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd64, %r13;

$L__BB33_10:
	and.b64  	%rd36, %rd27, -4294967296;
	setp.eq.s64 	%p5, %rd36, 0;
	@%p5 bra 	$L__BB33_12;

	div.u64 	%rd65, %rd1, %rd27;
	mul.lo.s64 	%rd37, %rd65, %rd27;
	sub.s64 	%rd66, %rd1, %rd37;
	bra.uni 	$L__BB33_13;

$L__BB33_12:
	cvt.u32.u64 	%r14, %rd27;
	cvt.u32.u64 	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd65, %r16;
	cvt.u64.u32 	%rd66, %r18;

$L__BB33_13:
	and.b64  	%rd38, %rd26, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB33_15;

	rem.u64 	%rd67, %rd65, %rd26;
	bra.uni 	$L__BB33_16;

$L__BB33_15:
	cvt.u32.u64 	%r19, %rd26;
	cvt.u32.u64 	%r20, %rd65;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd67, %r21;

$L__BB33_16:
	cvt.rn.f64.u64 	%fd3, %rd67;
	mul.f64 	%fd4, %fd3, %fd1;
	cvt.rzi.u64.f64 	%rd39, %fd4;
	cvt.rn.f64.u64 	%fd5, %rd66;
	mul.f64 	%fd6, %fd5, %fd2;
	cvt.rzi.u64.f64 	%rd40, %fd6;
	setp.lt.u64 	%p7, %rd39, %rd6;
	add.s64 	%rd41, %rd6, -1;
	selp.b64 	%rd42, %rd39, %rd41, %p7;
	setp.lt.u64 	%p8, %rd40, %rd7;
	add.s64 	%rd43, %rd7, -1;
	selp.b64 	%rd44, %rd40, %rd43, %p8;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd62;
	ld.global.u64 	%rd47, [%rd2+40];
	mul.lo.s64 	%rd48, %rd47, %rd64;
	add.s64 	%rd49, %rd48, %rd46;
	ld.global.u64 	%rd50, [%rd2+48];
	mul.lo.s64 	%rd51, %rd50, %rd42;
	add.s64 	%rd52, %rd49, %rd51;
	ld.global.u64 	%rd53, [%rd2+56];
	mul.lo.s64 	%rd54, %rd53, %rd44;
	add.s64 	%rd55, %rd52, %rd54;
	cvta.to.global.u64 	%rd56, %rd28;
	shl.b64 	%rd57, %rd55, 2;
	add.s64 	%rd58, %rd56, %rd57;
	ld.global.f32 	%f1, [%rd58];
	cvta.to.global.u64 	%rd59, %rd29;
	shl.b64 	%rd60, %rd1, 2;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.f32 	[%rd61], %f1;

$L__BB33_17:
	ret;

}
	// .globl	upsample_nearest2d_f64
.visible .entry upsample_nearest2d_f64(
	.param .u64 upsample_nearest2d_f64_param_0,
	.param .u64 upsample_nearest2d_f64_param_1,
	.param .f64 upsample_nearest2d_f64_param_2,
	.param .f64 upsample_nearest2d_f64_param_3,
	.param .u64 upsample_nearest2d_f64_param_4,
	.param .u64 upsample_nearest2d_f64_param_5,
	.param .u64 upsample_nearest2d_f64_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd26, [upsample_nearest2d_f64_param_0];
	ld.param.u64 	%rd27, [upsample_nearest2d_f64_param_1];
	ld.param.f64 	%fd1, [upsample_nearest2d_f64_param_2];
	ld.param.f64 	%fd2, [upsample_nearest2d_f64_param_3];
	ld.param.u64 	%rd30, [upsample_nearest2d_f64_param_4];
	ld.param.u64 	%rd28, [upsample_nearest2d_f64_param_5];
	ld.param.u64 	%rd29, [upsample_nearest2d_f64_param_6];
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	mul.lo.s64 	%rd3, %rd27, %rd26;
	ld.global.u64 	%rd4, [%rd2+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd31, [%rd2];
	mul.lo.s64 	%rd32, %rd5, %rd31;
	setp.le.u64 	%p1, %rd32, %rd1;
	@%p1 bra 	$L__BB34_17;

	ld.global.u64 	%rd6, [%rd2+16];
	ld.global.u64 	%rd7, [%rd2+24];
	and.b64  	%rd33, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB34_3;

	div.u64 	%rd62, %rd1, %rd5;
	bra.uni 	$L__BB34_4;

$L__BB34_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd62, %r7;

$L__BB34_4:
	and.b64  	%rd34, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB34_6;

	div.u64 	%rd63, %rd1, %rd3;
	bra.uni 	$L__BB34_7;

$L__BB34_6:
	cvt.u32.u64 	%r8, %rd3;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd63, %r10;

$L__BB34_7:
	and.b64  	%rd35, %rd4, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB34_9;

	rem.u64 	%rd64, %rd63, %rd4;
	bra.uni 	$L__BB34_10;

$L__BB34_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd63;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd64, %r13;

$L__BB34_10:
	and.b64  	%rd36, %rd27, -4294967296;
	setp.eq.s64 	%p5, %rd36, 0;
	@%p5 bra 	$L__BB34_12;

	div.u64 	%rd65, %rd1, %rd27;
	mul.lo.s64 	%rd37, %rd65, %rd27;
	sub.s64 	%rd66, %rd1, %rd37;
	bra.uni 	$L__BB34_13;

$L__BB34_12:
	cvt.u32.u64 	%r14, %rd27;
	cvt.u32.u64 	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd65, %r16;
	cvt.u64.u32 	%rd66, %r18;

$L__BB34_13:
	and.b64  	%rd38, %rd26, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB34_15;

	rem.u64 	%rd67, %rd65, %rd26;
	bra.uni 	$L__BB34_16;

$L__BB34_15:
	cvt.u32.u64 	%r19, %rd26;
	cvt.u32.u64 	%r20, %rd65;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd67, %r21;

$L__BB34_16:
	cvt.rn.f64.u64 	%fd3, %rd67;
	mul.f64 	%fd4, %fd3, %fd1;
	cvt.rzi.u64.f64 	%rd39, %fd4;
	cvt.rn.f64.u64 	%fd5, %rd66;
	mul.f64 	%fd6, %fd5, %fd2;
	cvt.rzi.u64.f64 	%rd40, %fd6;
	setp.lt.u64 	%p7, %rd39, %rd6;
	add.s64 	%rd41, %rd6, -1;
	selp.b64 	%rd42, %rd39, %rd41, %p7;
	setp.lt.u64 	%p8, %rd40, %rd7;
	add.s64 	%rd43, %rd7, -1;
	selp.b64 	%rd44, %rd40, %rd43, %p8;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd62;
	ld.global.u64 	%rd47, [%rd2+40];
	mul.lo.s64 	%rd48, %rd47, %rd64;
	add.s64 	%rd49, %rd48, %rd46;
	ld.global.u64 	%rd50, [%rd2+48];
	mul.lo.s64 	%rd51, %rd50, %rd42;
	add.s64 	%rd52, %rd49, %rd51;
	ld.global.u64 	%rd53, [%rd2+56];
	mul.lo.s64 	%rd54, %rd53, %rd44;
	add.s64 	%rd55, %rd52, %rd54;
	cvta.to.global.u64 	%rd56, %rd28;
	shl.b64 	%rd57, %rd55, 3;
	add.s64 	%rd58, %rd56, %rd57;
	ld.global.f64 	%fd7, [%rd58];
	cvta.to.global.u64 	%rd59, %rd29;
	shl.b64 	%rd60, %rd1, 3;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.f64 	[%rd61], %fd7;

$L__BB34_17:
	ret;

}
	// .globl	upsample_nearest2d_u8
.visible .entry upsample_nearest2d_u8(
	.param .u64 upsample_nearest2d_u8_param_0,
	.param .u64 upsample_nearest2d_u8_param_1,
	.param .f64 upsample_nearest2d_u8_param_2,
	.param .f64 upsample_nearest2d_u8_param_3,
	.param .u64 upsample_nearest2d_u8_param_4,
	.param .u64 upsample_nearest2d_u8_param_5,
	.param .u64 upsample_nearest2d_u8_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd26, [upsample_nearest2d_u8_param_0];
	ld.param.u64 	%rd27, [upsample_nearest2d_u8_param_1];
	ld.param.f64 	%fd1, [upsample_nearest2d_u8_param_2];
	ld.param.f64 	%fd2, [upsample_nearest2d_u8_param_3];
	ld.param.u64 	%rd30, [upsample_nearest2d_u8_param_4];
	ld.param.u64 	%rd28, [upsample_nearest2d_u8_param_5];
	ld.param.u64 	%rd29, [upsample_nearest2d_u8_param_6];
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	mul.lo.s64 	%rd3, %rd27, %rd26;
	ld.global.u64 	%rd4, [%rd2+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd31, [%rd2];
	mul.lo.s64 	%rd32, %rd5, %rd31;
	setp.le.u64 	%p1, %rd32, %rd1;
	@%p1 bra 	$L__BB35_17;

	ld.global.u64 	%rd6, [%rd2+16];
	ld.global.u64 	%rd7, [%rd2+24];
	and.b64  	%rd33, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB35_3;

	div.u64 	%rd60, %rd1, %rd5;
	bra.uni 	$L__BB35_4;

$L__BB35_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd60, %r7;

$L__BB35_4:
	and.b64  	%rd34, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB35_6;

	div.u64 	%rd61, %rd1, %rd3;
	bra.uni 	$L__BB35_7;

$L__BB35_6:
	cvt.u32.u64 	%r8, %rd3;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd61, %r10;

$L__BB35_7:
	and.b64  	%rd35, %rd4, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB35_9;

	rem.u64 	%rd62, %rd61, %rd4;
	bra.uni 	$L__BB35_10;

$L__BB35_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd61;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd62, %r13;

$L__BB35_10:
	and.b64  	%rd36, %rd27, -4294967296;
	setp.eq.s64 	%p5, %rd36, 0;
	@%p5 bra 	$L__BB35_12;

	div.u64 	%rd63, %rd1, %rd27;
	mul.lo.s64 	%rd37, %rd63, %rd27;
	sub.s64 	%rd64, %rd1, %rd37;
	bra.uni 	$L__BB35_13;

$L__BB35_12:
	cvt.u32.u64 	%r14, %rd27;
	cvt.u32.u64 	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd63, %r16;
	cvt.u64.u32 	%rd64, %r18;

$L__BB35_13:
	and.b64  	%rd38, %rd26, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB35_15;

	rem.u64 	%rd65, %rd63, %rd26;
	bra.uni 	$L__BB35_16;

$L__BB35_15:
	cvt.u32.u64 	%r19, %rd26;
	cvt.u32.u64 	%r20, %rd63;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd65, %r21;

$L__BB35_16:
	cvt.rn.f64.u64 	%fd3, %rd65;
	mul.f64 	%fd4, %fd3, %fd1;
	cvt.rzi.u64.f64 	%rd39, %fd4;
	cvt.rn.f64.u64 	%fd5, %rd64;
	mul.f64 	%fd6, %fd5, %fd2;
	cvt.rzi.u64.f64 	%rd40, %fd6;
	setp.lt.u64 	%p7, %rd39, %rd6;
	add.s64 	%rd41, %rd6, -1;
	selp.b64 	%rd42, %rd39, %rd41, %p7;
	setp.lt.u64 	%p8, %rd40, %rd7;
	add.s64 	%rd43, %rd7, -1;
	selp.b64 	%rd44, %rd40, %rd43, %p8;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd60;
	ld.global.u64 	%rd47, [%rd2+40];
	mul.lo.s64 	%rd48, %rd47, %rd62;
	add.s64 	%rd49, %rd48, %rd46;
	ld.global.u64 	%rd50, [%rd2+48];
	mul.lo.s64 	%rd51, %rd50, %rd42;
	add.s64 	%rd52, %rd49, %rd51;
	ld.global.u64 	%rd53, [%rd2+56];
	mul.lo.s64 	%rd54, %rd53, %rd44;
	add.s64 	%rd55, %rd52, %rd54;
	cvta.to.global.u64 	%rd56, %rd28;
	add.s64 	%rd57, %rd56, %rd55;
	ld.global.u8 	%rs1, [%rd57];
	cvta.to.global.u64 	%rd58, %rd29;
	add.s64 	%rd59, %rd58, %rd1;
	st.global.u8 	[%rd59], %rs1;

$L__BB35_17:
	ret;

}
	// .globl	upsample_nearest2d_u32
.visible .entry upsample_nearest2d_u32(
	.param .u64 upsample_nearest2d_u32_param_0,
	.param .u64 upsample_nearest2d_u32_param_1,
	.param .f64 upsample_nearest2d_u32_param_2,
	.param .f64 upsample_nearest2d_u32_param_3,
	.param .u64 upsample_nearest2d_u32_param_4,
	.param .u64 upsample_nearest2d_u32_param_5,
	.param .u64 upsample_nearest2d_u32_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd26, [upsample_nearest2d_u32_param_0];
	ld.param.u64 	%rd27, [upsample_nearest2d_u32_param_1];
	ld.param.f64 	%fd1, [upsample_nearest2d_u32_param_2];
	ld.param.f64 	%fd2, [upsample_nearest2d_u32_param_3];
	ld.param.u64 	%rd30, [upsample_nearest2d_u32_param_4];
	ld.param.u64 	%rd28, [upsample_nearest2d_u32_param_5];
	ld.param.u64 	%rd29, [upsample_nearest2d_u32_param_6];
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	mul.lo.s64 	%rd3, %rd27, %rd26;
	ld.global.u64 	%rd4, [%rd2+8];
	mul.lo.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd31, [%rd2];
	mul.lo.s64 	%rd32, %rd5, %rd31;
	setp.le.u64 	%p1, %rd32, %rd1;
	@%p1 bra 	$L__BB36_17;

	ld.global.u64 	%rd6, [%rd2+16];
	ld.global.u64 	%rd7, [%rd2+24];
	and.b64  	%rd33, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB36_3;

	div.u64 	%rd62, %rd1, %rd5;
	bra.uni 	$L__BB36_4;

$L__BB36_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd62, %r7;

$L__BB36_4:
	and.b64  	%rd34, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB36_6;

	div.u64 	%rd63, %rd1, %rd3;
	bra.uni 	$L__BB36_7;

$L__BB36_6:
	cvt.u32.u64 	%r8, %rd3;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd63, %r10;

$L__BB36_7:
	and.b64  	%rd35, %rd4, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB36_9;

	rem.u64 	%rd64, %rd63, %rd4;
	bra.uni 	$L__BB36_10;

$L__BB36_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd63;
	rem.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd64, %r13;

$L__BB36_10:
	and.b64  	%rd36, %rd27, -4294967296;
	setp.eq.s64 	%p5, %rd36, 0;
	@%p5 bra 	$L__BB36_12;

	div.u64 	%rd65, %rd1, %rd27;
	mul.lo.s64 	%rd37, %rd65, %rd27;
	sub.s64 	%rd66, %rd1, %rd37;
	bra.uni 	$L__BB36_13;

$L__BB36_12:
	cvt.u32.u64 	%r14, %rd27;
	cvt.u32.u64 	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	mul.lo.s32 	%r17, %r16, %r14;
	sub.s32 	%r18, %r15, %r17;
	cvt.u64.u32 	%rd65, %r16;
	cvt.u64.u32 	%rd66, %r18;

$L__BB36_13:
	and.b64  	%rd38, %rd26, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB36_15;

	rem.u64 	%rd67, %rd65, %rd26;
	bra.uni 	$L__BB36_16;

$L__BB36_15:
	cvt.u32.u64 	%r19, %rd26;
	cvt.u32.u64 	%r20, %rd65;
	rem.u32 	%r21, %r20, %r19;
	cvt.u64.u32 	%rd67, %r21;

$L__BB36_16:
	cvt.rn.f64.u64 	%fd3, %rd67;
	mul.f64 	%fd4, %fd3, %fd1;
	cvt.rzi.u64.f64 	%rd39, %fd4;
	cvt.rn.f64.u64 	%fd5, %rd66;
	mul.f64 	%fd6, %fd5, %fd2;
	cvt.rzi.u64.f64 	%rd40, %fd6;
	setp.lt.u64 	%p7, %rd39, %rd6;
	add.s64 	%rd41, %rd6, -1;
	selp.b64 	%rd42, %rd39, %rd41, %p7;
	setp.lt.u64 	%p8, %rd40, %rd7;
	add.s64 	%rd43, %rd7, -1;
	selp.b64 	%rd44, %rd40, %rd43, %p8;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd62;
	ld.global.u64 	%rd47, [%rd2+40];
	mul.lo.s64 	%rd48, %rd47, %rd64;
	add.s64 	%rd49, %rd48, %rd46;
	ld.global.u64 	%rd50, [%rd2+48];
	mul.lo.s64 	%rd51, %rd50, %rd42;
	add.s64 	%rd52, %rd49, %rd51;
	ld.global.u64 	%rd53, [%rd2+56];
	mul.lo.s64 	%rd54, %rd53, %rd44;
	add.s64 	%rd55, %rd52, %rd54;
	cvta.to.global.u64 	%rd56, %rd28;
	shl.b64 	%rd57, %rd55, 2;
	add.s64 	%rd58, %rd56, %rd57;
	ld.global.u32 	%r22, [%rd58];
	cvta.to.global.u64 	%rd59, %rd29;
	shl.b64 	%rd60, %rd1, 2;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u32 	[%rd61], %r22;

$L__BB36_17:
	ret;

}
	// .globl	im2col_f32
.visible .entry im2col_f32(
	.param .u64 im2col_f32_param_0,
	.param .u64 im2col_f32_param_1,
	.param .u64 im2col_f32_param_2,
	.param .u64 im2col_f32_param_3,
	.param .u64 im2col_f32_param_4,
	.param .u64 im2col_f32_param_5,
	.param .u64 im2col_f32_param_6,
	.param .u64 im2col_f32_param_7,
	.param .u64 im2col_f32_param_8,
	.param .u64 im2col_f32_param_9,
	.param .u64 im2col_f32_param_10
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<87>;


	ld.param.u64 	%rd40, [im2col_f32_param_0];
	ld.param.u64 	%rd30, [im2col_f32_param_1];
	ld.param.u64 	%rd31, [im2col_f32_param_2];
	ld.param.u64 	%rd32, [im2col_f32_param_3];
	ld.param.u64 	%rd33, [im2col_f32_param_4];
	ld.param.u64 	%rd34, [im2col_f32_param_5];
	ld.param.u64 	%rd35, [im2col_f32_param_6];
	ld.param.u64 	%rd36, [im2col_f32_param_7];
	ld.param.u64 	%rd37, [im2col_f32_param_8];
	ld.param.u64 	%rd38, [im2col_f32_param_9];
	ld.param.u64 	%rd39, [im2col_f32_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd40;
	@%p1 bra 	$L__BB37_21;

	cvta.to.global.u64 	%rd2, %rd37;
	ld.global.u64 	%rd3, [%rd2+16];
	mul.lo.s64 	%rd4, %rd33, %rd32;
	ld.global.u64 	%rd41, [%rd2+8];
	mul.lo.s64 	%rd5, %rd41, %rd4;
	mul.lo.s64 	%rd6, %rd5, %rd31;
	mul.lo.s64 	%rd7, %rd6, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd42, 0;
	@%p2 bra 	$L__BB37_3;

	div.u64 	%rd82, %rd1, %rd7;
	bra.uni 	$L__BB37_4;

$L__BB37_3:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd82, %r7;

$L__BB37_4:
	mul.lo.s64 	%rd43, %rd7, %rd82;
	sub.s64 	%rd11, %rd1, %rd43;
	or.b64  	%rd44, %rd11, %rd6;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB37_6;

	div.u64 	%rd83, %rd11, %rd6;
	bra.uni 	$L__BB37_7;

$L__BB37_6:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd11;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd83, %r10;

$L__BB37_7:
	mul.lo.s64 	%rd46, %rd83, %rd6;
	sub.s64 	%rd15, %rd11, %rd46;
	or.b64  	%rd47, %rd15, %rd5;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB37_9;

	div.u64 	%rd84, %rd15, %rd5;
	bra.uni 	$L__BB37_10;

$L__BB37_9:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd15;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd84, %r13;

$L__BB37_10:
	mul.lo.s64 	%rd49, %rd84, %rd5;
	sub.s64 	%rd19, %rd15, %rd49;
	or.b64  	%rd50, %rd19, %rd4;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB37_12;

	div.u64 	%rd85, %rd19, %rd4;
	bra.uni 	$L__BB37_13;

$L__BB37_12:
	cvt.u32.u64 	%r14, %rd4;
	cvt.u32.u64 	%r15, %rd19;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd85, %r16;

$L__BB37_13:
	mul.lo.s64 	%rd52, %rd85, %rd4;
	sub.s64 	%rd23, %rd19, %rd52;
	or.b64  	%rd53, %rd23, %rd33;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB37_15;

	div.u64 	%rd86, %rd23, %rd33;
	bra.uni 	$L__BB37_16;

$L__BB37_15:
	cvt.u32.u64 	%r17, %rd33;
	cvt.u32.u64 	%r18, %rd23;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd86, %r19;

$L__BB37_16:
	mul.lo.s64 	%rd55, %rd86, %rd33;
	sub.s64 	%rd56, %rd23, %rd55;
	mul.lo.s64 	%rd57, %rd86, %rd36;
	mul.lo.s64 	%rd58, %rd83, %rd34;
	add.s64 	%rd27, %rd57, %rd58;
	mul.lo.s64 	%rd59, %rd56, %rd36;
	mul.lo.s64 	%rd60, %rd84, %rd34;
	add.s64 	%rd28, %rd59, %rd60;
	setp.lt.u64 	%p7, %rd27, %rd35;
	add.s64 	%rd61, %rd3, %rd35;
	setp.ge.u64 	%p8, %rd27, %rd61;
	or.pred  	%p9, %p7, %p8;
	cvta.to.global.u64 	%rd62, %rd39;
	shl.b64 	%rd63, %rd1, 2;
	add.s64 	%rd29, %rd62, %rd63;
	@%p9 bra 	$L__BB37_20;
	bra.uni 	$L__BB37_17;

$L__BB37_20:
	mov.u32 	%r21, 0;
	st.global.u32 	[%rd29], %r21;
	bra.uni 	$L__BB37_21;

$L__BB37_17:
	ld.global.u64 	%rd64, [%rd2+24];
	add.s64 	%rd65, %rd64, %rd35;
	setp.ge.u64 	%p10, %rd28, %rd65;
	setp.lt.u64 	%p11, %rd28, %rd35;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB37_19;
	bra.uni 	$L__BB37_18;

$L__BB37_19:
	mov.u32 	%r20, 0;
	st.global.u32 	[%rd29], %r20;
	bra.uni 	$L__BB37_21;

$L__BB37_18:
	sub.s64 	%rd66, %rd27, %rd35;
	ld.global.u64 	%rd67, [%rd2+32];
	mul.lo.s64 	%rd68, %rd67, %rd82;
	ld.global.u64 	%rd69, [%rd2+40];
	mul.lo.s64 	%rd70, %rd69, %rd85;
	add.s64 	%rd71, %rd70, %rd68;
	ld.global.u64 	%rd72, [%rd2+48];
	mul.lo.s64 	%rd73, %rd72, %rd66;
	add.s64 	%rd74, %rd71, %rd73;
	ld.global.u64 	%rd75, [%rd2+56];
	sub.s64 	%rd76, %rd28, %rd35;
	mul.lo.s64 	%rd77, %rd75, %rd76;
	add.s64 	%rd78, %rd74, %rd77;
	cvta.to.global.u64 	%rd79, %rd38;
	shl.b64 	%rd80, %rd78, 2;
	add.s64 	%rd81, %rd79, %rd80;
	ld.global.f32 	%f1, [%rd81];
	st.global.f32 	[%rd29], %f1;

$L__BB37_21:
	ret;

}
	// .globl	im2col_f64
.visible .entry im2col_f64(
	.param .u64 im2col_f64_param_0,
	.param .u64 im2col_f64_param_1,
	.param .u64 im2col_f64_param_2,
	.param .u64 im2col_f64_param_3,
	.param .u64 im2col_f64_param_4,
	.param .u64 im2col_f64_param_5,
	.param .u64 im2col_f64_param_6,
	.param .u64 im2col_f64_param_7,
	.param .u64 im2col_f64_param_8,
	.param .u64 im2col_f64_param_9,
	.param .u64 im2col_f64_param_10
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<89>;


	ld.param.u64 	%rd40, [im2col_f64_param_0];
	ld.param.u64 	%rd30, [im2col_f64_param_1];
	ld.param.u64 	%rd31, [im2col_f64_param_2];
	ld.param.u64 	%rd32, [im2col_f64_param_3];
	ld.param.u64 	%rd33, [im2col_f64_param_4];
	ld.param.u64 	%rd34, [im2col_f64_param_5];
	ld.param.u64 	%rd35, [im2col_f64_param_6];
	ld.param.u64 	%rd36, [im2col_f64_param_7];
	ld.param.u64 	%rd37, [im2col_f64_param_8];
	ld.param.u64 	%rd38, [im2col_f64_param_9];
	ld.param.u64 	%rd39, [im2col_f64_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd40;
	@%p1 bra 	$L__BB38_21;

	cvta.to.global.u64 	%rd2, %rd37;
	ld.global.u64 	%rd3, [%rd2+16];
	mul.lo.s64 	%rd4, %rd33, %rd32;
	ld.global.u64 	%rd41, [%rd2+8];
	mul.lo.s64 	%rd5, %rd41, %rd4;
	mul.lo.s64 	%rd6, %rd5, %rd31;
	mul.lo.s64 	%rd7, %rd6, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd42, 0;
	@%p2 bra 	$L__BB38_3;

	div.u64 	%rd84, %rd1, %rd7;
	bra.uni 	$L__BB38_4;

$L__BB38_3:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd84, %r7;

$L__BB38_4:
	mul.lo.s64 	%rd43, %rd7, %rd84;
	sub.s64 	%rd11, %rd1, %rd43;
	or.b64  	%rd44, %rd11, %rd6;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB38_6;

	div.u64 	%rd85, %rd11, %rd6;
	bra.uni 	$L__BB38_7;

$L__BB38_6:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd11;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd85, %r10;

$L__BB38_7:
	mul.lo.s64 	%rd46, %rd85, %rd6;
	sub.s64 	%rd15, %rd11, %rd46;
	or.b64  	%rd47, %rd15, %rd5;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB38_9;

	div.u64 	%rd86, %rd15, %rd5;
	bra.uni 	$L__BB38_10;

$L__BB38_9:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd15;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd86, %r13;

$L__BB38_10:
	mul.lo.s64 	%rd49, %rd86, %rd5;
	sub.s64 	%rd19, %rd15, %rd49;
	or.b64  	%rd50, %rd19, %rd4;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB38_12;

	div.u64 	%rd87, %rd19, %rd4;
	bra.uni 	$L__BB38_13;

$L__BB38_12:
	cvt.u32.u64 	%r14, %rd4;
	cvt.u32.u64 	%r15, %rd19;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd87, %r16;

$L__BB38_13:
	mul.lo.s64 	%rd52, %rd87, %rd4;
	sub.s64 	%rd23, %rd19, %rd52;
	or.b64  	%rd53, %rd23, %rd33;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB38_15;

	div.u64 	%rd88, %rd23, %rd33;
	bra.uni 	$L__BB38_16;

$L__BB38_15:
	cvt.u32.u64 	%r17, %rd33;
	cvt.u32.u64 	%r18, %rd23;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd88, %r19;

$L__BB38_16:
	mul.lo.s64 	%rd55, %rd88, %rd33;
	sub.s64 	%rd56, %rd23, %rd55;
	mul.lo.s64 	%rd57, %rd88, %rd36;
	mul.lo.s64 	%rd58, %rd85, %rd34;
	add.s64 	%rd27, %rd57, %rd58;
	mul.lo.s64 	%rd59, %rd56, %rd36;
	mul.lo.s64 	%rd60, %rd86, %rd34;
	add.s64 	%rd28, %rd59, %rd60;
	setp.lt.u64 	%p7, %rd27, %rd35;
	add.s64 	%rd61, %rd3, %rd35;
	setp.ge.u64 	%p8, %rd27, %rd61;
	or.pred  	%p9, %p7, %p8;
	cvta.to.global.u64 	%rd62, %rd39;
	shl.b64 	%rd63, %rd1, 3;
	add.s64 	%rd29, %rd62, %rd63;
	@%p9 bra 	$L__BB38_20;
	bra.uni 	$L__BB38_17;

$L__BB38_20:
	mov.u64 	%rd83, 0;
	st.global.u64 	[%rd29], %rd83;
	bra.uni 	$L__BB38_21;

$L__BB38_17:
	ld.global.u64 	%rd64, [%rd2+24];
	add.s64 	%rd65, %rd64, %rd35;
	setp.ge.u64 	%p10, %rd28, %rd65;
	setp.lt.u64 	%p11, %rd28, %rd35;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB38_19;
	bra.uni 	$L__BB38_18;

$L__BB38_19:
	mov.u64 	%rd82, 0;
	st.global.u64 	[%rd29], %rd82;
	bra.uni 	$L__BB38_21;

$L__BB38_18:
	sub.s64 	%rd66, %rd27, %rd35;
	ld.global.u64 	%rd67, [%rd2+32];
	mul.lo.s64 	%rd68, %rd67, %rd84;
	ld.global.u64 	%rd69, [%rd2+40];
	mul.lo.s64 	%rd70, %rd69, %rd87;
	add.s64 	%rd71, %rd70, %rd68;
	ld.global.u64 	%rd72, [%rd2+48];
	mul.lo.s64 	%rd73, %rd72, %rd66;
	add.s64 	%rd74, %rd71, %rd73;
	ld.global.u64 	%rd75, [%rd2+56];
	sub.s64 	%rd76, %rd28, %rd35;
	mul.lo.s64 	%rd77, %rd75, %rd76;
	add.s64 	%rd78, %rd74, %rd77;
	cvta.to.global.u64 	%rd79, %rd38;
	shl.b64 	%rd80, %rd78, 3;
	add.s64 	%rd81, %rd79, %rd80;
	ld.global.f64 	%fd1, [%rd81];
	st.global.f64 	[%rd29], %fd1;

$L__BB38_21:
	ret;

}
	// .globl	im2col_u8
.visible .entry im2col_u8(
	.param .u64 im2col_u8_param_0,
	.param .u64 im2col_u8_param_1,
	.param .u64 im2col_u8_param_2,
	.param .u64 im2col_u8_param_3,
	.param .u64 im2col_u8_param_4,
	.param .u64 im2col_u8_param_5,
	.param .u64 im2col_u8_param_6,
	.param .u64 im2col_u8_param_7,
	.param .u64 im2col_u8_param_8,
	.param .u64 im2col_u8_param_9,
	.param .u64 im2col_u8_param_10
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<85>;


	ld.param.u64 	%rd40, [im2col_u8_param_0];
	ld.param.u64 	%rd30, [im2col_u8_param_1];
	ld.param.u64 	%rd31, [im2col_u8_param_2];
	ld.param.u64 	%rd32, [im2col_u8_param_3];
	ld.param.u64 	%rd33, [im2col_u8_param_4];
	ld.param.u64 	%rd34, [im2col_u8_param_5];
	ld.param.u64 	%rd35, [im2col_u8_param_6];
	ld.param.u64 	%rd36, [im2col_u8_param_7];
	ld.param.u64 	%rd37, [im2col_u8_param_8];
	ld.param.u64 	%rd38, [im2col_u8_param_9];
	ld.param.u64 	%rd39, [im2col_u8_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd40;
	@%p1 bra 	$L__BB39_21;

	cvta.to.global.u64 	%rd2, %rd37;
	ld.global.u64 	%rd3, [%rd2+16];
	mul.lo.s64 	%rd4, %rd33, %rd32;
	ld.global.u64 	%rd41, [%rd2+8];
	mul.lo.s64 	%rd5, %rd41, %rd4;
	mul.lo.s64 	%rd6, %rd5, %rd31;
	mul.lo.s64 	%rd7, %rd6, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd42, 0;
	@%p2 bra 	$L__BB39_3;

	div.u64 	%rd80, %rd1, %rd7;
	bra.uni 	$L__BB39_4;

$L__BB39_3:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd80, %r7;

$L__BB39_4:
	mul.lo.s64 	%rd43, %rd7, %rd80;
	sub.s64 	%rd11, %rd1, %rd43;
	or.b64  	%rd44, %rd11, %rd6;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB39_6;

	div.u64 	%rd81, %rd11, %rd6;
	bra.uni 	$L__BB39_7;

$L__BB39_6:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd11;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd81, %r10;

$L__BB39_7:
	mul.lo.s64 	%rd46, %rd81, %rd6;
	sub.s64 	%rd15, %rd11, %rd46;
	or.b64  	%rd47, %rd15, %rd5;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB39_9;

	div.u64 	%rd82, %rd15, %rd5;
	bra.uni 	$L__BB39_10;

$L__BB39_9:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd15;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd82, %r13;

$L__BB39_10:
	mul.lo.s64 	%rd49, %rd82, %rd5;
	sub.s64 	%rd19, %rd15, %rd49;
	or.b64  	%rd50, %rd19, %rd4;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB39_12;

	div.u64 	%rd83, %rd19, %rd4;
	bra.uni 	$L__BB39_13;

$L__BB39_12:
	cvt.u32.u64 	%r14, %rd4;
	cvt.u32.u64 	%r15, %rd19;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd83, %r16;

$L__BB39_13:
	mul.lo.s64 	%rd52, %rd83, %rd4;
	sub.s64 	%rd23, %rd19, %rd52;
	or.b64  	%rd53, %rd23, %rd33;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB39_15;

	div.u64 	%rd84, %rd23, %rd33;
	bra.uni 	$L__BB39_16;

$L__BB39_15:
	cvt.u32.u64 	%r17, %rd33;
	cvt.u32.u64 	%r18, %rd23;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd84, %r19;

$L__BB39_16:
	mul.lo.s64 	%rd55, %rd84, %rd33;
	sub.s64 	%rd56, %rd23, %rd55;
	mul.lo.s64 	%rd57, %rd84, %rd36;
	mul.lo.s64 	%rd58, %rd81, %rd34;
	add.s64 	%rd27, %rd57, %rd58;
	mul.lo.s64 	%rd59, %rd56, %rd36;
	mul.lo.s64 	%rd60, %rd82, %rd34;
	add.s64 	%rd28, %rd59, %rd60;
	setp.lt.u64 	%p7, %rd27, %rd35;
	add.s64 	%rd61, %rd3, %rd35;
	setp.ge.u64 	%p8, %rd27, %rd61;
	or.pred  	%p9, %p7, %p8;
	cvta.to.global.u64 	%rd62, %rd39;
	add.s64 	%rd29, %rd62, %rd1;
	@%p9 bra 	$L__BB39_20;
	bra.uni 	$L__BB39_17;

$L__BB39_20:
	mov.u16 	%rs3, 0;
	st.global.u8 	[%rd29], %rs3;
	bra.uni 	$L__BB39_21;

$L__BB39_17:
	ld.global.u64 	%rd63, [%rd2+24];
	add.s64 	%rd64, %rd63, %rd35;
	setp.ge.u64 	%p10, %rd28, %rd64;
	setp.lt.u64 	%p11, %rd28, %rd35;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB39_19;
	bra.uni 	$L__BB39_18;

$L__BB39_19:
	mov.u16 	%rs2, 0;
	st.global.u8 	[%rd29], %rs2;
	bra.uni 	$L__BB39_21;

$L__BB39_18:
	sub.s64 	%rd65, %rd27, %rd35;
	ld.global.u64 	%rd66, [%rd2+32];
	mul.lo.s64 	%rd67, %rd66, %rd80;
	ld.global.u64 	%rd68, [%rd2+40];
	mul.lo.s64 	%rd69, %rd68, %rd83;
	add.s64 	%rd70, %rd69, %rd67;
	ld.global.u64 	%rd71, [%rd2+48];
	mul.lo.s64 	%rd72, %rd71, %rd65;
	add.s64 	%rd73, %rd70, %rd72;
	ld.global.u64 	%rd74, [%rd2+56];
	sub.s64 	%rd75, %rd28, %rd35;
	mul.lo.s64 	%rd76, %rd74, %rd75;
	add.s64 	%rd77, %rd73, %rd76;
	cvta.to.global.u64 	%rd78, %rd38;
	add.s64 	%rd79, %rd78, %rd77;
	ld.global.u8 	%rs1, [%rd79];
	st.global.u8 	[%rd29], %rs1;

$L__BB39_21:
	ret;

}
	// .globl	im2col_u32
.visible .entry im2col_u32(
	.param .u64 im2col_u32_param_0,
	.param .u64 im2col_u32_param_1,
	.param .u64 im2col_u32_param_2,
	.param .u64 im2col_u32_param_3,
	.param .u64 im2col_u32_param_4,
	.param .u64 im2col_u32_param_5,
	.param .u64 im2col_u32_param_6,
	.param .u64 im2col_u32_param_7,
	.param .u64 im2col_u32_param_8,
	.param .u64 im2col_u32_param_9,
	.param .u64 im2col_u32_param_10
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<87>;


	ld.param.u64 	%rd40, [im2col_u32_param_0];
	ld.param.u64 	%rd30, [im2col_u32_param_1];
	ld.param.u64 	%rd31, [im2col_u32_param_2];
	ld.param.u64 	%rd32, [im2col_u32_param_3];
	ld.param.u64 	%rd33, [im2col_u32_param_4];
	ld.param.u64 	%rd34, [im2col_u32_param_5];
	ld.param.u64 	%rd35, [im2col_u32_param_6];
	ld.param.u64 	%rd36, [im2col_u32_param_7];
	ld.param.u64 	%rd37, [im2col_u32_param_8];
	ld.param.u64 	%rd38, [im2col_u32_param_9];
	ld.param.u64 	%rd39, [im2col_u32_param_10];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd40;
	@%p1 bra 	$L__BB40_21;

	cvta.to.global.u64 	%rd2, %rd37;
	ld.global.u64 	%rd3, [%rd2+16];
	mul.lo.s64 	%rd4, %rd33, %rd32;
	ld.global.u64 	%rd41, [%rd2+8];
	mul.lo.s64 	%rd5, %rd41, %rd4;
	mul.lo.s64 	%rd6, %rd5, %rd31;
	mul.lo.s64 	%rd7, %rd6, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p2, %rd42, 0;
	@%p2 bra 	$L__BB40_3;

	div.u64 	%rd82, %rd1, %rd7;
	bra.uni 	$L__BB40_4;

$L__BB40_3:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd82, %r7;

$L__BB40_4:
	mul.lo.s64 	%rd43, %rd7, %rd82;
	sub.s64 	%rd11, %rd1, %rd43;
	or.b64  	%rd44, %rd11, %rd6;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB40_6;

	div.u64 	%rd83, %rd11, %rd6;
	bra.uni 	$L__BB40_7;

$L__BB40_6:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd11;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd83, %r10;

$L__BB40_7:
	mul.lo.s64 	%rd46, %rd83, %rd6;
	sub.s64 	%rd15, %rd11, %rd46;
	or.b64  	%rd47, %rd15, %rd5;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB40_9;

	div.u64 	%rd84, %rd15, %rd5;
	bra.uni 	$L__BB40_10;

$L__BB40_9:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd15;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd84, %r13;

$L__BB40_10:
	mul.lo.s64 	%rd49, %rd84, %rd5;
	sub.s64 	%rd19, %rd15, %rd49;
	or.b64  	%rd50, %rd19, %rd4;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB40_12;

	div.u64 	%rd85, %rd19, %rd4;
	bra.uni 	$L__BB40_13;

$L__BB40_12:
	cvt.u32.u64 	%r14, %rd4;
	cvt.u32.u64 	%r15, %rd19;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd85, %r16;

$L__BB40_13:
	mul.lo.s64 	%rd52, %rd85, %rd4;
	sub.s64 	%rd23, %rd19, %rd52;
	or.b64  	%rd53, %rd23, %rd33;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB40_15;

	div.u64 	%rd86, %rd23, %rd33;
	bra.uni 	$L__BB40_16;

$L__BB40_15:
	cvt.u32.u64 	%r17, %rd33;
	cvt.u32.u64 	%r18, %rd23;
	div.u32 	%r19, %r18, %r17;
	cvt.u64.u32 	%rd86, %r19;

$L__BB40_16:
	mul.lo.s64 	%rd55, %rd86, %rd33;
	sub.s64 	%rd56, %rd23, %rd55;
	mul.lo.s64 	%rd57, %rd86, %rd36;
	mul.lo.s64 	%rd58, %rd83, %rd34;
	add.s64 	%rd27, %rd57, %rd58;
	mul.lo.s64 	%rd59, %rd56, %rd36;
	mul.lo.s64 	%rd60, %rd84, %rd34;
	add.s64 	%rd28, %rd59, %rd60;
	setp.lt.u64 	%p7, %rd27, %rd35;
	add.s64 	%rd61, %rd3, %rd35;
	setp.ge.u64 	%p8, %rd27, %rd61;
	or.pred  	%p9, %p7, %p8;
	cvta.to.global.u64 	%rd62, %rd39;
	shl.b64 	%rd63, %rd1, 2;
	add.s64 	%rd29, %rd62, %rd63;
	@%p9 bra 	$L__BB40_20;
	bra.uni 	$L__BB40_17;

$L__BB40_20:
	mov.u32 	%r22, 0;
	st.global.u32 	[%rd29], %r22;
	bra.uni 	$L__BB40_21;

$L__BB40_17:
	ld.global.u64 	%rd64, [%rd2+24];
	add.s64 	%rd65, %rd64, %rd35;
	setp.ge.u64 	%p10, %rd28, %rd65;
	setp.lt.u64 	%p11, %rd28, %rd35;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB40_19;
	bra.uni 	$L__BB40_18;

$L__BB40_19:
	mov.u32 	%r21, 0;
	st.global.u32 	[%rd29], %r21;
	bra.uni 	$L__BB40_21;

$L__BB40_18:
	sub.s64 	%rd66, %rd27, %rd35;
	ld.global.u64 	%rd67, [%rd2+32];
	mul.lo.s64 	%rd68, %rd67, %rd82;
	ld.global.u64 	%rd69, [%rd2+40];
	mul.lo.s64 	%rd70, %rd69, %rd85;
	add.s64 	%rd71, %rd70, %rd68;
	ld.global.u64 	%rd72, [%rd2+48];
	mul.lo.s64 	%rd73, %rd72, %rd66;
	add.s64 	%rd74, %rd71, %rd73;
	ld.global.u64 	%rd75, [%rd2+56];
	sub.s64 	%rd76, %rd28, %rd35;
	mul.lo.s64 	%rd77, %rd75, %rd76;
	add.s64 	%rd78, %rd74, %rd77;
	cvta.to.global.u64 	%rd79, %rd38;
	shl.b64 	%rd80, %rd78, 2;
	add.s64 	%rd81, %rd79, %rd80;
	ld.global.u32 	%r20, [%rd81];
	st.global.u32 	[%rd29], %r20;

$L__BB40_21:
	ret;

}
	// .globl	im2col1d_f32
.visible .entry im2col1d_f32(
	.param .u64 im2col1d_f32_param_0,
	.param .u64 im2col1d_f32_param_1,
	.param .u64 im2col1d_f32_param_2,
	.param .u64 im2col1d_f32_param_3,
	.param .u64 im2col1d_f32_param_4,
	.param .u64 im2col1d_f32_param_5,
	.param .u64 im2col1d_f32_param_6,
	.param .u64 im2col1d_f32_param_7,
	.param .u64 im2col1d_f32_param_8
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<58>;


	ld.param.u64 	%rd27, [im2col1d_f32_param_0];
	ld.param.u64 	%rd19, [im2col1d_f32_param_1];
	ld.param.u64 	%rd20, [im2col1d_f32_param_2];
	ld.param.u64 	%rd21, [im2col1d_f32_param_3];
	ld.param.u64 	%rd22, [im2col1d_f32_param_4];
	ld.param.u64 	%rd23, [im2col1d_f32_param_5];
	ld.param.u64 	%rd24, [im2col1d_f32_param_6];
	ld.param.u64 	%rd25, [im2col1d_f32_param_7];
	ld.param.u64 	%rd26, [im2col1d_f32_param_8];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd27;
	@%p1 bra 	$L__BB41_13;

	cvta.to.global.u64 	%rd2, %rd24;
	ld.global.u64 	%rd3, [%rd2+16];
	ld.global.u64 	%rd28, [%rd2+8];
	mul.lo.s64 	%rd4, %rd28, %rd20;
	mul.lo.s64 	%rd5, %rd4, %rd19;
	and.b64  	%rd29, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd29, 0;
	@%p2 bra 	$L__BB41_3;

	div.u64 	%rd55, %rd1, %rd5;
	bra.uni 	$L__BB41_4;

$L__BB41_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd55, %r7;

$L__BB41_4:
	mul.lo.s64 	%rd30, %rd55, %rd5;
	sub.s64 	%rd9, %rd1, %rd30;
	or.b64  	%rd31, %rd9, %rd4;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB41_6;

	div.u64 	%rd56, %rd9, %rd4;
	bra.uni 	$L__BB41_7;

$L__BB41_6:
	cvt.u32.u64 	%r8, %rd4;
	cvt.u32.u64 	%r9, %rd9;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd56, %r10;

$L__BB41_7:
	mul.lo.s64 	%rd33, %rd56, %rd4;
	sub.s64 	%rd13, %rd9, %rd33;
	or.b64  	%rd34, %rd13, %rd20;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB41_9;

	div.u64 	%rd57, %rd13, %rd20;
	bra.uni 	$L__BB41_10;

$L__BB41_9:
	cvt.u32.u64 	%r11, %rd20;
	cvt.u32.u64 	%r12, %rd13;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd57, %r13;

$L__BB41_10:
	mul.lo.s64 	%rd36, %rd57, %rd20;
	sub.s64 	%rd37, %rd13, %rd36;
	mul.lo.s64 	%rd38, %rd37, %rd23;
	mul.lo.s64 	%rd39, %rd56, %rd21;
	add.s64 	%rd17, %rd38, %rd39;
	setp.lt.u64 	%p5, %rd17, %rd22;
	add.s64 	%rd40, %rd3, %rd22;
	setp.ge.u64 	%p6, %rd17, %rd40;
	or.pred  	%p7, %p5, %p6;
	cvta.to.global.u64 	%rd41, %rd26;
	shl.b64 	%rd42, %rd1, 2;
	add.s64 	%rd18, %rd41, %rd42;
	@%p7 bra 	$L__BB41_12;
	bra.uni 	$L__BB41_11;

$L__BB41_12:
	mov.u32 	%r14, 0;
	st.global.u32 	[%rd18], %r14;
	bra.uni 	$L__BB41_13;

$L__BB41_11:
	sub.s64 	%rd43, %rd17, %rd22;
	ld.global.u64 	%rd44, [%rd2+24];
	mul.lo.s64 	%rd45, %rd44, %rd55;
	ld.global.u64 	%rd46, [%rd2+32];
	mul.lo.s64 	%rd47, %rd46, %rd57;
	add.s64 	%rd48, %rd47, %rd45;
	ld.global.u64 	%rd49, [%rd2+40];
	mul.lo.s64 	%rd50, %rd49, %rd43;
	add.s64 	%rd51, %rd48, %rd50;
	cvta.to.global.u64 	%rd52, %rd25;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.f32 	%f1, [%rd54];
	st.global.f32 	[%rd18], %f1;

$L__BB41_13:
	ret;

}
	// .globl	im2col1d_f64
.visible .entry im2col1d_f64(
	.param .u64 im2col1d_f64_param_0,
	.param .u64 im2col1d_f64_param_1,
	.param .u64 im2col1d_f64_param_2,
	.param .u64 im2col1d_f64_param_3,
	.param .u64 im2col1d_f64_param_4,
	.param .u64 im2col1d_f64_param_5,
	.param .u64 im2col1d_f64_param_6,
	.param .u64 im2col1d_f64_param_7,
	.param .u64 im2col1d_f64_param_8
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd27, [im2col1d_f64_param_0];
	ld.param.u64 	%rd19, [im2col1d_f64_param_1];
	ld.param.u64 	%rd20, [im2col1d_f64_param_2];
	ld.param.u64 	%rd21, [im2col1d_f64_param_3];
	ld.param.u64 	%rd22, [im2col1d_f64_param_4];
	ld.param.u64 	%rd23, [im2col1d_f64_param_5];
	ld.param.u64 	%rd24, [im2col1d_f64_param_6];
	ld.param.u64 	%rd25, [im2col1d_f64_param_7];
	ld.param.u64 	%rd26, [im2col1d_f64_param_8];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd27;
	@%p1 bra 	$L__BB42_13;

	cvta.to.global.u64 	%rd2, %rd24;
	ld.global.u64 	%rd3, [%rd2+16];
	ld.global.u64 	%rd28, [%rd2+8];
	mul.lo.s64 	%rd4, %rd28, %rd20;
	mul.lo.s64 	%rd5, %rd4, %rd19;
	and.b64  	%rd29, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd29, 0;
	@%p2 bra 	$L__BB42_3;

	div.u64 	%rd56, %rd1, %rd5;
	bra.uni 	$L__BB42_4;

$L__BB42_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd56, %r7;

$L__BB42_4:
	mul.lo.s64 	%rd30, %rd56, %rd5;
	sub.s64 	%rd9, %rd1, %rd30;
	or.b64  	%rd31, %rd9, %rd4;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB42_6;

	div.u64 	%rd57, %rd9, %rd4;
	bra.uni 	$L__BB42_7;

$L__BB42_6:
	cvt.u32.u64 	%r8, %rd4;
	cvt.u32.u64 	%r9, %rd9;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd57, %r10;

$L__BB42_7:
	mul.lo.s64 	%rd33, %rd57, %rd4;
	sub.s64 	%rd13, %rd9, %rd33;
	or.b64  	%rd34, %rd13, %rd20;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB42_9;

	div.u64 	%rd58, %rd13, %rd20;
	bra.uni 	$L__BB42_10;

$L__BB42_9:
	cvt.u32.u64 	%r11, %rd20;
	cvt.u32.u64 	%r12, %rd13;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd58, %r13;

$L__BB42_10:
	mul.lo.s64 	%rd36, %rd58, %rd20;
	sub.s64 	%rd37, %rd13, %rd36;
	mul.lo.s64 	%rd38, %rd37, %rd23;
	mul.lo.s64 	%rd39, %rd57, %rd21;
	add.s64 	%rd17, %rd38, %rd39;
	setp.lt.u64 	%p5, %rd17, %rd22;
	add.s64 	%rd40, %rd3, %rd22;
	setp.ge.u64 	%p6, %rd17, %rd40;
	or.pred  	%p7, %p5, %p6;
	cvta.to.global.u64 	%rd41, %rd26;
	shl.b64 	%rd42, %rd1, 3;
	add.s64 	%rd18, %rd41, %rd42;
	@%p7 bra 	$L__BB42_12;
	bra.uni 	$L__BB42_11;

$L__BB42_12:
	mov.u64 	%rd55, 0;
	st.global.u64 	[%rd18], %rd55;
	bra.uni 	$L__BB42_13;

$L__BB42_11:
	sub.s64 	%rd43, %rd17, %rd22;
	ld.global.u64 	%rd44, [%rd2+24];
	mul.lo.s64 	%rd45, %rd44, %rd56;
	ld.global.u64 	%rd46, [%rd2+32];
	mul.lo.s64 	%rd47, %rd46, %rd58;
	add.s64 	%rd48, %rd47, %rd45;
	ld.global.u64 	%rd49, [%rd2+40];
	mul.lo.s64 	%rd50, %rd49, %rd43;
	add.s64 	%rd51, %rd48, %rd50;
	cvta.to.global.u64 	%rd52, %rd25;
	shl.b64 	%rd53, %rd51, 3;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.f64 	%fd1, [%rd54];
	st.global.f64 	[%rd18], %fd1;

$L__BB42_13:
	ret;

}
	// .globl	im2col1d_u8
.visible .entry im2col1d_u8(
	.param .u64 im2col1d_u8_param_0,
	.param .u64 im2col1d_u8_param_1,
	.param .u64 im2col1d_u8_param_2,
	.param .u64 im2col1d_u8_param_3,
	.param .u64 im2col1d_u8_param_4,
	.param .u64 im2col1d_u8_param_5,
	.param .u64 im2col1d_u8_param_6,
	.param .u64 im2col1d_u8_param_7,
	.param .u64 im2col1d_u8_param_8
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd27, [im2col1d_u8_param_0];
	ld.param.u64 	%rd19, [im2col1d_u8_param_1];
	ld.param.u64 	%rd20, [im2col1d_u8_param_2];
	ld.param.u64 	%rd21, [im2col1d_u8_param_3];
	ld.param.u64 	%rd22, [im2col1d_u8_param_4];
	ld.param.u64 	%rd23, [im2col1d_u8_param_5];
	ld.param.u64 	%rd24, [im2col1d_u8_param_6];
	ld.param.u64 	%rd25, [im2col1d_u8_param_7];
	ld.param.u64 	%rd26, [im2col1d_u8_param_8];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd27;
	@%p1 bra 	$L__BB43_13;

	cvta.to.global.u64 	%rd2, %rd24;
	ld.global.u64 	%rd3, [%rd2+16];
	ld.global.u64 	%rd28, [%rd2+8];
	mul.lo.s64 	%rd4, %rd28, %rd20;
	mul.lo.s64 	%rd5, %rd4, %rd19;
	and.b64  	%rd29, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd29, 0;
	@%p2 bra 	$L__BB43_3;

	div.u64 	%rd53, %rd1, %rd5;
	bra.uni 	$L__BB43_4;

$L__BB43_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd53, %r7;

$L__BB43_4:
	mul.lo.s64 	%rd30, %rd53, %rd5;
	sub.s64 	%rd9, %rd1, %rd30;
	or.b64  	%rd31, %rd9, %rd4;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB43_6;

	div.u64 	%rd54, %rd9, %rd4;
	bra.uni 	$L__BB43_7;

$L__BB43_6:
	cvt.u32.u64 	%r8, %rd4;
	cvt.u32.u64 	%r9, %rd9;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;

$L__BB43_7:
	mul.lo.s64 	%rd33, %rd54, %rd4;
	sub.s64 	%rd13, %rd9, %rd33;
	or.b64  	%rd34, %rd13, %rd20;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB43_9;

	div.u64 	%rd55, %rd13, %rd20;
	bra.uni 	$L__BB43_10;

$L__BB43_9:
	cvt.u32.u64 	%r11, %rd20;
	cvt.u32.u64 	%r12, %rd13;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd55, %r13;

$L__BB43_10:
	mul.lo.s64 	%rd36, %rd55, %rd20;
	sub.s64 	%rd37, %rd13, %rd36;
	mul.lo.s64 	%rd38, %rd37, %rd23;
	mul.lo.s64 	%rd39, %rd54, %rd21;
	add.s64 	%rd17, %rd38, %rd39;
	setp.lt.u64 	%p5, %rd17, %rd22;
	add.s64 	%rd40, %rd3, %rd22;
	setp.ge.u64 	%p6, %rd17, %rd40;
	or.pred  	%p7, %p5, %p6;
	cvta.to.global.u64 	%rd41, %rd26;
	add.s64 	%rd18, %rd41, %rd1;
	@%p7 bra 	$L__BB43_12;
	bra.uni 	$L__BB43_11;

$L__BB43_12:
	mov.u16 	%rs2, 0;
	st.global.u8 	[%rd18], %rs2;
	bra.uni 	$L__BB43_13;

$L__BB43_11:
	sub.s64 	%rd42, %rd17, %rd22;
	ld.global.u64 	%rd43, [%rd2+24];
	mul.lo.s64 	%rd44, %rd43, %rd53;
	ld.global.u64 	%rd45, [%rd2+32];
	mul.lo.s64 	%rd46, %rd45, %rd55;
	add.s64 	%rd47, %rd46, %rd44;
	ld.global.u64 	%rd48, [%rd2+40];
	mul.lo.s64 	%rd49, %rd48, %rd42;
	add.s64 	%rd50, %rd47, %rd49;
	cvta.to.global.u64 	%rd51, %rd25;
	add.s64 	%rd52, %rd51, %rd50;
	ld.global.u8 	%rs1, [%rd52];
	st.global.u8 	[%rd18], %rs1;

$L__BB43_13:
	ret;

}
	// .globl	im2col1d_u32
.visible .entry im2col1d_u32(
	.param .u64 im2col1d_u32_param_0,
	.param .u64 im2col1d_u32_param_1,
	.param .u64 im2col1d_u32_param_2,
	.param .u64 im2col1d_u32_param_3,
	.param .u64 im2col1d_u32_param_4,
	.param .u64 im2col1d_u32_param_5,
	.param .u64 im2col1d_u32_param_6,
	.param .u64 im2col1d_u32_param_7,
	.param .u64 im2col1d_u32_param_8
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<58>;


	ld.param.u64 	%rd27, [im2col1d_u32_param_0];
	ld.param.u64 	%rd19, [im2col1d_u32_param_1];
	ld.param.u64 	%rd20, [im2col1d_u32_param_2];
	ld.param.u64 	%rd21, [im2col1d_u32_param_3];
	ld.param.u64 	%rd22, [im2col1d_u32_param_4];
	ld.param.u64 	%rd23, [im2col1d_u32_param_5];
	ld.param.u64 	%rd24, [im2col1d_u32_param_6];
	ld.param.u64 	%rd25, [im2col1d_u32_param_7];
	ld.param.u64 	%rd26, [im2col1d_u32_param_8];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.u64 	%p1, %rd1, %rd27;
	@%p1 bra 	$L__BB44_13;

	cvta.to.global.u64 	%rd2, %rd24;
	ld.global.u64 	%rd3, [%rd2+16];
	ld.global.u64 	%rd28, [%rd2+8];
	mul.lo.s64 	%rd4, %rd28, %rd20;
	mul.lo.s64 	%rd5, %rd4, %rd19;
	and.b64  	%rd29, %rd5, -4294967296;
	setp.eq.s64 	%p2, %rd29, 0;
	@%p2 bra 	$L__BB44_3;

	div.u64 	%rd55, %rd1, %rd5;
	bra.uni 	$L__BB44_4;

$L__BB44_3:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd55, %r7;

$L__BB44_4:
	mul.lo.s64 	%rd30, %rd55, %rd5;
	sub.s64 	%rd9, %rd1, %rd30;
	or.b64  	%rd31, %rd9, %rd4;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB44_6;

	div.u64 	%rd56, %rd9, %rd4;
	bra.uni 	$L__BB44_7;

$L__BB44_6:
	cvt.u32.u64 	%r8, %rd4;
	cvt.u32.u64 	%r9, %rd9;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd56, %r10;

$L__BB44_7:
	mul.lo.s64 	%rd33, %rd56, %rd4;
	sub.s64 	%rd13, %rd9, %rd33;
	or.b64  	%rd34, %rd13, %rd20;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p4, %rd35, 0;
	@%p4 bra 	$L__BB44_9;

	div.u64 	%rd57, %rd13, %rd20;
	bra.uni 	$L__BB44_10;

$L__BB44_9:
	cvt.u32.u64 	%r11, %rd20;
	cvt.u32.u64 	%r12, %rd13;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd57, %r13;

$L__BB44_10:
	mul.lo.s64 	%rd36, %rd57, %rd20;
	sub.s64 	%rd37, %rd13, %rd36;
	mul.lo.s64 	%rd38, %rd37, %rd23;
	mul.lo.s64 	%rd39, %rd56, %rd21;
	add.s64 	%rd17, %rd38, %rd39;
	setp.lt.u64 	%p5, %rd17, %rd22;
	add.s64 	%rd40, %rd3, %rd22;
	setp.ge.u64 	%p6, %rd17, %rd40;
	or.pred  	%p7, %p5, %p6;
	cvta.to.global.u64 	%rd41, %rd26;
	shl.b64 	%rd42, %rd1, 2;
	add.s64 	%rd18, %rd41, %rd42;
	@%p7 bra 	$L__BB44_12;
	bra.uni 	$L__BB44_11;

$L__BB44_12:
	mov.u32 	%r15, 0;
	st.global.u32 	[%rd18], %r15;
	bra.uni 	$L__BB44_13;

$L__BB44_11:
	sub.s64 	%rd43, %rd17, %rd22;
	ld.global.u64 	%rd44, [%rd2+24];
	mul.lo.s64 	%rd45, %rd44, %rd55;
	ld.global.u64 	%rd46, [%rd2+32];
	mul.lo.s64 	%rd47, %rd46, %rd57;
	add.s64 	%rd48, %rd47, %rd45;
	ld.global.u64 	%rd49, [%rd2+40];
	mul.lo.s64 	%rd50, %rd49, %rd43;
	add.s64 	%rd51, %rd48, %rd50;
	cvta.to.global.u64 	%rd52, %rd25;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.u32 	%r14, [%rd54];
	st.global.u32 	[%rd18], %r14;

$L__BB44_13:
	ret;

}

